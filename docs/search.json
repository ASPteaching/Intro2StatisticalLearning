[
  {
    "objectID": "labs/The-caret_package.html",
    "href": "labs/The-caret_package.html",
    "title": "The caret package",
    "section": "",
    "text": "options(width=100) \nif(!require(\"knitr\")) install.packages(\"knitr\")\nlibrary(\"knitr\")\n#getOption(\"width\")\nknitr::opts_chunk$set(comment=NA,echo = TRUE, cache=TRUE)"
  },
  {
    "objectID": "labs/The-caret_package.html#learning-to-use-caret",
    "href": "labs/The-caret_package.html#learning-to-use-caret",
    "title": "The caret package",
    "section": "Learning to use caret",
    "text": "Learning to use caret\nThere are multiple resources to learn caretthat go from simple tutorials like this one or similars to courses, papers and a book by Max Kuhn, the creator or the package."
  },
  {
    "objectID": "labs/The-caret_package.html#data-loading",
    "href": "labs/The-caret_package.html#data-loading",
    "title": "The caret package",
    "section": "Data loading",
    "text": "Data loading\n\nlibrary(\"mlbench\")\ndata(Sonar)\nnames(Sonar)\n\n [1] \"V1\"    \"V2\"    \"V3\"    \"V4\"    \"V5\"    \"V6\"    \"V7\"    \"V8\"    \"V9\"    \"V10\"   \"V11\"   \"V12\"  \n[13] \"V13\"   \"V14\"   \"V15\"   \"V16\"   \"V17\"   \"V18\"   \"V19\"   \"V20\"   \"V21\"   \"V22\"   \"V23\"   \"V24\"  \n[25] \"V25\"   \"V26\"   \"V27\"   \"V28\"   \"V29\"   \"V30\"   \"V31\"   \"V32\"   \"V33\"   \"V34\"   \"V35\"   \"V36\"  \n[37] \"V37\"   \"V38\"   \"V39\"   \"V40\"   \"V41\"   \"V42\"   \"V43\"   \"V44\"   \"V45\"   \"V46\"   \"V47\"   \"V48\"  \n[49] \"V49\"   \"V50\"   \"V51\"   \"V52\"   \"V53\"   \"V54\"   \"V55\"   \"V56\"   \"V57\"   \"V58\"   \"V59\"   \"V60\"  \n[61] \"Class\"\n\n\nThe sonarpackage has 208 data points collected on 60 predictors (energy within a particular frequency band)."
  },
  {
    "objectID": "labs/The-caret_package.html#traintest-splitting",
    "href": "labs/The-caret_package.html#traintest-splitting",
    "title": "The caret package",
    "section": "Train/test splitting",
    "text": "Train/test splitting\nWe will most of the time want to split the data into two groups: a training set and a test set.\nThis may be done with the createDataPartition function:\n\nset.seed(1234) # Control of data generation\ninTrain &lt;- createDataPartition(y=Sonar$Class, p=.75, list=FALSE)\nstr(inTrain)\n\n int [1:157, 1] 2 3 4 6 7 8 9 11 14 15 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr \"Resample1\"\n\ntraining &lt;- Sonar[inTrain,]\ntesting &lt;- Sonar[-inTrain,]\nnrow(training)\n\n[1] 157\n\n\nOthers similar functions are: createFolds and createResample,"
  },
  {
    "objectID": "labs/The-caret_package.html#preprocessing-and-training",
    "href": "labs/The-caret_package.html#preprocessing-and-training",
    "title": "The caret package",
    "section": "Preprocessing and training",
    "text": "Preprocessing and training\nUsually, before prediction, data may have to be cleaned and pre-processed.\nCaret allows to integrate it with the training step using the train function.\nThis function has multiple parameter such as:\n\nmethod: Can choose from more than 200 models\npreprocess: all type of filtering and transformations\n\n\nCART1Model &lt;- train (Class ~ ., \n                   data=training, \n                   method=\"rpart1SE\",\n                   preProc=c(\"center\",\"scale\"))\nCART1Model\n\nCART \n\n157 samples\n 60 predictor\n  2 classes: 'M', 'R' \n\nPre-processing: centered (60), scaled (60) \nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 157, 157, 157, 157, 157, 157, ... \nResampling results:\n\n  Accuracy   Kappa   \n  0.6752493  0.350363\n\n\n\nRefining specifications\nMany specifications can be passed using the trainControl instruction.\n\nctrl &lt;- trainControl(method = \"repeatedcv\", repeats=3)\nCART1Model3x10cv &lt;- train (Class ~ ., \n                         data=training, \n                         method=\"rpart1SE\",\n                         trControl=ctrl,\n                         preProc=c(\"center\",\"scale\"))\n\nCART1Model3x10cv\n\nCART \n\n157 samples\n 60 predictor\n  2 classes: 'M', 'R' \n\nPre-processing: centered (60), scaled (60) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 141, 142, 142, 141, 141, 142, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.7087173  0.4168066\n\n\nWe can change the method used by changing the trainControl parameter.\nIn the example below we fit a classification tree with different options:\n\nctrl &lt;- trainControl(method = \"repeatedcv\", repeats=3,\n                     classProbs=TRUE,\n                     summaryFunction=twoClassSummary)\n\nCART1Model3x10cv &lt;- train (Class ~ ., \n                         data=training, \n                         method=\"rpart1SE\", \n                         trControl=ctrl, \n                         metric=\"ROC\", \n                         preProc=c(\"center\",\"scale\"))\n\nCART1Model3x10cv\n\nCART \n\n157 samples\n 60 predictor\n  2 classes: 'M', 'R' \n\nPre-processing: centered (60), scaled (60) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 141, 141, 142, 141, 141, 142, ... \nResampling results:\n\n  ROC        Sens   Spec     \n  0.7757068  0.775  0.6869048\n\n\n\nCART2Fit3x10cv &lt;- train (Class ~ ., \n                       data=training, \n                       method=\"rpart\", \n                       trControl=ctrl, \n                       metric=\"ROC\", \n                       preProc=c(\"center\",\"scale\"))\nCART2Fit3x10cv\n\nCART \n\n157 samples\n 60 predictor\n  2 classes: 'M', 'R' \n\nPre-processing: centered (60), scaled (60) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 142, 142, 142, 142, 142, 140, ... \nResampling results across tuning parameters:\n\n  cp          ROC        Sens       Spec     \n  0.06849315  0.7033441  0.6851852  0.6779762\n  0.10958904  0.6829282  0.7523148  0.5922619\n  0.47945205  0.5517196  0.8629630  0.2404762\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.06849315.\n\nplot(CART2Fit3x10cv)\n\n\n\n\n\n\n\n\n\nCART2Fit3x10cv &lt;- train (Class ~ ., \n                       data=training, \n                       method=\"rpart\", \n                       trControl=ctrl, \n                       metric=\"ROC\",  \n                       tuneLength=10,\n                       preProc=c(\"center\",\"scale\"))\nCART2Fit3x10cv\n\nCART \n\n157 samples\n 60 predictor\n  2 classes: 'M', 'R' \n\nPre-processing: centered (60), scaled (60) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 141, 142, 140, 141, 141, 142, ... \nResampling results across tuning parameters:\n\n  cp          ROC        Sens       Spec     \n  0.00000000  0.7375744  0.7305556  0.6220238\n  0.05327245  0.7382523  0.7453704  0.6130952\n  0.10654490  0.6816468  0.7773148  0.5696429\n  0.15981735  0.6787368  0.8092593  0.5482143\n  0.21308980  0.6787368  0.8092593  0.5482143\n  0.26636225  0.6787368  0.8092593  0.5482143\n  0.31963470  0.6787368  0.8092593  0.5482143\n  0.37290715  0.6787368  0.8092593  0.5482143\n  0.42617960  0.6787368  0.8092593  0.5482143\n  0.47945205  0.5748016  0.8680556  0.2815476\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.05327245.\n\nplot(CART2Fit3x10cv)"
  },
  {
    "objectID": "labs/The-caret_package.html#predict-confusionmatrix-functions",
    "href": "labs/The-caret_package.html#predict-confusionmatrix-functions",
    "title": "The caret package",
    "section": "Predict & confusionMatrix functions",
    "text": "Predict & confusionMatrix functions\nTo predict new samples can be used predict function.\n\ntype = prob : to compute class probabilities\ntype = raw : to predict the class\n\nThe confusionMatrix function will compute the confusion matrix and associated statistics for the model fit.\n\nCART2Probs &lt;- predict(CART2Fit3x10cv, newdata = testing, type = \"prob\")\nCART2Classes &lt;- predict(CART2Fit3x10cv, newdata = testing, type = \"raw\")\nconfusionMatrix(data=CART2Classes,testing$Class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  M  R\n         M 21  5\n         R  6 19\n                                          \n               Accuracy : 0.7843          \n                 95% CI : (0.6468, 0.8871)\n    No Information Rate : 0.5294          \n    P-Value [Acc &gt; NIR] : 0.0001502       \n                                          \n                  Kappa : 0.5681          \n                                          \n Mcnemar's Test P-Value : 1.0000000       \n                                          \n            Sensitivity : 0.7778          \n            Specificity : 0.7917          \n         Pos Pred Value : 0.8077          \n         Neg Pred Value : 0.7600          \n             Prevalence : 0.5294          \n         Detection Rate : 0.4118          \n   Detection Prevalence : 0.5098          \n      Balanced Accuracy : 0.7847          \n                                          \n       'Positive' Class : M"
  },
  {
    "objectID": "labs/The-caret_package.html#model-comparison",
    "href": "labs/The-caret_package.html#model-comparison",
    "title": "The caret package",
    "section": "Model comparison",
    "text": "Model comparison\nThe resamplesfunction enable smodel comparison\n\nresamps=resamples(list(CART2=CART2Fit3x10cv,\n                       CART1=CART1Model3x10cv))\nsummary(resamps)\n\n\nCall:\nsummary.resamples(object = resamps)\n\nModels: CART2, CART1 \nNumber of resamples: 30 \n\nROC \n           Min.   1st Qu.    Median      Mean   3rd Qu.     Max. NA's\nCART2 0.5000000 0.6294643 0.7455357 0.7382523 0.8058036 0.952381    0\nCART1 0.5535714 0.7249504 0.7926587 0.7757068 0.8315972 0.937500    0\n\nSens \n           Min.   1st Qu.    Median      Mean 3rd Qu. Max. NA's\nCART2 0.4444444 0.6250000 0.7500000 0.7453704   0.875    1    0\nCART1 0.4444444 0.6666667 0.7777778 0.7750000   0.875    1    0\n\nSpec \n       Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nCART2 0.250 0.5714286 0.6250000 0.6130952 0.7142857 0.8750000    0\nCART1 0.375 0.5714286 0.7142857 0.6869048 0.8571429 0.8571429    0\n\nxyplot(resamps,what=\"BlandAltman\")\n\n\n\n\n\n\n\ndiffs&lt;-diff(resamps)\nsummary(diffs)\n\n\nCall:\nsummary.diff.resamples(object = diffs)\n\np-value adjustment: bonferroni \nUpper diagonal: estimates of the difference\nLower diagonal: p-value for H0: difference = 0\n\nROC \n      CART2  CART1   \nCART2        -0.03745\nCART1 0.1598         \n\nSens \n      CART2  CART1   \nCART2        -0.02963\nCART1 0.4514         \n\nSpec \n      CART2   CART1   \nCART2         -0.07381\nCART1 0.02404"
  },
  {
    "objectID": "labs/The-caret_package.html#adaboost",
    "href": "labs/The-caret_package.html#adaboost",
    "title": "The caret package",
    "section": "Adaboost",
    "text": "Adaboost\nIn this example, we are using the rpart algorithm as the base learner for AdaBoost. We can then use the predict function to make predictions on new data:\n\nlibrary(caret)\nlibrary(mlbench)\n\ndata(BreastCancer)\n\n# Split the data into training and testing sets\nset.seed(123)\ntrainIndex &lt;- createDataPartition(BreastCancer$Class, p = 0.7, list = FALSE)\ntraining &lt;- BreastCancer[trainIndex, ]\ntesting &lt;- BreastCancer[-trainIndex, ]\n\n# Next, set up \n# - the training control and \n# - tuning parameters for the AdaBoost algorithm:\n\nctrl &lt;- trainControl(method = \"repeatedcv\", \n                     number = 10, repeats = 3,\n                     classProbs = TRUE, \n                     summaryFunction = twoClassSummary)\n\nparams &lt;- data.frame(method = \"AdaBoost\", \n                     nIter = 100, \n                     interaction.depth = 1, \n                     shrinkage = 0.1)\n\n#  we are using 10-fold cross-validation with 3 repeats and the twoClassSummary function for evaluation. \n# We are also setting the number of iterations for the AdaBoost algorithm to 100, the maximum interaction depth to 1, and the shrinkage factor to 0.1.\n\n# Use the train function to train the AdaBoost algorithm on the training data and evaluate its performance on the testing data:\n\nadaboost &lt;- train(Class ~ ., data = training, \n                  method = \"rpart\", \n                  trControl = ctrl, \n                  tuneGrid = params)\n\npredictions &lt;- predict(adaboost, newdata = testing)\n\n# Evaluate the performance of the model\nconfusionMatrix(predictions, testData$diagnosis)"
  },
  {
    "objectID": "labs/The-caret_package.html#gradient-boosting",
    "href": "labs/The-caret_package.html#gradient-boosting",
    "title": "The caret package",
    "section": "Gradient boosting",
    "text": "Gradient boosting\nWe use the gbm method in train() function from the caret package to build a Gradient Boosting model on the Breast Cancer dataset.\n\nlibrary(caret)\nlibrary(gbm)\ndata(BreastCancer)\n\n# Convert the diagnosis column to a binary factor\nBreastCancer$diagnosis &lt;- ifelse(BreastCancer$diagnosis == \"M\", 1, 0)\n\n# Split the dataset into training and testing sets\ntrainIndex &lt;- createDataPartition(BreastCancer$diagnosis, p = 0.7, list = FALSE)\ntrainData &lt;- BreastCancer[trainIndex, ]\ntestData &lt;- BreastCancer[-trainIndex, ]\n\n# Define the training control\nctrl &lt;- trainControl(method = \"cv\", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)\n\n# Define the Gradient Boosting model\nmodel &lt;- train(diagnosis ~ ., data = trainData, method = \"gbm\", trControl = ctrl,\n               verbose = FALSE, metric = \"ROC\", n.trees = 1000, interaction.depth = 3, shrinkage = 0.01)\n\n# Make predictions on the testing set\npredictions &lt;- predict(model, testData)\n\n# Evaluate the performance of the model\nconfusionMatrix(predictions, testData$diagnosis)"
  },
  {
    "objectID": "labs/The-caret_package.html#xgboost",
    "href": "labs/The-caret_package.html#xgboost",
    "title": "The caret package",
    "section": "XGBoost",
    "text": "XGBoost\n\nIn this example, we use the xgbTree method in train() function from the caret package to build an XGBoost model on the BreastCancer dataset.\nThe hyperparameters are set to default values, except for parameters:\n\nnrounds,\nmax_depth,\neta, lambda, and\nalpha\n\nThe final performance is evaluated using a confusion matrix.\n\n\nlibrary(caret)\nlibrary(xgboost)\ndata(BreastCancer)\n\n# Convert the diagnosis column to a binary factor\nBreastCancer$diagnosis &lt;- ifelse(BreastCancer$diagnosis == \"M\", 1, 0)\n\n# Split the dataset into training and testing sets\ntrainIndex &lt;- createDataPartition(BreastCancer$diagnosis, p = 0.7, list = FALSE)\ntrainData &lt;- BreastCancer[trainIndex, ]\ntestData &lt;- BreastCancer[-trainIndex, ]\n\n# Define the training control\nctrl &lt;- trainControl(method = \"cv\", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)\n\n# Define the XGBoost model\nmodel &lt;- train(diagnosis ~ ., \n               data = trainData, \n               method = \"xgbTree\", trControl = ctrl,\n               verbose = FALSE, metric = \"ROC\", \n               nrounds = 1000, max_depth = 3, \n               eta = 0.01, lambda = 1, alpha = 0)\n\n# Make predictions on the testing set\npredictions &lt;- predict(model, testData)\n\n# Evaluate the performance of the model\nconfusionMatrix(predictions, testData$diagnosis)"
  },
  {
    "objectID": "labs/The-caret_package.html#references",
    "href": "labs/The-caret_package.html#references",
    "title": "The caret package",
    "section": "References",
    "text": "References\n\nOfficial references and resources\n\nCaret tutorial at UseR! 2014\nThe caret package\nJSS Paper\nApplied Predictive Modeling Blog\nCaret cheatsheet in Rstudio cheatsheet page\n\n\n\nOther resources\n\nCaret Package – A Practical Guide to Machine Learning in R -Create predictive models in R with Caret\nCaret R Package for Applied Predictive Modeling"
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "This example has been adapted from the book “Introduction to Statistical Learning with R”, lab 8.3.\nThe authors have decided to use the R tree package, which is not the most powerful R package for trees, but offers a good compromise between power and flexibility.\nThe lab relies on the Carseats dataset, a simulated dataset, that is included with the book’s package, containing several variables about sales of child car seats at different stores.\n\nrequire(ISLR2)\ndata(\"Carseats\")\n# help(\"Carseats\")\n\nA data frame with 400 observations on the following 11 variables.\n\nSales: Unit sales (in thousands) at each location\nCompPrice: Price charged by competitor at each location\nIncome: Community income level (in thousands of dollars)\nAdvertising: Local advertising budget for company at each location (in thousands of dollars)\nPopulation: Population size in region (in thousands)\nPrice: Price company charges for car seats at each site\nShelveLoc: A factor with levels Bad, Good and Medium indicating the quality of the shelving location for the car seats at each site\nAge: Average age of the local population\nEducation: Education level at each location\nUrban: A factor with levels No and Yes to indicate whether the store is in an urban or rural location\nUS: A factor with levels No and Yes to indicate whether the store is in the US or not\n\nThe first part of the lab will aim at predicting the variable sales.\nIn order to apply classification trees first, we start by categorizing the salesvariable. This is not usually seen as a good strategy, so take it only for didactical purpose.\n\n\n\nWe use a generic name for the dataset, in order to facilitate code reuse.\n\nmyDescription &lt;- \"The data are a simulated data set containing sales of child car seats at different stores [@james2013introduction]\"\nmydataset &lt;- Carseats\n\n\nn &lt;- nrow(mydataset)\np &lt;- ncol(mydataset)\n\nThere are 400 rows and 11 columns.\nThe variable Sales is categorized creating a new variable, High, which takes on a value of Yes if the Sales variable exceeds 8, and a value of No otherwise.\n\n# as.factor() changes the type of variable to factor\nmydataset$High=as.factor(ifelse(mydataset$Sales&lt;=8,\"No\",\"Yes\"))\n\nThe number of observations for each class is:\n\nkable(table(mydataset$High), caption= \"Number of observations for each class\", col.names = c('High','Freq'))\n\n\nNumber of observations for each class\n\n\nHigh\nFreq\n\n\n\n\nNo\n236\n\n\nYes\n164\n\n\n\n\n\nThe aim is of this study is to predict the categorical values of sales (High) using all variables but Sales.\nIt is a classification problem and we will build a classification tree model.\n\n\nThis is a short data set summary\n\nsummary(mydataset)\n\n     Sales          CompPrice       Income        Advertising    \n Min.   : 0.000   Min.   : 77   Min.   : 21.00   Min.   : 0.000  \n 1st Qu.: 5.390   1st Qu.:115   1st Qu.: 42.75   1st Qu.: 0.000  \n Median : 7.490   Median :125   Median : 69.00   Median : 5.000  \n Mean   : 7.496   Mean   :125   Mean   : 68.66   Mean   : 6.635  \n 3rd Qu.: 9.320   3rd Qu.:135   3rd Qu.: 91.00   3rd Qu.:12.000  \n Max.   :16.270   Max.   :175   Max.   :120.00   Max.   :29.000  \n   Population        Price        ShelveLoc        Age          Education   \n Min.   : 10.0   Min.   : 24.0   Bad   : 96   Min.   :25.00   Min.   :10.0  \n 1st Qu.:139.0   1st Qu.:100.0   Good  : 85   1st Qu.:39.75   1st Qu.:12.0  \n Median :272.0   Median :117.0   Medium:219   Median :54.50   Median :14.0  \n Mean   :264.8   Mean   :115.8                Mean   :53.32   Mean   :13.9  \n 3rd Qu.:398.5   3rd Qu.:131.0                3rd Qu.:66.00   3rd Qu.:16.0  \n Max.   :509.0   Max.   :191.0                Max.   :80.00   Max.   :18.0  \n Urban       US       High    \n No :118   No :142   No :236  \n Yes:282   Yes:258   Yes:164  \n                              \n                              \n                              \n                              \n\n\nAn improved description:\n\nskimr::skim(mydataset)\n\n\nData summary\n\n\nName\nmydataset\n\n\nNumber of rows\n400\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nShelveLoc\n0\n1\nFALSE\n3\nMed: 219, Bad: 96, Goo: 85\n\n\nUrban\n0\n1\nFALSE\n2\nYes: 282, No: 118\n\n\nUS\n0\n1\nFALSE\n2\nYes: 258, No: 142\n\n\nHigh\n0\n1\nFALSE\n2\nNo: 236, Yes: 164\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSales\n0\n1\n7.50\n2.82\n0\n5.39\n7.49\n9.32\n16.27\n▁▆▇▃▁\n\n\nCompPrice\n0\n1\n124.97\n15.33\n77\n115.00\n125.00\n135.00\n175.00\n▁▅▇▃▁\n\n\nIncome\n0\n1\n68.66\n27.99\n21\n42.75\n69.00\n91.00\n120.00\n▇▆▇▆▅\n\n\nAdvertising\n0\n1\n6.64\n6.65\n0\n0.00\n5.00\n12.00\n29.00\n▇▃▃▁▁\n\n\nPopulation\n0\n1\n264.84\n147.38\n10\n139.00\n272.00\n398.50\n509.00\n▇▇▇▇▇\n\n\nPrice\n0\n1\n115.80\n23.68\n24\n100.00\n117.00\n131.00\n191.00\n▁▂▇▆▁\n\n\nAge\n0\n1\n53.32\n16.20\n25\n39.75\n54.50\n66.00\n80.00\n▇▆▇▇▇\n\n\nEducation\n0\n1\n13.90\n2.62\n10\n12.00\n14.00\n16.00\n18.00\n▇▇▃▇▇\n\n\n\n\n\n\n\n\n\nIt is very common that the data need to be preprocessed before training the model*\nIn this case, there seem to be no missing values, no outliers and most variables are decently symmetrical, so no cleaning or preprocessing are required.\n\n\n\nIn order to properly evaluate the performance of a model, we must estimate the error rather than simply computing the training error.\nWith this aim in mind we proceed as follows:\n\nsplit the observations into a training set and a test set,\nbuild the model using the training set, and\nevaluate its performance on the test data.\n\n\nset.seed(2)\npt &lt;- 1/2\ntrain &lt;- sample(1:nrow(mydataset),pt*nrow(mydataset))\nmydataset.test &lt;- mydataset[-train,]\nHigh.test &lt;-  mydataset[-train,\"High\"]\n\nThe train and tets set have 200 200 observations respectively.\nIn train data, the number of observations for each class is:\n\nkableExtra::kable(table(mydataset[train,\"High\"]), caption= \"Train data: number of observations for each class\", col.names = c('High','Freq'))\n\n\nTrain data: number of observations for each class\n\n\nHigh\nFreq\n\n\n\n\nNo\n119\n\n\nYes\n81\n\n\n\n\n\n\n\n\nWe now use the tree() function to fit a classification tree in order to predict High using all variables but Sales using only de train set.\n\nlibrary(tree)\ntree.mydataset=tree(High~.-Sales, mydataset,\n                    subset=train, \n                    split=\"deviance\")\n\n\n\n\n\nThe summary() function lists the variables that are used as internal nodes in the tree, the number of terminal nodes, and the training error rate\n\nsummary(tree.mydataset)\n\n\nClassification tree:\ntree(formula = High ~ . - Sales, data = mydataset, subset = train, \n    split = \"deviance\")\nVariables actually used in tree construction:\n[1] \"Price\"       \"Population\"  \"ShelveLoc\"   \"Age\"         \"Education\"  \n[6] \"CompPrice\"   \"Advertising\" \"Income\"      \"US\"         \nNumber of terminal nodes:  21 \nResidual mean deviance:  0.5543 = 99.22 / 179 \nMisclassification error rate: 0.115 = 23 / 200 \n\n# summary(tree.mydataset2)\n\nFor classification trees the deviance of a tree (roughly equivalent to the concept of impurity) is defined as the sum over all terminal leaves of: \\[\n-2 \\sum_m \\sum_k n_{mk} log(\\hat{p}_{mk}),\n\\]\nwhere \\(n_{mk}\\) is the number of observations in the mth terminal node that belong to the kth class.\nThe residual mean deviance reported is simply the deviance divided by \\(n - |T_0|\\) where \\(T_0\\) is the number of terminal nodes.\n\n\n\nThe next step is display the tree graphically. We use the plot() function to display the tree structure, and the text()function to display the node labels.\n\nplot(tree.mydataset)\ntext(tree.mydataset,pretty=0, cex=0.6)\n\n\n\n\nClassification tree\n\n\n\n\nIt is also possible to show a R print output corresponding to each branch of the tree.\n\ntree.mydataset\n\nnode), split, n, deviance, yval, (yprob)\n      * denotes terminal node\n\n  1) root 200 270.000 No ( 0.59500 0.40500 )  \n    2) Price &lt; 96.5 40  47.050 Yes ( 0.27500 0.72500 )  \n      4) Population &lt; 414 31  40.320 Yes ( 0.35484 0.64516 )  \n        8) ShelveLoc: Bad,Medium 25  34.300 Yes ( 0.44000 0.56000 )  \n         16) Age &lt; 64.5 17  20.600 Yes ( 0.29412 0.70588 )  \n           32) Education &lt; 13.5 7   0.000 Yes ( 0.00000 1.00000 ) *\n           33) Education &gt; 13.5 10  13.860 Yes ( 0.50000 0.50000 )  \n             66) Education &lt; 16.5 5   5.004 No ( 0.80000 0.20000 ) *\n             67) Education &gt; 16.5 5   5.004 Yes ( 0.20000 0.80000 ) *\n         17) Age &gt; 64.5 8   8.997 No ( 0.75000 0.25000 ) *\n        9) ShelveLoc: Good 6   0.000 Yes ( 0.00000 1.00000 ) *\n      5) Population &gt; 414 9   0.000 Yes ( 0.00000 1.00000 ) *\n    3) Price &gt; 96.5 160 201.800 No ( 0.67500 0.32500 )  \n      6) ShelveLoc: Bad,Medium 135 154.500 No ( 0.74074 0.25926 )  \n       12) Price &lt; 124.5 82 107.700 No ( 0.63415 0.36585 )  \n         24) Age &lt; 49.5 34  45.230 Yes ( 0.38235 0.61765 )  \n           48) CompPrice &lt; 130.5 21  28.680 No ( 0.57143 0.42857 )  \n             96) Population &lt; 134.5 6   0.000 No ( 1.00000 0.00000 ) *\n             97) Population &gt; 134.5 15  20.190 Yes ( 0.40000 0.60000 )  \n              194) Population &lt; 343 7   5.742 Yes ( 0.14286 0.85714 ) *\n              195) Population &gt; 343 8  10.590 No ( 0.62500 0.37500 ) *\n           49) CompPrice &gt; 130.5 13   7.051 Yes ( 0.07692 0.92308 ) *\n         25) Age &gt; 49.5 48  46.330 No ( 0.81250 0.18750 )  \n           50) CompPrice &lt; 124.5 28  14.410 No ( 0.92857 0.07143 )  \n            100) Price &lt; 101.5 8   8.997 No ( 0.75000 0.25000 ) *\n            101) Price &gt; 101.5 20   0.000 No ( 1.00000 0.00000 ) *\n           51) CompPrice &gt; 124.5 20  25.900 No ( 0.65000 0.35000 )  \n            102) Price &lt; 119 14  19.410 No ( 0.50000 0.50000 )  \n              204) Advertising &lt; 10.5 9  11.460 No ( 0.66667 0.33333 ) *\n              205) Advertising &gt; 10.5 5   5.004 Yes ( 0.20000 0.80000 ) *\n            103) Price &gt; 119 6   0.000 No ( 1.00000 0.00000 ) *\n       13) Price &gt; 124.5 53  33.120 No ( 0.90566 0.09434 )  \n         26) Population &lt; 393.5 34   0.000 No ( 1.00000 0.00000 ) *\n         27) Population &gt; 393.5 19  21.900 No ( 0.73684 0.26316 )  \n           54) CompPrice &lt; 143.5 13   7.051 No ( 0.92308 0.07692 ) *\n           55) CompPrice &gt; 143.5 6   7.638 Yes ( 0.33333 0.66667 ) *\n      7) ShelveLoc: Good 25  31.340 Yes ( 0.32000 0.68000 )  \n       14) Income &lt; 43 7   8.376 No ( 0.71429 0.28571 ) *\n       15) Income &gt; 43 18  16.220 Yes ( 0.16667 0.83333 )  \n         30) US: No 6   8.318 Yes ( 0.50000 0.50000 ) *\n         31) US: Yes 12   0.000 Yes ( 0.00000 1.00000 ) *\n\n\n\n\n\nIn order to properly evaluate the performance of a classification tree on these data, we must estimate the test error rather than simply computing the training error.\nWe have split the observations into a training set and a test set, and the tree has been built using the training set.\nAfter this, the tree performance is evaluated on the test data. The predict() function can be used for this purpose.\n\ntree.pred=predict(tree.mydataset,mydataset.test,type=\"class\")\nres &lt;- table(tree.pred,High.test)\nres\n\n         High.test\ntree.pred  No Yes\n      No  104  33\n      Yes  13  50\n\naccrcy &lt;- sum(diag(res)/sum(res))\n\nThe accuracy is 0.77 or misclassification error rate is 0.23, which are respectively smaller and biiger than those computed from the tree built on the train data.\n\n\n\nWe know there is a chance that fitting the tree produces some overfitting so we can consider whether pruning the tree could lead to improved results.\nThe function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity. - Cost complexity pruning is used in order to select a sequence of trees for consideration. - We use the argument FUN = prune.misclass in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the cv.tree() function, which is deviance.\nThe cv.tree() function reports the number of terminal nodes of each tree considered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used\n\nset.seed(123987)\ncv.mydataset=cv.tree(tree.mydataset,FUN=prune.misclass)\nnames(cv.mydataset)\n\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n\ncv.mydataset\n\n$size\n[1] 21 19 14  9  8  5  3  2  1\n\n$dev\n[1] 82 80 78 78 78 76 76 84 82\n\n$k\n[1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n\nNote that, despite the name, dev corresponds to the cross-validation error rate in this instance.\nThe output shows how, as the size of the tree increases, so does the deviance.\nThis can be better visualized by plotting the error rate as a function of sizeand k.\n\npar(mfrow=c(1,2))\nplot(cv.mydataset$size,cv.mydataset$dev,type=\"b\")\nplot(cv.mydataset$k,cv.mydataset$dev,type=\"b\")\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nThese plots can be used to suggest the best tree, but it can also be chosen automatically by taking the minimal value \\(k\\) from the output of the cv.tree function.\n\nmyBest &lt;- cv.mydataset$size[which.min(cv.mydataset$dev)]\n\nNow, the prune.misclass() function can be used to prune the tree and obtain a “best tree”. If we decide to call the best tree the one that has reached the smallest deviance we can proceed as follows:\n\nprune.mydataset=prune.misclass(tree.mydataset,best=myBest)\n\n\nplot(prune.mydataset)\ntext(prune.mydataset,pretty=0)\n\nThe tree is clearly smaller than the original one, but how well does this pruned tree perform on the test data set?\n\nprunedTree.pred=predict(prune.mydataset,mydataset.test,type=\"class\")\nprunedRes &lt;- table(prunedTree.pred,High.test)\nprunedRes\n\n               High.test\nprunedTree.pred No Yes\n            No  82  16\n            Yes 35  67\n\nprunedAccrcy &lt;- sum(diag(prunedRes)/sum(prunedRes))\n\nThe accuracy is 0.745.\nIf we increase the value of best, for example 21 terminal nodes, we obtain a larger pruned tree with lower classification accuracy:\n\nprune.mydataset=prune.misclass(tree.mydataset, \n                               best = cv.mydataset$size[1])\nplot(prune.mydataset)\ntext(prune.mydataset, pretty=0)\n\n\n\n\nOther classification pruned tree\n\n\n\n\n\nptree.pred=predict(prune.mydataset, mydataset.test, type=\"class\")\npres &lt;- table(ptree.pred, High.test)\npres\n\n          High.test\nptree.pred  No Yes\n       No  104  31\n       Yes  13  52\n\npaccrcy &lt;- sum(diag(pres)/sum(pres))\n\nThe accuracy is 0.78.\nIn conclusion It can be seen that the difference in accuracy between the pruned tree and the original one is small. Indeed, changing the seed for splitting can lead to both smaller or bigger accuracy in the pruned tree than in the original one.\nObviously, the pruned tree is smaller so even if the original tree is slightly more accurate than the pruned one we might prefer the second one, because it is relying on less data to produce almost the same accuracy, whic is something that most users usually prefer.\n\n\n\nA reasonable question is how would the accuracy of the trees be affected if, instead of categorizing sales we had used it “as.is”, building a regression tree instead.\nAlthough it may seem straightforward to answer this question by building a regression tree using the approach described in next section, the fact is that it is no so immediate as it may seem.\nThe reason for this is that, if we wish to compare the perfomance of both approaches we need a common measure of accuracy. For regression trees the Mean Square Error is generally used, while accuracy or some other measures derived from the confusion matrix are common for classification trees. Comparing those two measures, however, is not straightforward. One may think of relying on some kind of information measure, that can be computed on both regresion and classification trees such as entropy or Kullback-Leiber divergence, but the problem then is how to derive such measure for both the classification and the regression trees."
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#introduction",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#introduction",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "This example has been adapted from the book “Introduction to Statistical Learning with R”, lab 8.3.\nThe authors have decided to use the R tree package, which is not the most powerful R package for trees, but offers a good compromise between power and flexibility.\nThe lab relies on the Carseats dataset, a simulated dataset, that is included with the book’s package, containing several variables about sales of child car seats at different stores.\n\nrequire(ISLR2)\ndata(\"Carseats\")\n# help(\"Carseats\")\n\nA data frame with 400 observations on the following 11 variables.\n\nSales: Unit sales (in thousands) at each location\nCompPrice: Price charged by competitor at each location\nIncome: Community income level (in thousands of dollars)\nAdvertising: Local advertising budget for company at each location (in thousands of dollars)\nPopulation: Population size in region (in thousands)\nPrice: Price company charges for car seats at each site\nShelveLoc: A factor with levels Bad, Good and Medium indicating the quality of the shelving location for the car seats at each site\nAge: Average age of the local population\nEducation: Education level at each location\nUrban: A factor with levels No and Yes to indicate whether the store is in an urban or rural location\nUS: A factor with levels No and Yes to indicate whether the store is in the US or not\n\nThe first part of the lab will aim at predicting the variable sales.\nIn order to apply classification trees first, we start by categorizing the salesvariable. This is not usually seen as a good strategy, so take it only for didactical purpose."
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#data-description",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#data-description",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "We use a generic name for the dataset, in order to facilitate code reuse.\n\nmyDescription &lt;- \"The data are a simulated data set containing sales of child car seats at different stores [@james2013introduction]\"\nmydataset &lt;- Carseats\n\n\nn &lt;- nrow(mydataset)\np &lt;- ncol(mydataset)\n\nThere are 400 rows and 11 columns.\nThe variable Sales is categorized creating a new variable, High, which takes on a value of Yes if the Sales variable exceeds 8, and a value of No otherwise.\n\n# as.factor() changes the type of variable to factor\nmydataset$High=as.factor(ifelse(mydataset$Sales&lt;=8,\"No\",\"Yes\"))\n\nThe number of observations for each class is:\n\nkable(table(mydataset$High), caption= \"Number of observations for each class\", col.names = c('High','Freq'))\n\n\nNumber of observations for each class\n\n\nHigh\nFreq\n\n\n\n\nNo\n236\n\n\nYes\n164\n\n\n\n\n\nThe aim is of this study is to predict the categorical values of sales (High) using all variables but Sales.\nIt is a classification problem and we will build a classification tree model.\n\n\nThis is a short data set summary\n\nsummary(mydataset)\n\n     Sales          CompPrice       Income        Advertising    \n Min.   : 0.000   Min.   : 77   Min.   : 21.00   Min.   : 0.000  \n 1st Qu.: 5.390   1st Qu.:115   1st Qu.: 42.75   1st Qu.: 0.000  \n Median : 7.490   Median :125   Median : 69.00   Median : 5.000  \n Mean   : 7.496   Mean   :125   Mean   : 68.66   Mean   : 6.635  \n 3rd Qu.: 9.320   3rd Qu.:135   3rd Qu.: 91.00   3rd Qu.:12.000  \n Max.   :16.270   Max.   :175   Max.   :120.00   Max.   :29.000  \n   Population        Price        ShelveLoc        Age          Education   \n Min.   : 10.0   Min.   : 24.0   Bad   : 96   Min.   :25.00   Min.   :10.0  \n 1st Qu.:139.0   1st Qu.:100.0   Good  : 85   1st Qu.:39.75   1st Qu.:12.0  \n Median :272.0   Median :117.0   Medium:219   Median :54.50   Median :14.0  \n Mean   :264.8   Mean   :115.8                Mean   :53.32   Mean   :13.9  \n 3rd Qu.:398.5   3rd Qu.:131.0                3rd Qu.:66.00   3rd Qu.:16.0  \n Max.   :509.0   Max.   :191.0                Max.   :80.00   Max.   :18.0  \n Urban       US       High    \n No :118   No :142   No :236  \n Yes:282   Yes:258   Yes:164  \n                              \n                              \n                              \n                              \n\n\nAn improved description:\n\nskimr::skim(mydataset)\n\n\nData summary\n\n\nName\nmydataset\n\n\nNumber of rows\n400\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nShelveLoc\n0\n1\nFALSE\n3\nMed: 219, Bad: 96, Goo: 85\n\n\nUrban\n0\n1\nFALSE\n2\nYes: 282, No: 118\n\n\nUS\n0\n1\nFALSE\n2\nYes: 258, No: 142\n\n\nHigh\n0\n1\nFALSE\n2\nNo: 236, Yes: 164\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSales\n0\n1\n7.50\n2.82\n0\n5.39\n7.49\n9.32\n16.27\n▁▆▇▃▁\n\n\nCompPrice\n0\n1\n124.97\n15.33\n77\n115.00\n125.00\n135.00\n175.00\n▁▅▇▃▁\n\n\nIncome\n0\n1\n68.66\n27.99\n21\n42.75\n69.00\n91.00\n120.00\n▇▆▇▆▅\n\n\nAdvertising\n0\n1\n6.64\n6.65\n0\n0.00\n5.00\n12.00\n29.00\n▇▃▃▁▁\n\n\nPopulation\n0\n1\n264.84\n147.38\n10\n139.00\n272.00\n398.50\n509.00\n▇▇▇▇▇\n\n\nPrice\n0\n1\n115.80\n23.68\n24\n100.00\n117.00\n131.00\n191.00\n▁▂▇▆▁\n\n\nAge\n0\n1\n53.32\n16.20\n25\n39.75\n54.50\n66.00\n80.00\n▇▆▇▇▇\n\n\nEducation\n0\n1\n13.90\n2.62\n10\n12.00\n14.00\n16.00\n18.00\n▇▇▃▇▇"
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#preprocess",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#preprocess",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "It is very common that the data need to be preprocessed before training the model*\nIn this case, there seem to be no missing values, no outliers and most variables are decently symmetrical, so no cleaning or preprocessing are required."
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#traintest-partition-of-data",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#traintest-partition-of-data",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "In order to properly evaluate the performance of a model, we must estimate the error rather than simply computing the training error.\nWith this aim in mind we proceed as follows:\n\nsplit the observations into a training set and a test set,\nbuild the model using the training set, and\nevaluate its performance on the test data.\n\n\nset.seed(2)\npt &lt;- 1/2\ntrain &lt;- sample(1:nrow(mydataset),pt*nrow(mydataset))\nmydataset.test &lt;- mydataset[-train,]\nHigh.test &lt;-  mydataset[-train,\"High\"]\n\nThe train and tets set have 200 200 observations respectively.\nIn train data, the number of observations for each class is:\n\nkableExtra::kable(table(mydataset[train,\"High\"]), caption= \"Train data: number of observations for each class\", col.names = c('High','Freq'))\n\n\nTrain data: number of observations for each class\n\n\nHigh\nFreq\n\n\n\n\nNo\n119\n\n\nYes\n81"
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#train-model",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#train-model",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "We now use the tree() function to fit a classification tree in order to predict High using all variables but Sales using only de train set.\n\nlibrary(tree)\ntree.mydataset=tree(High~.-Sales, mydataset,\n                    subset=train, \n                    split=\"deviance\")\n\n\n\n\n\nThe summary() function lists the variables that are used as internal nodes in the tree, the number of terminal nodes, and the training error rate\n\nsummary(tree.mydataset)\n\n\nClassification tree:\ntree(formula = High ~ . - Sales, data = mydataset, subset = train, \n    split = \"deviance\")\nVariables actually used in tree construction:\n[1] \"Price\"       \"Population\"  \"ShelveLoc\"   \"Age\"         \"Education\"  \n[6] \"CompPrice\"   \"Advertising\" \"Income\"      \"US\"         \nNumber of terminal nodes:  21 \nResidual mean deviance:  0.5543 = 99.22 / 179 \nMisclassification error rate: 0.115 = 23 / 200 \n\n# summary(tree.mydataset2)\n\nFor classification trees the deviance of a tree (roughly equivalent to the concept of impurity) is defined as the sum over all terminal leaves of: \\[\n-2 \\sum_m \\sum_k n_{mk} log(\\hat{p}_{mk}),\n\\]\nwhere \\(n_{mk}\\) is the number of observations in the mth terminal node that belong to the kth class.\nThe residual mean deviance reported is simply the deviance divided by \\(n - |T_0|\\) where \\(T_0\\) is the number of terminal nodes."
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#plot-the-tree",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#plot-the-tree",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "The next step is display the tree graphically. We use the plot() function to display the tree structure, and the text()function to display the node labels.\n\nplot(tree.mydataset)\ntext(tree.mydataset,pretty=0, cex=0.6)\n\n\n\n\nClassification tree\n\n\n\n\nIt is also possible to show a R print output corresponding to each branch of the tree.\n\ntree.mydataset\n\nnode), split, n, deviance, yval, (yprob)\n      * denotes terminal node\n\n  1) root 200 270.000 No ( 0.59500 0.40500 )  \n    2) Price &lt; 96.5 40  47.050 Yes ( 0.27500 0.72500 )  \n      4) Population &lt; 414 31  40.320 Yes ( 0.35484 0.64516 )  \n        8) ShelveLoc: Bad,Medium 25  34.300 Yes ( 0.44000 0.56000 )  \n         16) Age &lt; 64.5 17  20.600 Yes ( 0.29412 0.70588 )  \n           32) Education &lt; 13.5 7   0.000 Yes ( 0.00000 1.00000 ) *\n           33) Education &gt; 13.5 10  13.860 Yes ( 0.50000 0.50000 )  \n             66) Education &lt; 16.5 5   5.004 No ( 0.80000 0.20000 ) *\n             67) Education &gt; 16.5 5   5.004 Yes ( 0.20000 0.80000 ) *\n         17) Age &gt; 64.5 8   8.997 No ( 0.75000 0.25000 ) *\n        9) ShelveLoc: Good 6   0.000 Yes ( 0.00000 1.00000 ) *\n      5) Population &gt; 414 9   0.000 Yes ( 0.00000 1.00000 ) *\n    3) Price &gt; 96.5 160 201.800 No ( 0.67500 0.32500 )  \n      6) ShelveLoc: Bad,Medium 135 154.500 No ( 0.74074 0.25926 )  \n       12) Price &lt; 124.5 82 107.700 No ( 0.63415 0.36585 )  \n         24) Age &lt; 49.5 34  45.230 Yes ( 0.38235 0.61765 )  \n           48) CompPrice &lt; 130.5 21  28.680 No ( 0.57143 0.42857 )  \n             96) Population &lt; 134.5 6   0.000 No ( 1.00000 0.00000 ) *\n             97) Population &gt; 134.5 15  20.190 Yes ( 0.40000 0.60000 )  \n              194) Population &lt; 343 7   5.742 Yes ( 0.14286 0.85714 ) *\n              195) Population &gt; 343 8  10.590 No ( 0.62500 0.37500 ) *\n           49) CompPrice &gt; 130.5 13   7.051 Yes ( 0.07692 0.92308 ) *\n         25) Age &gt; 49.5 48  46.330 No ( 0.81250 0.18750 )  \n           50) CompPrice &lt; 124.5 28  14.410 No ( 0.92857 0.07143 )  \n            100) Price &lt; 101.5 8   8.997 No ( 0.75000 0.25000 ) *\n            101) Price &gt; 101.5 20   0.000 No ( 1.00000 0.00000 ) *\n           51) CompPrice &gt; 124.5 20  25.900 No ( 0.65000 0.35000 )  \n            102) Price &lt; 119 14  19.410 No ( 0.50000 0.50000 )  \n              204) Advertising &lt; 10.5 9  11.460 No ( 0.66667 0.33333 ) *\n              205) Advertising &gt; 10.5 5   5.004 Yes ( 0.20000 0.80000 ) *\n            103) Price &gt; 119 6   0.000 No ( 1.00000 0.00000 ) *\n       13) Price &gt; 124.5 53  33.120 No ( 0.90566 0.09434 )  \n         26) Population &lt; 393.5 34   0.000 No ( 1.00000 0.00000 ) *\n         27) Population &gt; 393.5 19  21.900 No ( 0.73684 0.26316 )  \n           54) CompPrice &lt; 143.5 13   7.051 No ( 0.92308 0.07692 ) *\n           55) CompPrice &gt; 143.5 6   7.638 Yes ( 0.33333 0.66667 ) *\n      7) ShelveLoc: Good 25  31.340 Yes ( 0.32000 0.68000 )  \n       14) Income &lt; 43 7   8.376 No ( 0.71429 0.28571 ) *\n       15) Income &gt; 43 18  16.220 Yes ( 0.16667 0.83333 )  \n         30) US: No 6   8.318 Yes ( 0.50000 0.50000 ) *\n         31) US: Yes 12   0.000 Yes ( 0.00000 1.00000 ) *"
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#prediction",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#prediction",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "In order to properly evaluate the performance of a classification tree on these data, we must estimate the test error rather than simply computing the training error.\nWe have split the observations into a training set and a test set, and the tree has been built using the training set.\nAfter this, the tree performance is evaluated on the test data. The predict() function can be used for this purpose.\n\ntree.pred=predict(tree.mydataset,mydataset.test,type=\"class\")\nres &lt;- table(tree.pred,High.test)\nres\n\n         High.test\ntree.pred  No Yes\n      No  104  33\n      Yes  13  50\n\naccrcy &lt;- sum(diag(res)/sum(res))\n\nThe accuracy is 0.77 or misclassification error rate is 0.23, which are respectively smaller and biiger than those computed from the tree built on the train data."
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#pruning-the-tree-tunning-model",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#pruning-the-tree-tunning-model",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "We know there is a chance that fitting the tree produces some overfitting so we can consider whether pruning the tree could lead to improved results.\nThe function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity. - Cost complexity pruning is used in order to select a sequence of trees for consideration. - We use the argument FUN = prune.misclass in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the cv.tree() function, which is deviance.\nThe cv.tree() function reports the number of terminal nodes of each tree considered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used\n\nset.seed(123987)\ncv.mydataset=cv.tree(tree.mydataset,FUN=prune.misclass)\nnames(cv.mydataset)\n\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n\ncv.mydataset\n\n$size\n[1] 21 19 14  9  8  5  3  2  1\n\n$dev\n[1] 82 80 78 78 78 76 76 84 82\n\n$k\n[1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n\nNote that, despite the name, dev corresponds to the cross-validation error rate in this instance.\nThe output shows how, as the size of the tree increases, so does the deviance.\nThis can be better visualized by plotting the error rate as a function of sizeand k.\n\npar(mfrow=c(1,2))\nplot(cv.mydataset$size,cv.mydataset$dev,type=\"b\")\nplot(cv.mydataset$k,cv.mydataset$dev,type=\"b\")\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nThese plots can be used to suggest the best tree, but it can also be chosen automatically by taking the minimal value \\(k\\) from the output of the cv.tree function.\n\nmyBest &lt;- cv.mydataset$size[which.min(cv.mydataset$dev)]\n\nNow, the prune.misclass() function can be used to prune the tree and obtain a “best tree”. If we decide to call the best tree the one that has reached the smallest deviance we can proceed as follows:\n\nprune.mydataset=prune.misclass(tree.mydataset,best=myBest)\n\n\nplot(prune.mydataset)\ntext(prune.mydataset,pretty=0)\n\nThe tree is clearly smaller than the original one, but how well does this pruned tree perform on the test data set?\n\nprunedTree.pred=predict(prune.mydataset,mydataset.test,type=\"class\")\nprunedRes &lt;- table(prunedTree.pred,High.test)\nprunedRes\n\n               High.test\nprunedTree.pred No Yes\n            No  82  16\n            Yes 35  67\n\nprunedAccrcy &lt;- sum(diag(prunedRes)/sum(prunedRes))\n\nThe accuracy is 0.745.\nIf we increase the value of best, for example 21 terminal nodes, we obtain a larger pruned tree with lower classification accuracy:\n\nprune.mydataset=prune.misclass(tree.mydataset, \n                               best = cv.mydataset$size[1])\nplot(prune.mydataset)\ntext(prune.mydataset, pretty=0)\n\n\n\n\nOther classification pruned tree\n\n\n\n\n\nptree.pred=predict(prune.mydataset, mydataset.test, type=\"class\")\npres &lt;- table(ptree.pred, High.test)\npres\n\n          High.test\nptree.pred  No Yes\n       No  104  31\n       Yes  13  52\n\npaccrcy &lt;- sum(diag(pres)/sum(pres))\n\nThe accuracy is 0.78.\nIn conclusion It can be seen that the difference in accuracy between the pruned tree and the original one is small. Indeed, changing the seed for splitting can lead to both smaller or bigger accuracy in the pruned tree than in the original one.\nObviously, the pruned tree is smaller so even if the original tree is slightly more accurate than the pruned one we might prefer the second one, because it is relying on less data to produce almost the same accuracy, whic is something that most users usually prefer."
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#predicting-car-sales-with-regression-trees",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#predicting-car-sales-with-regression-trees",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "A reasonable question is how would the accuracy of the trees be affected if, instead of categorizing sales we had used it “as.is”, building a regression tree instead.\nAlthough it may seem straightforward to answer this question by building a regression tree using the approach described in next section, the fact is that it is no so immediate as it may seem.\nThe reason for this is that, if we wish to compare the perfomance of both approaches we need a common measure of accuracy. For regression trees the Mean Square Error is generally used, while accuracy or some other measures derived from the confusion matrix are common for classification trees. Comparing those two measures, however, is not straightforward. One may think of relying on some kind of information measure, that can be computed on both regresion and classification trees such as entropy or Kullback-Leiber divergence, but the problem then is how to derive such measure for both the classification and the regression trees."
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#the-car-sales-problem-again",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#the-car-sales-problem-again",
    "title": "Decision Trees Lab 1",
    "section": "The Car Sales problem (again)",
    "text": "The Car Sales problem (again)\nEven if we do not aim at comparing regression and classification problems, the carseats problem proivides a good example on how to build and optimize a regression tree.\nRemember our goal is to predict car sales from a simulated data set containing sales of child car seats at different stores [@james2013introduction]. In order to make sections reproducible, we reload the package and the data.\n\nGet the Data\n\nrequire(ISLR2)\ndata(\"Carseats\")\nmydataset &lt;- Carseats\n\n\n\nCreate train/test sets\nWe split original data into test and training sets. Package resample allows to do a weighted splitting to enbsure that no class is underrepresented due to chance. If sample size is high this can usually be ignored.\n\n# Split the data into training and test sets\nset.seed(2)\npt &lt;- 1/2\ntrain &lt;- sample(1:nrow(mydataset), pt * nrow(mydataset))\nmydataset.test &lt;- mydataset[-train,]\nsales.test &lt;- mydataset$Sales[-train]\n\n\n\nBuild (and check) the model\n\n# Fit the regression tree using the Sales variable\n\ntree.mydataset &lt;- tree(Sales ~ . , mydataset,\n                       subset = train)\n\n# Summary of the fitted regression tree\nsummary(tree.mydataset)\n\n\nRegression tree:\ntree(formula = Sales ~ ., data = mydataset, subset = train)\nVariables actually used in tree construction:\n[1] \"Price\"       \"ShelveLoc\"   \"CompPrice\"   \"Age\"         \"Advertising\"\n[6] \"Population\" \nNumber of terminal nodes:  14 \nResidual mean deviance:  2.602 = 484 / 186 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-4.71700 -1.08700 -0.01026  0.00000  1.11300  4.06600 \n\n\n\n# Plot the regression tree\nplot(tree.mydataset)\ntext(tree.mydataset, pretty = 0, cex = 0.6)\n\n\n\n\n\n\n\n\n\n\nMake prediction\n\n# Predict using the test data\ntree.pred &lt;- predict(tree.mydataset, mydataset.test)\n\n\n\nEstimate prediction error\nA common measure of prediction error is the Mean Square Error.\nNotice that it is computed from a direct substraction between the predicted sales and the original values in the test subset.\n\nmse1 &lt;- mean((tree.pred - sales.test)^2)\nmse1\n\n[1] 4.471569\n\n\nThe mean squared error obtained from the original tree is 4.4715694.\n\n\nOptimize the tree\nIn order to optimize the trune we first compute the best cost-complexity parameter using cross-validation and then use it to prune the tree.\n\n# Prune the regression tree\nset.seed(123987)\ncv.mydataset &lt;- cv.tree(tree.mydataset, FUN = prune.tree)\nnames(cv.mydataset)\n\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n\ncv.mydataset\n\n$size\n [1] 14 13 12 11 10  9  8  7  6  4  3  2  1\n\n$dev\n [1] 1146.347 1178.392 1178.275 1201.676 1239.316 1217.896 1242.089 1253.068\n [9] 1202.806 1211.749 1206.363 1295.017 1578.720\n\n$k\n [1]      -Inf  16.92509  19.38585  23.44178  29.89370  36.28493  50.16562\n [8]  54.84825  65.75957  80.79945  90.11022 179.77305 277.78708\n\n$method\n[1] \"deviance\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n\nBefore selecting the best \\(\\alpha\\) value it may be useful to plot the MSE as a function of the tree size or of \\(\\alpha\\) itself. Notice that \\(\\alpha\\) is named as “\\(k\\)” in the tree package.\n\n# Plot the cross-validation error\npar(mfrow = c(1, 2))\nplot(cv.mydataset$size, cv.mydataset$dev, type = \"b\")\nplot(cv.mydataset$k, cv.mydataset$dev, type = \"b\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\nIt seems clear that, in this case, the smallest error is attained when the tree is not pruned (size=14), so the “best” value of \\(\\alpha\\) leads to not pruning the tree.\n\n# Choose the best tree size\nmyBest &lt;- cv.mydataset$size[which.min(cv.mydataset$dev)]\n\n# Prune the tree with the best size\nprune.mydataset &lt;- prune.tree(tree.mydataset, \n                              best = myBest)\n\n# Plot the pruned regression tree\nplot(prune.mydataset)\ntext(prune.mydataset, pretty = 0)\n\n\n\n\n\n\n\n# Predict using the pruned tree\nprunedTree.pred &lt;- predict(prune.mydataset, mydataset.test)\n\n# Calculate mean squared error for pruned tree\nprunedMSE &lt;- mean((prunedTree.pred - sales.test)^2)\nprunedMSE\n\n[1] 4.471569\n\n\nIn this case, pruning does not improve the tree and the best tree is the one returned by the initial tun of the algorithm.\nIf however, we look for a compromise between the tree size and the deviance we can choose, based on the cv plots, a size of 6 or even 3:\n\n# Prune the tree with the best size\npruneto5.mydataset &lt;- prune.tree(tree.mydataset, \n                              best = 6)\n\n# Plot the pruned regression tree\nplot(pruneto5.mydataset)\ntext(pruneto5.mydataset, pretty = 0)\n\n\n\n\n\n\n\n# Predict using the pruned tree\nprunedTree5.pred &lt;- predict(pruneto5.mydataset, mydataset.test)\n\n# Calculate mean squared error for pruned tree\nprunedMSE5 &lt;- mean((prunedTree5.pred - sales.test)^2)\nprunedMSE5\n\n[1] 5.001169\n\n\n\n# Prune the tree with the best size\npruneto3.mydataset &lt;- prune.tree(tree.mydataset, \n                              best = 3)\n\n# Plot the pruned regression tree\nplot(pruneto3.mydataset)\ntext(pruneto3.mydataset, pretty = 0)\n\n\n\n\n\n\n\n# Predict using the pruned tree\nprunedTree3.pred &lt;- predict(pruneto3.mydataset, mydataset.test)\n\n# Calculate mean squared error for pruned tree\nprunedMSE3 &lt;- mean((prunedTree3.pred - sales.test)^2)\nprunedMSE3\n\n[1] 6.555128\n\n\nClearly, the best compromise seems to prune with a size of 5, which hardly increases the MSE while providinga good simplification of the tree"
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#predicting-boston-house-prices",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#predicting-boston-house-prices",
    "title": "Decision Trees Lab 1",
    "section": "Predicting Boston house prices",
    "text": "Predicting Boston house prices\nThis example is borrowed from [@amat2017].\nThe Boston dataset available in the MASS package contains housing prices for the city of Boston, as well as socioeconomic information for the neighborhood in which they are located.\n\nlibrary(ISLR2)\ndata(\"Boston\")\ndatos &lt;- Boston\nhead(datos, 3)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat medv\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98 24.0\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14 21.6\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03 34.7\n\n\nOur goal is to fit a regression model that allows predicting the average price of a home (medv) based on the available variables.\nA quick visualization of the available variables shows that, not only they are of mixed types, but also the relation between them is far from linear inmost if not all cases.\n\ncolor &lt;- adjustcolor(\"forestgreen\", alpha.f = 0.5)\nps &lt;- function(x, y, ...) {  # custom panel function\n  panel.smooth(x, y, col = color, col.smooth = \"black\", \n               cex = 0.7, lwd = 2)\n}\nnc&lt;- ncol(datos)\npairs(datos[,c(1:6,nc)], cex = 0.7, upper.panel = ps, col = color)\n\n\n\n\n\n\n\n# pairs(datos[,c(7:14)], cex = 0.7, upper.panel = ps, col = color)\n\nThis is a good scenario to consider regression trees as a good option.\n\nModel fitting\nCreate a train and test sets\n\nset.seed(123)\ntrain &lt;- sample(1:nrow(datos), size = nrow(datos)/2)\ndatos_train &lt;- datos[train,]\ndatos_test  &lt;- datos[-train,]\n\nWe use the tree function of the tree package to build the model. This function grows the tree until it meets a stop condition. By default, these conditions are:\n\nmincut: minimum number of observations that at least one of the child nodes must have for the division to occur.\nminsize: minimum number of observations a node must have in order for it to be split.\n\n\nset.seed(123)\nregTree&lt;- tree::tree(\n                    formula = medv ~ .,\n                    data    = datos_train,\n                    split   = \"deviance\",\n                    mincut  = 20,\n                    minsize = 50\n                  )\nsummary(regTree)\n\n\nRegression tree:\ntree::tree(formula = medv ~ ., data = datos_train, split = \"deviance\", \n    mincut = 20, minsize = 50)\nVariables actually used in tree construction:\n[1] \"rm\"    \"lstat\" \"dis\"   \"tax\"  \nNumber of terminal nodes:  6 \nResidual mean deviance:  20.56 = 5078 / 247 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-14.5500  -2.8680  -0.3628   0.0000   2.0050  22.1300 \n\n\nThe summary shows that the trained tree has a total of 6 terminal nodes and that the variables rm, lstat, dis and tax have been used as predictors.\nIn the context of regression trees, the Residual mean deviance term is the residual sum of squares divided by (number of observations - number of terminal nodes). The smaller the deviance, the better the fit of the tree to the training observations.\nThe tree can be visualized:\n\npar(mar = c(1,1,1,1))\nplot(x = regTree, type = \"proportional\")\ntext(x = regTree, splits = TRUE, pretty = 0, cex = 0.8, col = \"firebrick\")\n\n\n\n\n\n\n\n\n\n\nPrunning the tree\nWe use the cv.tree function that uses cross validation to identify the optimal penalty value. By default, this function relies on the deviance to guide the pruning process.\nWe grow the tree again with less restrictive parameters so we have a big tree to prune:\n\nregTree2&lt;- tree::tree(\n                    formula = medv ~ .,\n                    data    = datos_train,\n                    split   = \"deviance\",\n                    mincut  = 1,\n                    minsize = 2,\n                    mindev  = 0\n                  )\n\n\nset.seed(123)\ncv_regTree2 &lt;- tree::cv.tree(regTree2, K = 5)\n\nThe function returns an object cv_regTree2 containing:\n\nsize: The size (number of terminal nodes) of each tree.\ndev: The cross-validation test error estimate for each tree size.\nk: The range of penalty values \\(\\alpha\\) evaluated.\nmethod: The criteria used to select the best tree.\n\nThese can be used to visualize and understand the optimization performed.\n\noptSize &lt;- rev(cv_regTree2$size)[which.min(rev(cv_regTree2$dev))]\npaste(\"Optimal size obtained is:\", optSize)\n\n[1] \"Optimal size obtained is: 10\"\n\n\n\nlibrary(ggplot2)\nlibrary(ggpubr)\n\n\nresultados_cv &lt;- data.frame(\n                   n_nodes  = cv_regTree2$size,\n                   deviance = cv_regTree2$dev,\n                   alpha    = cv_regTree2$k\n                 )\n\np1 &lt;- ggplot(data = resultados_cv, aes(x = n_nodes, y = deviance)) +\n      geom_line() + \n      geom_point() +\n      geom_vline(xintercept = optSize, color = \"red\") +\n      labs(title = \"Error vs tree size\") +\n      theme_bw() \n  \np2 &lt;- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +\n      geom_line() + \n      geom_point() +\n      labs(title = \"Error vs penalization (alpha)\") +\n      theme_bw() \n\nggarrange(p1, p2)\n\n\n\n\n\n\n\n\nOnce the optimal value identified, the final pruning is applied with the prune.tree function. This function also accepts the optimal value of \\(\\alpha\\) instead of size.\n\nfinalTree &lt;- tree::prune.tree(\n                  tree = regTree2,\n                  best = optSize\n               )\n\npar(mar = c(1,1,1,1))\nplot(x = finalTree, type = \"proportional\")\ntext(x = finalTree, splits = TRUE, pretty = 0, cex = 0.8, col = \"firebrick\")\n\n\n\n\n\n\n\n\n\n\nPredicting and checking model accuracy\nWe can use both, original and pruned trees to predict the data for the test set.\nThe quality of the prediction is based in the Root Mean Square.\nFor the original tree one has:\n\npredicciones &lt;- predict(regTree, newdata = datos_test)\ntest_rmse    &lt;- sqrt(mean((predicciones - datos_test$medv)^2))\npaste(\"Error de test (rmse) del árbol inicial:\", round(test_rmse,2))\n\n[1] \"Error de test (rmse) del árbol inicial: 5.74\"\n\n\nAnd for the final tree:\n\npredicciones_finales &lt;- predict(finalTree, newdata = datos_test)\ntest_rmse    &lt;- sqrt(mean((predicciones_finales - datos_test$medv)^2))\npaste(\"Error de test (rmse) del árbol final:\", round(test_rmse,2))\n\n[1] \"Error de test (rmse) del árbol final: 5.13\"\n\n\nThat is The error associated with the prediction has slightly decreased, while the tree is much simpler.\nThat is what we ideal are aiming at!"
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#comparison-between-caret-rpart-and-tree",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#comparison-between-caret-rpart-and-tree",
    "title": "Decision Trees Lab 1",
    "section": "Comparison between caret, rpart, and tree",
    "text": "Comparison between caret, rpart, and tree\nTwo popular packages for working with decision trees are rpart and tree. Both offer functionalities for building and visualizing decision trees. The table below shows a comparison between the main functions of these packages, as well as caret, which is a generic framework for performing classification and prediction tasks, including trees.\n\nTable: Comparison of important functions for working with decision trees\n\n\n\n\n\n\n\n\n\nFunction / Package\ntree\nrpart\ncaret\n\n\n\n\nBuilding Decision Tree\ntree()\nrpart()\ntrain() with method = “rpart”\n\n\nVisualizing Decision Tree\n-\nplot()\nplot() with type = “text”\n\n\nPruning Decision Tree\ncv.tree()\nprune()\ntrain() with method = “rpart” and tuneLength &gt; 1\n\n\nEvaluating Model Performance\n-\npredict()\ntrain() with method = “rpart” and metric = “Accuracy”\n\n\nHandling Missing Values\nna.action\nna.action\npreProcess() with method = “medianImpute”\n\n\nTuning Hyperparameters\n-\nrpart.control()\ntrain() with method = “rpart” and tuneGrid argument\n\n\nVisualizing Variable Importance\n-\nimportance()\nvarImp()\n\n\n\n\n\nExamples of usage:\n\n\n\n\n\n\n\n\n\nFunction / Package\ntree\nrpart\ncaret\n\n\n\n\nBuilding Decision Tree\ntree(Species ~ ., data = iris)\nrpart(Species ~ ., data = iris)\ntrain(Species ~ ., method = \"rpart\", data = iris)\n\n\nVisualizing Decision Tree\n-\nplot(fit)\nplot(fit, type = \"text\")\n\n\nPruning Decision Tree\ncv.tree(Species ~ ., data = iris)\nprune(fit, cp = 0.02)\ntrain(Species ~ ., method = \"rpart\", data = iris, tuneLength = 5)\n\n\nEvaluating Model Performance\n-\npred &lt;- predict(fit, iris, type = \"class\")\ntrain(Species ~ ., method = \"rpart\", data = iris, metric = \"Accuracy\")\n\n\nHandling Missing Values\ntree(Species ~ ., data = na.omit(iris))\nrpart(Species ~ ., data = na.omit(iris), na.action = na.rpart)\npreProcess(iris, method = \"medianImpute\")\n\n\nTuning Hyperparameters\n-\nrpart(Species ~ ., data = iris, control = rpart.control(cp = c(0.001, 0.01, 0.1)))\ntrain(Species ~ ., method = \"rpart\", data = iris, tuneGrid = expand.grid(cp = c(0.001, 0.01, 0.1)))\n\n\nVisualizing Variable Importance\n-\nimportance(fit)\nvarImp(fit)\n\n\n\nThese examples illustrate how to perform various tasks related to decision trees using the tree, rpart, and caret packages. Each package has its own syntax and set of functions, so they can be used according to the user’s needs and preferences."
  },
  {
    "objectID": "labs/Lab-C1.2-knn4classification.html",
    "href": "labs/Lab-C1.2-knn4classification.html",
    "title": "k-NN for classification",
    "section": "",
    "text": "Introduction\nThe k-nearest neighbors (k-NN) classifier is a non-parametric method used for classification tasks. Given a new observation ( s ), the method assigns it the most frequent class among its ( k ) nearest neighbors in the training set.\nMathematically, the probability estimate of belonging to class 1 at point ( s ) is:\n\\[\n\\hat{P}_1(s) = \\frac{1}{k} \\sum_{i \\in N_k(s)} I(y_i = 1)\n\\]\nwhere: - ( N_k(s) ) represents the set of ( k ) nearest neighbors of ( s ), - ( I(y_i = 1) ) is an indicator function that equals 1 if the observation belongs to class 1 and 0 otherwise.\nThe predicted class is given by:\n\\[\n\\hat{y}(s) = \\arg\\max_{c} \\hat{P}_c(s)\n\\]\nwhere ( _c(s) ) is the estimated probability of class ( c ).\nIn this document we illustrate how the boundaries of the classifer are affected by the tuning parameter \\(k\\).\n\n\nLoading Data\nWe use a simulated dataset consisting of two-dimensional observations (features x and y) along with their class labels (BLUE / ORANGE).\n\ndf.xy &lt;- read.table(file=\"2clas2dim.csv\", dec=\".\", sep=\";\", header = TRUE)\ndim(df.xy)\n\n[1] 200   3\n\ndf.xy[sample(1:nrow(df.xy),10),]\n\n              x           y  class\n49   1.12821955 -0.72956096   BLUE\n36   0.23090193  0.77797760   BLUE\n165 -0.02438679  2.29309266 ORANGE\n60   0.16520982  0.93360084   BLUE\n68   0.35564860  1.20330520   BLUE\n188  1.04171675  2.36471548 ORANGE\n41   0.77926721 -0.09974683   BLUE\n38   0.91652168  1.21631239   BLUE\n98   2.14415047 -2.61251237   BLUE\n35   1.87323917 -2.18541999   BLUE\n\n\nNow let’s see how Knn works for one point\n\nk &lt;- 40\ns &lt;- 0\nt &lt;- 0\nst &lt;- c(s,t)\n\n# Compute distances between (s,t) and all data points\nd_st_xy &lt;- as.matrix(dist(rbind(st, df.xy[,1:2])))[1, -1]\n\n# Identify the k-th smallest distance\nd_st_xy_k &lt;- sort(d_st_xy, partial=k)[k]\n\n# Identify the indices of the k nearest neighbors\nN_st_k &lt;- unname(which(d_st_xy &lt;= d_st_xy_k))\n\n# Compute probability estimate of class 'ORANGE' at (s,t)\n(pr_1_k_st &lt;- sum(df.xy[N_st_k,3] == 'ORANGE') / k)\n\n[1] 0.25\n\n\nThis code: 1. Defines a test point ( (s,t) = (0,0) ). 2. Computes distances to all training points. 3. Selects the ( k ) nearest neighbors. 4. Estimates the probability of class ‘ORANGE’ at ( (s,t) ).\n\n\nVisualization of k-NN Neighborhood\n\nplot(df.xy[,1:2], col=df.xy$class, pch=1 + 18 * ((1:200) %in% N_st_k), asp=1)\npoints(s, t, pch=\"*\", col=3, cex=3)\n\n\n\n\n\n\n\n\nThis plot:\n\nDisplays the dataset with colors representing different classes.\nHighlights the selected neighbors of the test point ( (s,t) ).\n\n\n\nGeneralizing the k-NN Classifier\nThe process can be encapsulated in a function.\n\nknn.class &lt;- function(st, xy, group, k=3) {\n  d_st_xy &lt;- as.matrix(dist(rbind(st, xy)))[1, -1]\n  d_st_xy_k &lt;- sort(d_st_xy, partial=k)[k]\n  N_st_k &lt;- unname(which(d_st_xy &lt;= d_st_xy_k))\n  return(sum(group[N_st_k] == 1) / k)  # Probability of class 1\n}\n\nThis function: - Computes distances between a test point st and all training points xy. - Identifies the ( k ) nearest neighbors. - Estimates the probability of the test point belonging to class 1.\n\n\nUsing the k-NN Classifier\n\nst &lt;- c(0,0)\ngroup &lt;- as.numeric(df.xy[,3] == 'ORANGE')\nknn.class(st=st, xy=df.xy[,1:2], group=group, k=40)\n\n[1] 0.25\n\n\nThis code predicts the class probability for ( (0,0) ).\n\n\nCreating a Probability Map\n\ns &lt;- t &lt;- seq(-3.5, 3.5, by=.1)\nns &lt;- length(s)\nnt &lt;- length(t)\nhat_p &lt;- matrix(0, nrow=ns, ncol=nt)\n\nk &lt;- 50\n\nfor (i in 1:ns) {\n  for (j in 1:nt) {\n    hat_p[i, j] &lt;- knn.class(st=c(s[i], t[j]), xy=df.xy[,1:2], group=group, k=k)\n  }\n}\n\nThis block: - Defines a grid of test points in the feature space. - Computes class probabilities for each grid point using k-NN.\n\n\nDecision Boundary Visualization\n\nplot(df.xy[,1], df.xy[,2], col=df.xy[,3], asp=1)\ncontour(s, t, hat_p, levels=.5, lwd=2, add=TRUE)\n\n\n\n\n\n\n\n\nThis plot: - Shows the decision boundary where the class probability is 0.5.\n\n\nComparison with Logistic Regression\n\n# Fit logistic regression model\nglm.class &lt;- glm(group ~ x + y, data=df.xy, family=binomial)\nb012 &lt;- coefficients(glm.class)\n\n# Create a new plot to ensure abline() has a base plot\nplot(df.xy[,1], df.xy[,2], col=df.xy[,3], asp=1, main=\"Logistic Regression Decision Boundary\")\n\n# Add decision boundary line\nabline(a = -b012[1]/b012[3], b = -b012[2]/b012[3], lwd=2, col=6)\n\n\n\n\n\n\n\n\nThis fits a logistic regression model and plots its decision boundary for comparison.\n\n\nK-NN classifier is also a flexible classifier\nWe observe that: - k-NN produces a flexible decision boundary, adapting to the local structure of the data. - Larger ( k ) values create smoother boundaries, reducing variance. - Logistic regression assumes a linear boundary, which may not be suitable for non-linear patterns."
  },
  {
    "objectID": "labs/Lab-C.2.4b-BoostingOptimization.html",
    "href": "labs/Lab-C.2.4b-BoostingOptimization.html",
    "title": "Ensembles Lab 2: Boosting",
    "section": "",
    "text": "# Helper packages\nlibrary(dplyr)       # for data wrangling\nlibrary(ggplot2)     # for awesome plotting\nlibrary(modeldata)  \nlibrary(foreach)     # for parallel processing with for loops\n\n# Modeling packages\n# library(tidymodels)\nlibrary(xgboost)\nlibrary(gbm)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Boosting Optimization Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4b-BoostingOptimization.html#ames-housing-dataset",
    "href": "labs/Lab-C.2.4b-BoostingOptimization.html#ames-housing-dataset",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Ames Housing dataset",
    "text": "Ames Housing dataset\nPackge AmesHousing contains the data jointly with some instructions to create the required dataset.\nWe will use, however data from the modeldata package where some preprocessing of the data has already been performed (see: https://www.tmwr.org/ames)\nThe dataset has 74 variables so a descriptive analysis is not provided.\n\ndim(ames)\n\n[1] 2930   74\n\nboxplot(ames)\n\n\n\n\n\n\n\n\nWe proceed as in the previous lab and divide the reponse variable by 1000 facilitate reviewing the results .\n\nrequire(dplyr)\names &lt;- ames %&gt;% mutate(Sale_Price = Sale_Price/1000)\nboxplot(ames)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Boosting Optimization Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4b-BoostingOptimization.html#spliting-the-data-into-testtrain",
    "href": "labs/Lab-C.2.4b-BoostingOptimization.html#spliting-the-data-into-testtrain",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Spliting the data into test/train",
    "text": "Spliting the data into test/train\nThe data are split in separate test / training sets and do it in such a way that samplig is balanced for the response variable, Sale_Price.\n\n# Stratified sampling with the rsample package\nset.seed(123)\nsplit &lt;- rsample::initial_split(ames, prop = 0.7, \n                       strata = \"Sale_Price\")\names_train  &lt;- training(split)\names_test   &lt;- testing(split)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Boosting Optimization Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4b-BoostingOptimization.html#tree-number",
    "href": "labs/Lab-C.2.4b-BoostingOptimization.html#tree-number",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Tree number",
    "text": "Tree number\nThis is a critical parameter as far as adding new trees increases risk of overfitting.\nBefore optimization is run, data is shaped into an object of class xgb.DMatrix, which is required to run XGBoost through this package.\n\names_train_num &lt;- model.matrix(Sale_Price ~ . , data = ames_train)[,-1]\names_test_num &lt;- model.matrix(Sale_Price ~ . , data = ames_test)[,-1]\n\ntrain_labels &lt;- ames_train$Sale_Price\ntest_labels &lt;- ames_test$Sale_Price\n\names_train_matrix &lt;- xgb.DMatrix(\n  data = ames_train_num,\n  label = train_labels\n)\n\names_test_matrix &lt;- xgb.DMatrix(\n  data = ames_test_num,\n  label = test_labels\n)\n\n\nboostResult_cv &lt;- xgb.cv(\n  data = ames_train_matrix,\n  params = list(eta = 0.3, max_depth = 6, subsample = 1, objective = \"reg:squarederror\"),\n  nrounds = 500,\n  nfold = 5,\n  metrics = \"rmse\", \n  verbose = 0\n)\n\nboostResult_cv &lt;- boostResult_cv$evaluation_log\nprint(boostResult_cv)\n\n      iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n     &lt;num&gt;           &lt;num&gt;          &lt;num&gt;          &lt;num&gt;         &lt;num&gt;\n  1:     1    1.412915e+02   0.9570592195      141.81926      3.756892\n  2:     2    1.019455e+02   0.7941089688      103.61188      3.395008\n  3:     3    7.430242e+01   0.7320607454       77.11700      3.556748\n  4:     4    5.479365e+01   0.6093311366       59.41119      3.735165\n  5:     5    4.118576e+01   0.5362387078       48.04453      4.272442\n ---                                                                  \n496:   496    7.383272e-03   0.0006022381       29.16183      4.817437\n497:   497    7.293321e-03   0.0005722715       29.16184      4.817432\n498:   498    7.164931e-03   0.0005468412       29.16184      4.817431\n499:   499    7.053888e-03   0.0005474267       29.16184      4.817429\n500:   500    6.977090e-03   0.0005256163       29.16187      4.817472\n\n\nWe aim at at the lowest number of trees that has associated a small cross-validation error.\n\nggplot(data = boostResult_cv) +\n  geom_line(aes(x = iter, y = train_rmse_mean, color = \"train rmse\")) + \n  geom_line(aes(x = iter, y = test_rmse_mean, color = \"cv rmse\")) +\n  geom_point(\n    data = slice_min(boostResult_cv, order_by = test_rmse_mean, n = 1),\n    aes(x = iter, y = test_rmse_mean),\n    color = \"firebrick\"\n  ) +\n  labs(\n    title = \"Evolution of cv-error vs number of trees\",\n    x     = \"number of trees\",\n    y     = \"cv-error (rmse)\",\n    color = \"\"\n  ) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\npaste(\"Optimal number of rounds (nrounds):\", slice_min(boostResult_cv, order_by = test_rmse_mean, n = 1)$iter)\n\n[1] \"Optimal number of rounds (nrounds): 77\"",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Boosting Optimization Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4b-BoostingOptimization.html#learning-rate",
    "href": "labs/Lab-C.2.4b-BoostingOptimization.html#learning-rate",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Learning rate",
    "text": "Learning rate\nAlongside the number of trees, the learning rate (eta) is the most crucial hyperparameter in Gradient Boosting. It controls how quickly the model learns and thus influences the risk of overfitting.\nThese two hyperparameters are interdependent: a lower learning rate requires more trees to achieve good results but reduces the risk of overfitting.\n\n# Rango de valores para la tasa de aprendizaje (eta)\neta_range &lt;- c(0.001, 0.01, 0.1, 0.3)\ndf_results_cv &lt;- data.frame()\n\nfor (i in seq_along(eta_range)) {\n  set.seed(123)\n  \n  # Validación cruzada con el eta actual\n  results_cv &lt;- xgb.cv(\n    data = ames_train_matrix,  # ✅ Usamos el xgb.DMatrix correcto\n    params = list(\n      eta = eta_range[i], \n      max_depth = 6, \n      subsample = 1, \n      objective = \"reg:squarederror\"\n    ),\n    nrounds = 1000,\n    nfold = 5,\n    metrics = \"rmse\", \n    verbose = 0\n  )\n  \n  # Extraer la evaluación de RMSE y registrar resultados\n  results_cv &lt;- results_cv$evaluation_log\n  results_cv &lt;- results_cv %&gt;%\n    select(iter, test_rmse_mean) %&gt;%\n    mutate(eta = as.character(eta_range[i]))  # Guardamos el eta usado\n  \n  df_results_cv &lt;- df_results_cv %&gt;% bind_rows(results_cv)\n}\n\n\nggplot(data = df_results_cv) +\n  geom_line(aes(x = iter, y = test_rmse_mean, color = eta)) + \n  labs(\n    title = \"Evolución del error en validación cruzada vs tasa de aprendizaje (eta)\",\n    x = \"Número de iteraciones\",\n    y = \"Error RMSE en validación cruzada\",\n    color = \"Eta\"\n  ) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Boosting Optimization Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4b-BoostingOptimization.html#optimized-predictor",
    "href": "labs/Lab-C.2.4b-BoostingOptimization.html#optimized-predictor",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Optimized predictor",
    "text": "Optimized predictor\nIn order to obtain improved predictors one can perform a a grid search for the best parameter combination can be performed.\n\n# Convertir variables categóricas a dummy variables usando model.matrix()\names_train_num &lt;- model.matrix(Sale_Price ~ . , data = ames_train)[,-1]\names_test_num &lt;- model.matrix(Sale_Price ~ . , data = ames_test)[,-1]\n\n# Extraer etiquetas de Sale_Price\ntrain_labels &lt;- ames_train$Sale_Price\ntest_labels &lt;- ames_test$Sale_Price\n\n# Convertir a xgb.DMatrix\names_train_matrix &lt;- xgb.DMatrix(\n  data = ames_train_num,\n  label = train_labels\n)\n\names_test_matrix &lt;- xgb.DMatrix(\n  data = ames_test_num,\n  label = test_labels\n)\n\n# Range of parameter values to test\neta_values &lt;- c(0.01, 0.05, 0.1, 0.3)  \nnrounds_values &lt;- c(500, 1000, 2000)  \n\nbest_rmse &lt;- Inf\nbest_params &lt;- list()\n\n\ncv_results_df &lt;- data.frame()\n\nset.seed(123)\n\nfor (eta in eta_values) {\n  for (nrounds in nrounds_values) {\n    \n    cv_results &lt;- xgb.cv(\n      data = ames_train_matrix,  \n      params = list(\n        eta = eta,\n        max_depth = 6,\n        subsample = 0.8,\n        colsample_bytree = 0.8,\n        objective = \"reg:squarederror\"\n      ),\n      nrounds = nrounds,\n      nfold = 5,\n      metrics = \"rmse\",\n      verbose = 0,\n      early_stopping_rounds = 10\n    )\n\n    if (is.null(cv_results)) next\n    \n    results_row &lt;- data.frame(\n      eta = eta,\n      nrounds = nrounds,\n      min_rmse = min(cv_results$evaluation_log$test_rmse_mean),\n      best_nrounds = cv_results$evaluation_log$iter[which.min(cv_results$evaluation_log$test_rmse_mean)]\n    )\n    \n    cv_results_df &lt;- bind_rows(cv_results_df, results_row)\n\n    if (results_row$min_rmse &lt; best_rmse) {\n      best_rmse &lt;- results_row$min_rmse\n      best_params &lt;- list(\n        eta = results_row$eta,\n        nrounds = results_row$best_nrounds\n      )\n    }\n  }\n}\n\n\ncat(\"\\n, Best hyperparameters values found:\\n\")\n\n\n, Best hyperparameters values found:\n\ncat(\"Eta:\", best_params$eta, \"\\n\")\n\nEta: 0.01 \n\ncat(\"Nrounds:\", best_params$nrounds, \"\\n\")\n\nNrounds: 1000 \n\ncat(\"RMSE mínimo:\", round(best_rmse, 4), \"\\n\")\n\nRMSE mínimo: 24.4476",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Boosting Optimization Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Boosting.html",
    "href": "labs/Lab-C.2.4-Boosting.html",
    "title": "Ensembles Lab 2: Boosting",
    "section": "",
    "text": "# Helper packages\nlibrary(dplyr)       # for data wrangling\nlibrary(ggplot2)     # for awesome plotting\nlibrary(modeldata)  \nlibrary(foreach)     # for parallel processing with for loops\n\n# Modeling packages\n# library(tidymodels)\nlibrary(xgboost)\nlibrary(gbm)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Boosting Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Boosting.html#ames-housing-dataset",
    "href": "labs/Lab-C.2.4-Boosting.html#ames-housing-dataset",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Ames Housing dataset",
    "text": "Ames Housing dataset\nPackge AmesHousing contains the data jointly with some instructions to create the required dataset.\nWe will use, however data from the modeldata package where some preprocessing of the data has already been performed (see: https://www.tmwr.org/ames)\nThe dataset has 74 variables so a descriptive analysis is not provided.\n\ndim(ames)\n\n[1] 2930   74\n\nboxplot(ames)\n\n\n\n\n\n\n\n\nWe proceed as in the previous lab and divide the reponse variable by 1000 facilitate reviewing the results .\n\nrequire(dplyr)\names &lt;- ames %&gt;% mutate(Sale_Price = Sale_Price/1000)\nboxplot(ames)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Boosting Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Boosting.html#spliting-the-data-into-testtrain",
    "href": "labs/Lab-C.2.4-Boosting.html#spliting-the-data-into-testtrain",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Spliting the data into test/train",
    "text": "Spliting the data into test/train\nThe data are split in separate test / training sets and do it in such a way that samplig is balanced for the response variable, Sale_Price.\n\n# Stratified sampling with the rsample package\nset.seed(123)\nsplit &lt;- rsample::initial_split(ames, prop = 0.7, \n                       strata = \"Sale_Price\")\names_train  &lt;- training(split)\names_test   &lt;- testing(split)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Boosting Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Boosting.html#xgboost-parameters-overview",
    "href": "labs/Lab-C.2.4-Boosting.html#xgboost-parameters-overview",
    "title": "Ensembles Lab 2: Boosting",
    "section": "XGBoost Parameters Overview",
    "text": "XGBoost Parameters Overview\nThe xgboost() function in the XGBoost package trains Gradient Boosting models for regression and classification tasks. Key parameters and hyperparameters include:\n\nParameters:\n\nparams: List of training parameters.\ndata: Training data.\nnrounds: Number of boosting rounds.\nwatchlist: Validation set for early stopping.\nobj: Custom objective function.\nfeval: Custom evaluation function.\nverbose: Verbosity level.\nprint_every_n: Print frequency.\nearly_stopping_rounds: Rounds for early stopping.\nmaximize: Maximize evaluation metric.\nsave_period: Model save frequency.\nsave_name: Name for saved model.\nxgb_model: Existing XGBoost model.\ncallbacks: List of callback functions.\n\n\n\nXGBoost Parameters Overview\nNumerous parameters govern XGBoost’s behavior. A detailed description of all parameters can be found in the XGBoost documentation. Key considerations include those controlling tree growth, model learning rate, and early stopping to prevent overfitting:\n\nParameters:\n\nbooster [default = gbtree]: Type of weak learner, trees (“gbtree”, “dart”) or linear models (“gblinear”).\neta [default=0.3, alias: learning_rate]: Reduces each tree’s contribution by multiplying its original influence by this value.\ngamma [default=0, alias: min_split_loss]: Minimum cost reduction required for a split to occur.\nmax_depth [default=6]: Maximum depth trees can reach.\nsubsample [default=1]: Proportion of observations used for each tree’s training. If less than 1, applies Stochastic Gradient Boosting.\ncolsample_bytree: Number of predictors considered at each split.\nnrounds: Number of boosting iterations, i.e., the number of models in the ensemble.\nearly_stopping_rounds: Number of consecutive iterations without improvement to trigger early stopping. If NULL, early stopping is disabled. Requires a separate validation set (watchlist) for early stopping.\nwatchlist: Validation set used for early stopping.\nseed: Seed for result reproducibility. Note: use set.seed() instead.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Boosting Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Boosting.html#test-training-in-xgboost",
    "href": "labs/Lab-C.2.4-Boosting.html#test-training-in-xgboost",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Test / Training in xGBoost",
    "text": "Test / Training in xGBoost\n\nXGBoost Data Formats\nXGBoost models can work with various data formats, including R matrices.\nHowever, it’s advisable to use xgb.DMatrix, a specialized and optimized data structure within this library.\n\names_train &lt;- xgb.DMatrix(\n                data  = ames_train %&gt;% select(-Sale_Price)\n                %&gt;% data.matrix(),\n                label = ames_train$Sale_Price,\n               )\n\names_test &lt;- xgb.DMatrix(\n                data  = ames_test %&gt;% select(-Sale_Price)\n                %&gt;% data.matrix(),\n                label = ames_test$Sale_Price\n               )",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Boosting Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Boosting.html#fit-the-model",
    "href": "labs/Lab-C.2.4-Boosting.html#fit-the-model",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Fit the model",
    "text": "Fit the model\n\nset.seed(123)\names.boost &lt;- xgb.train(\n            data    = ames_train,\n            params  = list(max_depth = 2),\n            nrounds = 1000,\n            eta= 0.05\n          )\names.boost\n\n##### xgb.Booster\nraw: 872.5 Kb \ncall:\n  xgb.train(params = list(max_depth = 2), data = ames_train, nrounds = 1000, \n    eta = 0.05)\nparams (as set within xgb.train):\n  max_depth = \"2\", eta = \"0.05\", validate_parameters = \"1\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.print.evaluation(period = print_every_n)\n# of features: 73 \nniter: 1000\nnfeatures : 73",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Boosting Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Boosting.html#prediction-and-model-assessment",
    "href": "labs/Lab-C.2.4-Boosting.html#prediction-and-model-assessment",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Prediction and model assessment",
    "text": "Prediction and model assessment\n\names.boost.trainpred &lt;- predict(ames.boost,\n                   newdata = ames_train\n                 )\n\names.boost.pred &lt;- predict(ames.boost,\n                   newdata = ames_test\n                 )\n\ntrain_rmseboost &lt;- sqrt(mean((ames.boost.trainpred - getinfo(ames_train, \"label\"))^2))\n\ntest_rmseboost &lt;- sqrt(mean((ames.boost.pred - getinfo(ames_test, \"label\"))^2))\n\npaste(\"Error train (rmse) in XGBoost:\", round(train_rmseboost,2))\n\n[1] \"Error train (rmse) in XGBoost: 14.31\"\n\npaste(\"Error test (rmse) in XGBoost:\", round(test_rmseboost,2))\n\n[1] \"Error test (rmse) in XGBoost: 22.69\"",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Boosting Lab"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "This web page and its associated github repository is intended to provide materials (slides, scripts datasets etc) for some of the units[^1] of the Statistical Learning course at the UPC-UB MSc in Statistics and Operations Research (MESIO).\n[^1:] Specifically the materials in this page are about chapters:\n\nIntroduction\nTree and Ensemble Methods\nArtificial Neural Networks..\n\nThe main reason for creating this page, apart of making it open and facilitatigt the hierrchjichal presentation of the materials, is that some labs may take time to run. So, given that it is not an option to run them in class we provide links to the HTML files obtaind when running the R or Python notebooks provided.\nBy the other side, in order to make the materials completely reproducible, the page is based on a github repository, which can be cloned or downladed in order for anyone who wishes to play around with the materials and reproduce or improve the examples.\n\n\n\nCourse presentation\n1.1 Overview of Supervised Learning\n1.2 Model validation and Resampling\nR-labs\n\nRegression with KNN\nClassification with KNN\n\nComplements\n\nIntroduction to biomarkers and diagnostic tests\n\n\n\n\n\n\n\nDecision trees are a type of non-parametric classifiers which have been Very successful because of their interpretability, flexibility and a very decent accuracy.\n\n2.1-DecisionTrees-Slides\nR-labs\n\nLab_1- Classification and Regression Trees\n\nPython-labs\n\nLab_1- Decision Trees lab (from ISL. Ch 08)\n\n\n\n\n\nThe term “Ensemble” (together in french) refers to distinct approaches to build predictiors by combining multiple models.\nThey have proved to addres well some limitations of trees therefore improving accuracy and robustness as well as being able to reduce overfitting and capture complex relationships.\n\n2.2-Ensemble Methods. Slides\nR-Labs\n\nEnsemble Lab 1 (Random Forest)\nEnsemble Lab 2 (Boosting)\nEnsemble Lab 2b (Boosting Optimization)\nEnsemble Lab 3 (Caret)\n\n\n\n\n\n\n\n\nThese are raditional ML models, inspired in brain, that simulate neuron behavior, thata is they receive an input, which is processed and an output prediction is produced.\nFor long their applicability has been relatively restricted to a few fields or problems due mainly to their “black box” functioning that made them hard to interpret.\nThe scenario has now completely changed with the advent of deep neural networks which are in the basis of many powerful applications of artificial intelligence.\n\nNeural Networks Slides\nLabs\n\nNeuralNets Lab (Rmd version)\nNeuralNets Lab (Python notebook\n\n\n\n\n\nEsssentially these are ANN with multiple hidden layers with allow overpassing many of their limitations.\nThey can be tuned in a much more automatical way and have been applied to many complex tasks. such as Computer vision, Natural Language Processing or Recommender systems.\n\nDeep learning Slides\n\nDeepLearning Lab (Rmd version)\nDeepLearning Lab (Python notebook",
    "crumbs": [
      "Home",
      "Introduction",
      "Home"
    ]
  },
  {
    "objectID": "index.html#introduction-to-statistical-learning",
    "href": "index.html#introduction-to-statistical-learning",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "Course presentation\n1.1 Overview of Supervised Learning\n1.2 Model validation and Resampling\nR-labs\n\nRegression with KNN\nClassification with KNN\n\nComplements\n\nIntroduction to biomarkers and diagnostic tests",
    "crumbs": [
      "Home",
      "Introduction",
      "Home"
    ]
  },
  {
    "objectID": "index.html#tree-based-methods",
    "href": "index.html#tree-based-methods",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "Decision trees are a type of non-parametric classifiers which have been Very successful because of their interpretability, flexibility and a very decent accuracy.\n\n2.1-DecisionTrees-Slides\nR-labs\n\nLab_1- Classification and Regression Trees\n\nPython-labs\n\nLab_1- Decision Trees lab (from ISL. Ch 08)\n\n\n\n\n\nThe term “Ensemble” (together in french) refers to distinct approaches to build predictiors by combining multiple models.\nThey have proved to addres well some limitations of trees therefore improving accuracy and robustness as well as being able to reduce overfitting and capture complex relationships.\n\n2.2-Ensemble Methods. Slides\nR-Labs\n\nEnsemble Lab 1 (Random Forest)\nEnsemble Lab 2 (Boosting)\nEnsemble Lab 2b (Boosting Optimization)\nEnsemble Lab 3 (Caret)",
    "crumbs": [
      "Home",
      "Introduction",
      "Home"
    ]
  },
  {
    "objectID": "index.html#artifical-neural-networks",
    "href": "index.html#artifical-neural-networks",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "These are raditional ML models, inspired in brain, that simulate neuron behavior, thata is they receive an input, which is processed and an output prediction is produced.\nFor long their applicability has been relatively restricted to a few fields or problems due mainly to their “black box” functioning that made them hard to interpret.\nThe scenario has now completely changed with the advent of deep neural networks which are in the basis of many powerful applications of artificial intelligence.\n\nNeural Networks Slides\nLabs\n\nNeuralNets Lab (Rmd version)\nNeuralNets Lab (Python notebook\n\n\n\n\n\nEsssentially these are ANN with multiple hidden layers with allow overpassing many of their limitations.\nThey can be tuned in a much more automatical way and have been applied to many complex tasks. such as Computer vision, Natural Language Processing or Recommender systems.\n\nDeep learning Slides\n\nDeepLearning Lab (Rmd version)\nDeepLearning Lab (Python notebook",
    "crumbs": [
      "Home",
      "Introduction",
      "Home"
    ]
  },
  {
    "objectID": "index.html#references-for-tree-based-methods",
    "href": "index.html#references-for-tree-based-methods",
    "title": "Introduction to Statistical Learning",
    "section": "References for Tree based methods",
    "text": "References for Tree based methods\n\nBreiman, L., Friedman, J., Stone, C. J., & Olshen, R. A. (1984). Classification and regression trees. CRC press.\nBrandon M. Greenwell (202) Tree-Based Methods for Statistical Learning in R. 1st Edition. Chapman and Hall/CRC DOI: https://doi.org/10.1201/9781003089032 Web site\nEfron, B., Hastie T. (2016) Computer Age Statistical Inference. Cambridge University Press. Web site\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: Data mining, inference, and prediction. Springer.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112). Springer.",
    "crumbs": [
      "Home",
      "Introduction",
      "Home"
    ]
  },
  {
    "objectID": "index.html#references-for-deep-neural-networks",
    "href": "index.html#references-for-deep-neural-networks",
    "title": "Introduction to Statistical Learning",
    "section": "References for deep neural networks",
    "text": "References for deep neural networks\n\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning (Vol. 1). MIT press. Web site\nLeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444.\nChollet, F. (2018). Deep learning with Python. Manning Publications.\nChollet, F. (2023). Deep learning with R . 2nd edition. Manning Publications.",
    "crumbs": [
      "Home",
      "Introduction",
      "Home"
    ]
  },
  {
    "objectID": "index.html#some-interesting-online-resources",
    "href": "index.html#some-interesting-online-resources",
    "title": "Introduction to Statistical Learning",
    "section": "Some interesting online resources",
    "text": "Some interesting online resources\n\nDecision Trees free course (9 videos). By Analytics Vidhya\nApplied Data Mining and Statistical Learning (Penn Statte-University)\nR for statistical learning\nAn Introduction to Recursive Partitioning Using the RPART Routines\nIntroduction to Artificial Neural Networks\n\nThis page has been created as Quarto Website project.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.",
    "crumbs": [
      "Home",
      "Introduction",
      "Home"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#historical-background-1",
    "href": "C3.1-Introduction_to_ANN-Slides.html#historical-background-1",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Historical Background (1)",
    "text": "Historical Background (1)\n\nIn the post-pandemic world, a lightning rise of AI, with a mess of realities and promises is impacting society.\nSince ChatGPT entered the scene everybody has an experience, an opinion, or a fear on the topic.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#is-it-just-machine-learning",
    "href": "C3.1-Introduction_to_ANN-Slides.html#is-it-just-machine-learning",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Is it just machine learning?",
    "text": "Is it just machine learning?\n\nMost tasks performed by AI can be described as Classification or Prediction used in applications as:\n\nRecommendation systems,\nImage recognition, Image generation\nNatural language processing\n\nAI relies on machine learning algorithms, to make predictions based on large amounts of data.\nAI has far-reaching implications beyond its predictive capabilities, including ethical, social or technological.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#ai-anns-and-deep-learning",
    "href": "C3.1-Introduction_to_ANN-Slides.html#ai-anns-and-deep-learning",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "AI, ANNs and Deep learning",
    "text": "AI, ANNs and Deep learning\n\nIn many contexts, talking about AI means talking about Deep Learning (DL).\nDL is a successful AI model which has powered many application such as self-driving cars, voice assistants, and medical diagnosis systems.\nDL originates in the field of Artificial Neural Networks\nBut DL extends the basic principles of ANNs by:\n\nAdding complex architectures and algorithms and\nAt the same time becoming more automatic",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#the-early-history-of-ai-1",
    "href": "C3.1-Introduction_to_ANN-Slides.html#the-early-history-of-ai-1",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "The early history of AI (1)",
    "text": "The early history of AI (1)\n\nA Quick History of AI, ML and DL",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#milestones-in-the-history-of-dl",
    "href": "C3.1-Introduction_to_ANN-Slides.html#milestones-in-the-history-of-dl",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Milestones in the history of DL",
    "text": "Milestones in the history of DL\nWe can see several hints worth to account for:\n\nThe Perceptron and the first Artificial Neural Network where the basic building block was introduced.\nThe Multilayered perceptron and back-propagation where complex architectures were suggested to improve the capabilities.\nDeep Neural Networks, with many hidden layers, and auto-tunability capabilities.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#from-ann-to-deep-learning",
    "href": "C3.1-Introduction_to_ANN-Slides.html#from-ann-to-deep-learning",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "From ANN to Deep learning",
    "text": "From ANN to Deep learning\n\nWhy Deep Learning Now?\nSource: Alex Amini’s MIT Introduction to Deep Learning’ course",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#success-stories",
    "href": "C3.1-Introduction_to_ANN-Slides.html#success-stories",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Success stories",
    "text": "Success stories\nSuccess stories such as\n\nthe development of self-driving cars,\nthe use of AI in medical diagnosis, and\nonline shopping personalized recommendations\n\nhave also contributed to the widespread adoption of AI.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#not-to-talk-abou-the-fears",
    "href": "C3.1-Introduction_to_ANN-Slides.html#not-to-talk-abou-the-fears",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Not to talk abou the fears",
    "text": "Not to talk abou the fears\n\n\n\nAI also comes with fears from multiple sources from science fiction to religion\n\nMass unemployment\nLoss of privacity\nAI bias\nAI fakes\nOr, simply, AI takeover",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#back-to-science",
    "href": "C3.1-Introduction_to_ANN-Slides.html#back-to-science",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Back to science",
    "text": "Back to science\nWhere/How does it all fit?",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#ai-ml-dl",
    "href": "C3.1-Introduction_to_ANN-Slides.html#ai-ml-dl",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "AI, ML, DL …",
    "text": "AI, ML, DL …\n\nArtificial intelligence: Ability of a computer to perform tasks commonly associated with intelligent beings.\nMachine learning: study of algorithms that learn from examples and experience instead of relying on hard-coded rules and make predictions on new data\nDeep learning: sub field of ML focusing on learning data representations as successive successive layers of increasingly meaningful representations.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#how-does-dl-improve",
    "href": "C3.1-Introduction_to_ANN-Slides.html#how-does-dl-improve",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "How does DL improve",
    "text": "How does DL improve\n\nML and DL Approaches for Brain Disease Diagnosis\n\nDNN: feature extraction and classification without (or with much les) human intervention.\nDNN improves with data availability, without seemingly reaching plateaus.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#size-does-matter",
    "href": "C3.1-Introduction_to_ANN-Slides.html#size-does-matter",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Size does matter!",
    "text": "Size does matter!\n\nAn illustration of the performance comparison between deep learning (DL) and other machine learning (ML) algorithms, where DL modeling from large amounts of data can increase the performance",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#the-impact-of-deep-learning",
    "href": "C3.1-Introduction_to_ANN-Slides.html#the-impact-of-deep-learning",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "The impact of Deep learning",
    "text": "The impact of Deep learning\n\nNear-human-level image classification\nNear-human-level speech transcription\nNear-human-level handwriting transcription\nDramatically improved machine translation\nDramatically improved text-to-speech conversion\nDigital assistants such as Google Assistant and Amazon Alexa\nNear-human-level autonomous driving\nImproved ad targeting, as used by Google, Baidu, or Bing\nImproved search results on the web\nAbility to answer natural language questions\nSuperhuman Go playing",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#not-all-that-glitters-is-gold",
    "href": "C3.1-Introduction_to_ANN-Slides.html#not-all-that-glitters-is-gold",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Not all that glitters is gold …",
    "text": "Not all that glitters is gold …\n\nAccording to F. Chollet, the developer of Keras,\n\n“we shouldn’t believe the short-term hype, but should believe in the long-term vision.\nIt may take a while for AI to be deployed to its true potential—a potential the full extent of which no one has yet dared to dream\nbut AI is coming, and it will transform our world in a fantastic way”.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#emulating-biological-neurons",
    "href": "C3.1-Introduction_to_ANN-Slides.html#emulating-biological-neurons",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Emulating biological neurons",
    "text": "Emulating biological neurons\n\n\n\n\n\n\n\nA biological Neuron\n\n\n\n\n\n\n\n\n\n\nMuCulloch & Pitts proposal\n\n\n\n\n\n\nThe first model of an artifial neurone was proposed by Mc Cullough & Pitts in 1943",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#mc-culloughs-neuron",
    "href": "C3.1-Introduction_to_ANN-Slides.html#mc-culloughs-neuron",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Mc Cullough’s neuron",
    "text": "Mc Cullough’s neuron\n\nIt may be divided into 2 parts.\n\nThe first part, \\(g\\),takes an input (as the dendrites of a neuron would do),\nIt performs an aggregation and\nbased on the aggregated value the second part, \\(f\\), makes a decision.\n\n\nSee the source of this picture for an illustration on how this can be used to emulate logical operations such as AND, OR or NOT, but not XOR.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#limitations",
    "href": "C3.1-Introduction_to_ANN-Slides.html#limitations",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Limitations",
    "text": "Limitations\nThis first attempt to emulate neurons succeeded but with limitations:\n\nWhat about non-Boolean (say, real) inputs?\nWhat if all inputs are not equal?\nWhat if we want to assign more importance to some inputs?\nWhat about functions which are not linearly separable? Say XOR function",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#overcoming-the-limitations",
    "href": "C3.1-Introduction_to_ANN-Slides.html#overcoming-the-limitations",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Overcoming the limitations",
    "text": "Overcoming the limitations\n\nTo overcome these limitations Rosenblatt, proposed the perceptron model, or artificial neuron, in 1958.\nGeneralizes McCullough-Pitts neuron in that weights and thresholds can be learnt over time.\n\nIt takes a weighted sum of the inputs and\nIt sets the output to iff the sum is more than an arbitrary threshold (\\(\\theta\\)).",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#rosenblatts-perceptron",
    "href": "C3.1-Introduction_to_ANN-Slides.html#rosenblatts-perceptron",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Rosenblatt’s perceptron",
    "text": "Rosenblatt’s perceptron",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#rosenblatts-perceptron-1",
    "href": "C3.1-Introduction_to_ANN-Slides.html#rosenblatts-perceptron-1",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Rosenblatt’s perceptron",
    "text": "Rosenblatt’s perceptron\n\n\nInstead of hand coding the thresholding parameter \\(\\theta\\),\nIt is added as one of the inputs, with the weight \\(w_0=-\\theta\\).",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#comparison-between-the-two",
    "href": "C3.1-Introduction_to_ANN-Slides.html#comparison-between-the-two",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Comparison between the two",
    "text": "Comparison between the two",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#comparison-between-the-two-1",
    "href": "C3.1-Introduction_to_ANN-Slides.html#comparison-between-the-two-1",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Comparison between the two",
    "text": "Comparison between the two\n\nThis is an improvement because\n\nboth, weights and threshold, can be learned and\nthe inputs can be real values\n\nBut there is still a drawback in that a single perceptron can only be used to implement linearly separable functions.\nArtificial Neural Networks improve on this by introducing Activation Functions",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#activation-in-biological-neurons",
    "href": "C3.1-Introduction_to_ANN-Slides.html#activation-in-biological-neurons",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Activation in biological neurons",
    "text": "Activation in biological neurons\n\nBiological neurons are specialized cells that transmit signals to communicate with each other.\nNeuron’s activation is based on releasing neurotransmitters, chemicals that transmit signals between nerve cells.\n\nWhen the signal reaching the neuron exceeds a certain threshold, it releases neurotransmitters to continue the communication process.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#activation-functions-in-an",
    "href": "C3.1-Introduction_to_ANN-Slides.html#activation-functions-in-an",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Activation functions in AN",
    "text": "Activation functions in AN\n\nAnalogously, activation functions in AN are functions to decide if the AN it is activated or not.\nAN’s activation function is a mathematical function applied to the neuron’s input to produce an output.\n\nIn practice it extends to complicated functions that can learn complex patterns in the data.\nActivation functions can incorporate non-linearity, improving over linear classifiers.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#activation-function",
    "href": "C3.1-Introduction_to_ANN-Slides.html#activation-function",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Activation function",
    "text": "Activation function",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#artificial-neuron",
    "href": "C3.1-Introduction_to_ANN-Slides.html#artificial-neuron",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Artificial Neuron",
    "text": "Artificial Neuron\nWith all these ideas in mind we can now define an Artificial Neuron as a computational unit that :\n\ntakes as input \\(x=(x_0,x_1,x_2,x_3),\\ (x_0 = +1 \\equiv bias)\\),\noutputs \\(h_{\\theta}(x) = f(\\theta^\\intercal x) = f(\\sum_i \\theta_ix_i)\\),\nwhere \\(f:\\mathbb{R}\\mapsto \\mathbb{R}\\) is called the activation function.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#activation-functions",
    "href": "C3.1-Introduction_to_ANN-Slides.html#activation-functions",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Activation functions",
    "text": "Activation functions\n\n\nGoal of activation function is to provide the neuron with the capability of producing the required outputs.\nFlexible enough to produce\n\nEither linear or non-linear transformations.\nOutput in the desired range ([0,1], {-1,1}, \\(\\mathbb{R}^+\\)…)\n\nUsually chosen from a (small) set of possibilities.\n\nSigmoid function\nHyperbolic tangent, or tanh, function\nReLU",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#the-sigmoid-function",
    "href": "C3.1-Introduction_to_ANN-Slides.html#the-sigmoid-function",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "The sigmoid function",
    "text": "The sigmoid function\n\n\n\\[\nf(z)=\\frac{1}{1+e^{-z}}\n\\]\n\nOutput real values \\(\\in (0,1)\\).\nNatural interpretations as probability",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#the-hyperbolic-tangent",
    "href": "C3.1-Introduction_to_ANN-Slides.html#the-hyperbolic-tangent",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "the hyperbolic tangent",
    "text": "the hyperbolic tangent\n\n\nAlso called tanh, function:\n\\[\nf(z)=\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}\n\\]\n\noutputs are zero-centered and bounded in −1,1\nscaled and shifted Sigmoid\nstronger gradient but still has vanishing gradient problem\nIts derivative is \\(f'(z)=1-(f(z))^2\\).",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#the-relu",
    "href": "C3.1-Introduction_to_ANN-Slides.html#the-relu",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "The ReLU",
    "text": "The ReLU\n\n\n\nrectified linear unit: \\(f(z)=\\max\\{0,z\\}\\).\nClose to a linear: piece-wise linear function with two linear pieces.\nOutputs are in %(0,)$ , thus not bounded\nHalf rectified: activation threshold at 0\nNo vanishing gradient problem",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#more-activation-functions",
    "href": "C3.1-Introduction_to_ANN-Slides.html#more-activation-functions",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "More activation functions",
    "text": "More activation functions\n.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#putting-it-all-together",
    "href": "C3.1-Introduction_to_ANN-Slides.html#putting-it-all-together",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Putting it all together",
    "text": "Putting it all together",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#in-words",
    "href": "C3.1-Introduction_to_ANN-Slides.html#in-words",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "In words",
    "text": "In words\n\nAn ANN takes a vector of input values \\(x_{1}, \\ldots, x_{d}\\) and combines it with some weights that are local to the neuron \\(\\left(w_{0}, w_{1}, . ., w_{d}\\right)\\) to compute a net input \\(w_{0}+\\sum_{i=1}^{d} w_{i} \\cdot x_{i}\\).\nTo compute its output, it then passes the net input through a possibly non-linear univariate activation function \\(g(\\cdot)\\), usually vchosen from a set of options such as Sigmoid, Tanh or ReLU functions\nTo deal with the bias, we create an extra input variable \\(x_{0}\\) with value always equal to 1 , and so the function computed by a single artificial neuron (parameterized by its weights \\(\\mathbf{w}\\) ) is:\n\n\\[\ny(\\mathbf{x})=g\\left(w_{0}+\\sum_{i=1}^{d} w_{i} x_{i}\\right)=g\\left(\\sum_{i=0}^{d} w_{i} x_{i}\\right)=g\\left(\\mathbf{w}^{\\mathbf{T}} \\mathbf{x}\\right)\n\\]",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#the-basic-neural-network",
    "href": "C3.1-Introduction_to_ANN-Slides.html#the-basic-neural-network",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "The basic neural network",
    "text": "The basic neural network\nFollowing with the brain analogy one can combine (artificial) neurons to create better learners.\nA simple artificial neural network is usually created by combining two types of modifications to the basic perceptron (AN).\n\nStacking several neurons insteads of just one.\nAdding an additional layer of neurons, which is call a hidden layer,\n\nThis yields a system where the output of a can be the input of another in many different ways.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#an-artificial-neural-network",
    "href": "C3.1-Introduction_to_ANN-Slides.html#an-artificial-neural-network",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "An Artificial Neural network",
    "text": "An Artificial Neural network",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#the-architecture-of-ann",
    "href": "C3.1-Introduction_to_ANN-Slides.html#the-architecture-of-ann",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "The architecture of ANN",
    "text": "The architecture of ANN\nIn this figure, we have used circles to also denote the inputs to the network.\n\nCircles labeled +1 are bias units, and correspond to the intercept term.\nThe leftmost layer of the network is called the input layer.\nThe rightmost layer of the network is called the output layer.\nThe middle layer of nodes is called the hidden layer, because its values are not observed in the training set.\n\nBias nodes are not counted when stating the neuron size.\nWith all this in mind our example neural network has three layers with:\n\n3 input units (not counting the bias unit),\n3 hidden units,\n\n1 output unit.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#how-an-ann-works",
    "href": "C3.1-Introduction_to_ANN-Slides.html#how-an-ann-works",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "How an ANN works",
    "text": "How an ANN works\nAn ANN is a predictive model (a learner) whose properties and behaviour can be well characterized.\n\n\nIt operates through a process known as forward propagation, which encompasses the information flow from the input layer to the output layer.\nForward propagation is performed by composing a series of linear and non-linear (activation) functions.\nThese are characterized (parametrized) by their weights and biases, that need to be learnt.\n\nThis is done by training the ANN.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#training-the-ann",
    "href": "C3.1-Introduction_to_ANN-Slides.html#training-the-ann",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Training the ANN",
    "text": "Training the ANN\n\nIn order for the ANN to perform well, the training process aims at finding the best possible parameter values for the learning task defined by the fnctions. This is done by\n\nSelecting an appropriate (convex) loss function,\nFinding those weights that minimize a the total cost function (avg. loss).\n\nThis is usually done using some iterative optimization procedure such as gradient descent.\n\nThis requires evaluating derivatives in a huge number of points.\nSuch high number may be reduced by Stochastic Gradient Descent.\nThe evaluation of derivatives is simplified thanks to Backpropagation.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#forward-propagation",
    "href": "C3.1-Introduction_to_ANN-Slides.html#forward-propagation",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Forward propagation",
    "text": "Forward propagation\nAs described above the process that encompasses the computations required to go from the input values to the final output is known as forward propagation.\nThe weights are combined with the input to produce the final output.\nEach node, \\(a_i^{(2)}\\) of the hidden layer opperates on all nodes of the input values\n\\[\\begin{eqnarray}\na_1^{(2)}&=&f(\\theta_{10}^{(1)}+\\theta_{11}^{(1)}x_1+\\theta_{12}^{(1)}x_2+\\theta_{13}^{(1)}x_3)\\\\\na_2^{(2)}&=&f(\\theta_{20}^{(1)}+\\theta_{21}^{(1)}x_1+\\theta_{22}^{(1)}x_2+\\theta_{23}^{(1)}x_3)\\\\\na_3^{(2)}&=&f(\\theta_{30}^{(1)}+\\theta_{31}^{(1)}x_1+\\theta_{32}^{(1)}x_2+\\theta_{33}^{(1)}x_3))\n\\end{eqnarray}\\]\nThe output of the hidden layer is transformed through the activation function:\n\\[\nh_{\\Theta}(x)=a_1^{(3)}=f(\\theta_{10}^{(2)}+\\theta_{11}^{(2)}a_1^{(2)}+\\theta_{12}^{(2)}a_2^{(2)}+\\theta_{13}^{(2)}a_3^{(2)}\n\\]",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#a-compact-notation-1",
    "href": "C3.1-Introduction_to_ANN-Slides.html#a-compact-notation-1",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "A compact notation (1)",
    "text": "A compact notation (1)\nLet \\(z_i^{(l)}\\) denote the total weighted sum of inputs to unit \\(i\\) in layer \\(l\\):\n\\[\nz_i^{(2)}=\\theta_{i0}^{(1)}+\\theta_{i1}^{(1)}x_1+\\theta_{i2}^{(1)}x_2+\\theta_{i3}^{(1)}x_3,\n\\] the output becomes: \\(a_i^{(l)}=f(z_i^{(l)})\\).\nExtending the activation function \\(f(\\cdot)\\) to apply elementwise to vectors:\n\\[\n    f([z_1,z_2,z_3]) = [f(z_1), f(z_2),f(z_3)],\n\\] we can write the previous equations more compactly as:\n\\[\\begin{eqnarray}\nz^{(2)}&=&\\Theta^{(1)}x\\nonumber\\\\\na^{(2)}&=&f(z^{(2)})\\nonumber\\\\\nz^{(3)}&=&\\Theta^{(2)}a^{(2)}\\nonumber\\\\\nh_{\\Theta}(x)&=&a^{(3)}=f(z^{(3)})\\nonumber\n\\end{eqnarray}\\]",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#a-compact-notation-2",
    "href": "C3.1-Introduction_to_ANN-Slides.html#a-compact-notation-2",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "A compact notation (2)",
    "text": "A compact notation (2)\nMore generally, recalling that we also use \\(a^{(1)}=x\\) to also denote the values from the input layer,\nGiven layer \\(l\\)’s activations \\(a^{(l)}\\), we can compute layer \\(l+1\\)’s activations \\(a^{(l+1)}\\) as:\n\\[\\begin{equation}\nz^{(l+1)}=\\Theta^{(l)}a^{(l)}\n\\label{eqforZs}\n\\end{equation}\\]\n\\[\\begin{equation}\na^{(l+1)}=f(z^{(l+1)})\n\\label{eqforAs}\n\\end{equation}\\]",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#a-compact-notation-3",
    "href": "C3.1-Introduction_to_ANN-Slides.html#a-compact-notation-3",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "A compact notation (3)",
    "text": "A compact notation (3)\nThis can be used to provide a matrix representation for the weighted sum of inputs of all neurons:\n\\[\nz^{(l+1)}=\n\\begin{bmatrix}\nz_1^{(l+1)}\\\\\nz_2^{(l+1)}\\\\\n\\vdots\\\\\nz_{s_{l+1}}^{(l)}\n\\end{bmatrix}=\n\\begin{bmatrix}\n\\theta_{10}^{(l)}& \\theta_{11}^{(l)}&\\theta_{12}^{(l)}&...&\\theta_{1s_{l}}^{(l)}&\\\\\n\\theta_{20}^{(l)}& \\theta_{21}^{(l)}&\\theta_{22}^{(l)}&...&\\theta_{2s_{l}}^{(l)}&\\\\\n\\vdots & \\vdots& \\vdots & \\vdots & \\vdots\\\\\n\\theta_{s_{l+1}0}^{(l)}& \\theta_{s_{l+1}1}^{(l)}&\\theta_{s_{l+1}2}^{(l)}&...&\\theta_{s_{l+1}s_{l}}^{(l)}&\\\\\n\\end{bmatrix}\n\\cdot\\begin{bmatrix}\n1\\\\\na_1^{(l)}\\\\\na_2^{(l)}\\\\\n\\vdots\\\\\na_{s_l}^{(l)}\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#a-compact-notation-4",
    "href": "C3.1-Introduction_to_ANN-Slides.html#a-compact-notation-4",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "A compact notation (4)",
    "text": "A compact notation (4)\nSo that, the activation is then:\n\\[\na^{(l+1)}=\n\\begin{bmatrix}\na_1^{(l+1)}\\\\\na_2^{(l+1)}\\\\\n\\vdots\\\\\na_{s_{l+1}}^{(l)}\n\\end{bmatrix}=f(z^{(l+1)})=\\begin{bmatrix}\nf(z_1^{(l+1)})\\\\\nf(z_2^{(l+1)})\\\\\n\\vdots\\\\\nf(z_{s_{l+1}}^{(l)})\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#eficient-forward-propagation",
    "href": "C3.1-Introduction_to_ANN-Slides.html#eficient-forward-propagation",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Eficient Forward propagation",
    "text": "Eficient Forward propagation\n\nThe way input data is transformed, through a series of weightings and transformations, until the ouput layer is called forward propagation.\nBy organizing parameters in matrices, and using matrix-vector operations, fast linear algebra routines can be used to perform the required calculations in a fast efficent way.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#multiple-architectures-for-ann",
    "href": "C3.1-Introduction_to_ANN-Slides.html#multiple-architectures-for-ann",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Multiple architectures for ANN",
    "text": "Multiple architectures for ANN\n\nWe have so far focused on a single hidden layer neural network of the example.\nOne can. however build neural networks with many distinct architectures (meaning patterns of connectivity between neurons), including ones with multiple hidden layers.\nSee here the Neural Network Zoo.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#multiple-architectures-for-ann-1",
    "href": "C3.1-Introduction_to_ANN-Slides.html#multiple-architectures-for-ann-1",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Multiple architectures for ANN",
    "text": "Multiple architectures for ANN\n\n\n\nWe have so far focused on a single hidden layer neural network of the example\nOne can build neural networks with many distinct architectures (meaning patterns of connectivity between neurons), including ones with multiple hidden layers.\n\n\n The Neural Network Zoo",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#multiple-layer-dense-networks",
    "href": "C3.1-Introduction_to_ANN-Slides.html#multiple-layer-dense-networks",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Multiple layer dense Networks",
    "text": "Multiple layer dense Networks\n\nMost common choice is a \\(n_l\\)-layered network:\n\nlayer 1 is the input layer,\nlayer \\(n_l\\) is the output layer,\nand each layer \\(l\\) is densely connected to layer \\(l+1\\).\n\nIn this setting, to compute the output of the network, we can compute all the activations in layer \\(L_2\\), then layer \\(L_3\\), and so on, up to layer \\(L_{nl}\\), using equations seen previously.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#feed-forward-nns",
    "href": "C3.1-Introduction_to_ANN-Slides.html#feed-forward-nns",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Feed Forward NNs",
    "text": "Feed Forward NNs\n\nThe type of NN described is called feed-forward neural network (FFNN), since\n\nAll computations are done by Forward propagation\nThe connectivity graph does not have any directed loops or cycles.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#training-an-ann",
    "href": "C3.1-Introduction_to_ANN-Slides.html#training-an-ann",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Training an ANN",
    "text": "Training an ANN\n\nAn ANN is a predictive model whose properties and behaviour can be mathematically characterized.\nIn practice this means:\n\nThe ANN acts by composing a series of linear and non-linear (activation) functions.\nThese are characterized by their weights and biases, that need to be learnt .\n\nTraining the network is done by\n\nSelecting an appropriate (convex) loss function,\nFinding those weights that minimize a the total cost function (avg loss).",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#the-tools-for-training",
    "href": "C3.1-Introduction_to_ANN-Slides.html#the-tools-for-training",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "The tools for training",
    "text": "The tools for training\n\nTraining an ANN is usually done using some iterative optimization procedure such as Gradient Descent.\nThis requires evaluating derivatives in a huge number of points.\n\nSuch high number may be reduced by Stochastic Gradient Descent.\nThe evaluation of derivatives is simplified thanks to Backpropagation.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#a-loss-function-for-optimization",
    "href": "C3.1-Introduction_to_ANN-Slides.html#a-loss-function-for-optimization",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "A loss function for optimization",
    "text": "A loss function for optimization\nDepending on the form of the activation function we may decide to use one or another form of loss function.\n\nAt first an idea may be to use squared error loss: \\[\n  l(h_\\theta(x),y)=\\left (y-\\frac{1}{1+e^{-\\theta^\\intercal x}}\\right )^2\n\\]\nHowever it happens to be that, given a sigmoid activation function, the resulting loss function * is not a convex problem* which means that MSE is not appropriate.\nQuadratic loss may be used, for instance, with ReLu activation.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#illustrating-non-convexity",
    "href": "C3.1-Introduction_to_ANN-Slides.html#illustrating-non-convexity",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Illustrating non-convexity",
    "text": "Illustrating non-convexity\n\nlibrary(plot3D)\n\n# Define the squared error loss function\nsquared_error &lt;- function(y, y_hat) {\n  return(0.5 * (y - y_hat)^2)\n}\n\n# Define the logistic activation function\nlogistic &lt;- function(z) {\n  return(1 / (1 + exp(-z)))\n}\n\n# Generate data\nx &lt;- seq(-10, 10, length.out = 200)\ny &lt;- seq(-10, 10, length.out = 200)\nz &lt;- outer(x, y, FUN = function(x, y) squared_error(1, logistic(x + y)))\n\n# Plot the exaggerated loss surface\npersp3D(x, y, z, theta = 30, phi = 30, expand = 0.5, col = \"lightblue\",\n        ticktype = \"detailed\", xlab = \"Weight 1\", ylab = \"Weight 2\", zlab = \"Loss\")",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#illustrating-non-convexity-1",
    "href": "C3.1-Introduction_to_ANN-Slides.html#illustrating-non-convexity-1",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Illustrating non-convexity",
    "text": "Illustrating non-convexity",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#cross-entropy-loss-function",
    "href": "C3.1-Introduction_to_ANN-Slides.html#cross-entropy-loss-function",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Cross-entropy loss function",
    "text": "Cross-entropy loss function\n\\[\n    l(h_\\theta(x),y)=\\big{\\{}\\begin{array}{ll}\n    -\\log h_\\theta(x) & \\textrm{if }y=1\\\\\n    -\\log(1-h_\\theta(x))& \\textrm{if }y=0\n    \\end{array}\n\\]\nThis function can also be written as:\n\\[\nl(h_\\theta(x),y)=-y\\log h_\\theta(x) - (1-y)\\log(1-h_\\theta(x))\n\\]\nUsing cross-entropy loss, the cost function is of the form:\n\\[\\begin{eqnarray*}\n    J(\\theta)=-\\frac{1}{n}\\big[\\sum_{i=1}^n&&(y^{(i)}\\log h_\\theta(x^{(i)})+\\\\ &&(1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))\\big]\n\\end{eqnarray*}\\]\nNow, this is a convex optimization problem.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#regularized-cross-entropy",
    "href": "C3.1-Introduction_to_ANN-Slides.html#regularized-cross-entropy",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Regularized cross entropy",
    "text": "Regularized cross entropy\nIn practice we often work with a regularized version of the cost function (we don’t regularize the bias units)\n\\[\\begin{eqnarray*}\nJ(\\Theta)&=&-\\frac{1}{n}\\big[\\sum_{i=1}^n \\sum_{k=1}^K y_k^{(i)}\\log( h_\\theta(x^{(i)}))_k\\\\\n&+&(1-y_k^{(i)})\\log(1-(h_\\theta(x^{(i)}))_k)\\big]\\\\\n&+&\\lambda\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\n(\\theta_{ji}^{(l)})^2\n\\end{eqnarray*}\\]",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent",
    "href": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nTraining a network corresponds to choosing the parameters, that is, the weights and biases, that minimize the cost function.\nThe weights and biases take the form of matrices and vectors, but at this stage it is convenient to imagine them stored as a single vector that we call \\(\\theta\\).\nGenerally, we will suppose \\(\\theta\\in\\mathbb{R}^p\\), and write the cost function as \\(J(\\theta)\\) to emphasize its dependence on the parameters. So Cost \\(J: \\mathbb{R}^p\\rightarrow \\mathbb{R}\\).",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent-1",
    "href": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent-1",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nError hyper-surface",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent-2",
    "href": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent-2",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nGradient Descent is a classical method in optimization that is often referred to as steepest descent.\nGiven a function \\(J(\\theta)\\) to be minimized, the method proceeds iteratively, computing a sequence of vectors \\(\\theta^1, \\theta^2, ..., \\theta^n\\) in \\(\\mathbb{R}^p\\) with the aim of converging to a vector that minimizes the cost function.\nSuppose that our current vector is \\(\\theta\\). How should we choose a perturbation, \\(\\Delta\\theta\\), so that the next vector, \\(\\theta+\\Delta\\theta\\), represents an improvement, that is: \\(J(\\theta +\\Delta\\theta) &lt; J(\\theta)\\)?",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent-3",
    "href": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent-3",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nWe proceed by linearization of the cost function using a Taylor approximation\nIf \\(\\Delta\\theta\\) is small, then ignoring terms of order \\(||\\Delta\\theta||^2\\) or higher:\n\n\\[\nJ(\\theta+\\Delta\\theta)\\approx J(\\theta)+\\sum_{i=1}^p\\frac{\\partial J(\\theta)}{\\partial\\theta_i}\\Delta\\theta_i\n\\]\n\nwhere \\({\\displaystyle \\frac{\\partial J(\\theta)}{\\partial\\theta_i}}\\) denotes the partial derivative of the cost function with respect to the \\(i\\)-th weight.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent-4",
    "href": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent-4",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nLet \\(\\nabla J(\\theta)\\in\\mathbb{R}^p\\) denote the gradient, i.e. the vector of partial derivatives.\n\n\\[\\begin{equation}\\label{g1}\n\\nabla J(\\theta)=\\left(\\frac{\\partial J(\\theta)}{\\partial\\theta_1},...,\\frac{\\partial J(\\theta)}{\\partial\\theta_p}\\right)^\\intercal\n\\end{equation}\\]\n\nNow the approximation can be written as:\n\n\\[\\begin{equation}\\label{g2}\nJ(\\theta+\\Delta\\theta)\\approx J(\\theta)+\\nabla J(\\theta)^\\intercal\\Delta\\theta\n\\end{equation}\\]",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent-5",
    "href": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent-5",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nRecalling that our aim is to reduce the value of the cost function,\nTaylor approximation above motivates the idea of choosing \\(\\Delta\\theta\\) to make \\(\\nabla J(\\theta)^\\intercal\\Delta\\theta\\) negative,\nbecause this will make the value of \\(J(\\theta+\\Delta\\theta)\\) smaller.\nThe bigger in absolute value we can make this negative expression, the smaller will be the value of the cost function.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#the-cauchy-schwarz-inequality",
    "href": "C3.1-Introduction_to_ANN-Slides.html#the-cauchy-schwarz-inequality",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "The Cauchy-Schwarz inequality",
    "text": "The Cauchy-Schwarz inequality\n\nThe Cauchy-Schwarz inequality, states that for any \\(f,g\\in\\mathbb{R}^p\\), we have: \\[\n|f^\\intercal g|\\leq ||f||\\cdot ||g||.\n\\]\nMoreover, the two sides are equal if and only if \\(f\\) and \\(g\\) are linearly dependent (meaning they are parallel).",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#back-to-gradient-descent",
    "href": "C3.1-Introduction_to_ANN-Slides.html#back-to-gradient-descent",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Back to gradient descent",
    "text": "Back to gradient descent\n\nHow much can \\(\\nabla J(\\theta)^\\intercal\\Delta\\theta\\) decrease?\nBy Cauchy-Schwarz,biggest possible value for \\(\\nabla J(\\theta)^\\intercal\\Delta\\theta\\) is the upper bound, \\(||\\nabla J(\\theta)||\\cdot ||\\Delta\\theta||\\).\n\nThe equality is only reached when \\(||\\nabla J(\\theta)||= ||\\Delta\\theta||\\)\nThe highest possible negative value will come out when \\(-\\nabla J(\\theta)=\\Delta\\theta\\)\n\nThat is, we should choose \\(\\Delta\\theta\\) to lie in the direction of \\(-\\nabla J(\\theta)\\).",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent-6",
    "href": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent-6",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nKeeping in mind that the Taylor linearization of \\(J(\\theta)\\) is an approximation that is relevant only for small \\(\\Delta\\theta\\),\nWe will limit ourselves to a small step in that direction, defined by the learning rate. \\(\\eta\\).\nThis leads to the update formula that defines the steepest descent method.\n\n\\[\\begin{equation}\\label{g3}\n\\theta \\rightarrow \\theta-\\eta\\nabla J(\\theta)\n\\end{equation}\\]",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent-7",
    "href": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent-7",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nIn summary, givent a cost function \\(J(\\theta)\\) to be optimized the gradient descent optimization proceeds as follows:\n\nInitialize \\(\\theta_0\\) randomly or with some predetermined values\nRepeat until convergence: \\[\n\\theta_{t+1} = \\theta_{t} - \\eta \\nabla J(\\theta_{t})\n\\]\nStop when: \\(|J(\\theta_{t+1}) - J(\\theta_{t})| &lt; \\epsilon\\)\n\n\n\\(\\theta_0\\) is the initial parameter vector,\n\\(\\theta_t\\) is the parameter vector at iteration \\(t\\),\n\\(\\eta\\) is the learning rate,\n\\(\\nabla J(\\theta_{t})\\) is the gradient of the loss function with respect to \\(\\theta\\) at iteration \\(t\\),\n\\(\\epsilon\\) is a small positive value indicating the desired level of convergence.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#computing-gradients",
    "href": "C3.1-Introduction_to_ANN-Slides.html#computing-gradients",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Computing gradients",
    "text": "Computing gradients\n\nThe gradient method provides a way to optimize weights and biases \\((\\theta =\\{W, b\\})\\) by minimizing the cost function, \\(J(\\theta)\\).\nThis minimization requires, of course, the computation of an important number of partial derivatives \\[\n\\frac{\\partial}{\\partial\\theta_j}J(\\theta)\n\\]\nThe algorithm used to perform these computation is known as the backpropagation algorithm",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#a-short-history",
    "href": "C3.1-Introduction_to_ANN-Slides.html#a-short-history",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "A short history",
    "text": "A short history\n\nThe backpropagation algorithm was originally introduced in the 1970s in a MSc thesis.\nIn 1986 a paper by Rumelhart, Hinton, and Williams describes several neural networks where backpropagation works far faster than earlier approaches to learning.\nThis improvement in efficiency made it possible to use neural nets to solve problems which had previously been insoluble.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#error-backpropagation",
    "href": "C3.1-Introduction_to_ANN-Slides.html#error-backpropagation",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Error backpropagation",
    "text": "Error backpropagation\n\\[\n\\begin{equation*}\n\\Delta w=\\eta(c-y) d \\tag{3.21}\n\\end{equation*}\n\\]\n\nLa regla delta es la base de la retropropagación:\n\n\\(c\\) es el valor que indica la clase en el ejemplo de entrenamiento,\n\\(y\\) el valor de la función de salida para la instancia \\(d\\), y\n\\(\\eta\\) la tasa o velocidad de aprendizaje (learning rate).\n\nEl método utilizado para entrenar este tipo de redes, y basado en la regla delta, se muestra en el algoritmo siguiente (Algoritmo 1).\n\nPara que el algoritmo converja es necesario que las clases de los datos sean linealmente separables.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#algoritmo-1-gradiente",
    "href": "C3.1-Introduction_to_ANN-Slides.html#algoritmo-1-gradiente",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Algoritmo 1 (Gradiente)",
    "text": "Algoritmo 1 (Gradiente)\n\nEntrada: \\(W\\) (conjunto de vectores de pesos) y \\(D\\) (conjunto de instancias de entrenamiento)\n\nmientras \\(\\left(y \\not \\approx c_{i} \\forall\\left(d_{i}, c_{i}\\right) \\in D\\right)\\) hacer\n\npara todo \\(\\left(\\left(d_{i}, c_{i}\\right) \\in D\\right)\\) hacer\n\nCalcular la salida \\(y\\) de la red cuando la entrada es \\(d_{i}\\)\nsi \\(\\left(y \\not \\approx c_{i}\\right)\\) entonces\n\nModificar el vector de pesos \\(w^{\\prime}=w+\\eta(c-y) d_{i}\\)\n\nfin si\n\nfin para\n\nfin mientras\n\n\nDevolver: El conjunto de vectores de pesos \\(W\\)",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#retropropagación-del-error",
    "href": "C3.1-Introduction_to_ANN-Slides.html#retropropagación-del-error",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Retropropagación del error",
    "text": "Retropropagación del error\n\nEn las redes multicapa no podemos aplicar el algoritmo de entrenamiento visto en la sección anterior.\nEl problema aparece con los nodos de las capas ocultas: no podemos saber a priori cuáles son los valores de salida correctos.\nEn el caso de una neurona \\(j\\) con función sigmoide, la regla delta es:\n\n\\[\n\\begin{equation*}\n\\Delta w_{i}^{j}=\\eta \\sigma^{\\prime}\\left(z^{j}\\right)\\left(c^{j}-y^{j}\\right) x_{i}^{j}, \\tag{3.24}\n\\end{equation*}\n\\]\ndonde:\n\n\\(\\sigma^{\\prime}\\left(z^{j}\\right)\\) indica la pendiente (derivada) de la función sigmoide, que representa el factor con que el nodo \\(j\\) puede afectar al error.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#ajustando-los-pesos",
    "href": "C3.1-Introduction_to_ANN-Slides.html#ajustando-los-pesos",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Ajustando los pesos",
    "text": "Ajustando los pesos\n\\[\n\\begin{equation*}\n\\Delta w_{i}^{j}=\\eta \\sigma^{\\prime}\\left(z^{j}\\right)\\left(c^{j}-y^{j}\\right) x_{i}^{j}, \\tag{3.24}\n\\end{equation*}\n\\]\n\nSi el valor de \\(\\sigma^{\\prime}\\left(z^{j}\\right)\\) es pequeño, nos encontramos en los extremos de la función, donde los cambios no afectan demasiado a la salida.\nPor el contrario, si el valor es grande, nos encontramos en el centro de la función, donde pequeñas variaciones pueden alterar considerablemente la salida.\n\n\\(\\left(c^{j}-y^{j}\\right)\\) representa la medida del error que se produce en la neurona \\(j\\).\n\\(x_{i}^{j}\\) indica la responsabilidad de la entrada \\(i\\) de la neurona \\(j\\) en el error.\n\nCuando este valor es igual a cero, no se modifica el peso, mientras que si es superior a cero, se modifica proporcionalmente a este valor.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#expresion-general-de-delta",
    "href": "C3.1-Introduction_to_ANN-Slides.html#expresion-general-de-delta",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Expresion general de \\(delta\\)",
    "text": "Expresion general de \\(delta\\)\n\nLa \\(\\delta\\) que corresponde a la neurona \\(j\\) puede expresarse de forma general para toda neurona, simplificando la notación anterior:\n\n\\[\n\\begin{gather*}\n\\delta^{j}=\\sigma^{\\prime}\\left(z^{j}\\right)\\left(c^{j}-y^{j}\\right)  \\tag{3.25}\\\\\n\\Delta w_{i}^{j}=\\eta \\delta^{j} x_{i}^{j} \\tag{3.26}\n\\end{gather*}\n\\]\n\nA partir de aquí debemos determinar qué parte del error total se asigna a cada una de las neuronas de las capas ocultas.\nEs decir, debemos definir cómo modificar los pesos y tasas de aprendizaje de las neuronas de las capas ocultas a partir del error observado en la capa de salida.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#la-retropropagación",
    "href": "C3.1-Introduction_to_ANN-Slides.html#la-retropropagación",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "La retropropagación",
    "text": "La retropropagación\n\nEl método de retropropagación (backpropagation) se basa en un esquema general de dos pasos:\n\n\nPropagación hacia adelante (feedforward), que consiste en introducir una instancia de entrenamiento y obtener la salida de la red neuronal.\nPropagación hacia atrás (backpropagation), que consiste en calcular el error cometido en la capa de salida y propagarlo hacia atrás para calcular los valores delta de las neuronas de las capas ocultas.\n\n\nLa idea que subyace a este método es relativamente sencilla, y se basa en propagar el error de forma proporcional a la influencia que ha tenido cada nodo de las capas ocultas en el error final producido por cada una de las neuronas de la capa de salida.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#el-algoritmo-1",
    "href": "C3.1-Introduction_to_ANN-Slides.html#el-algoritmo-1",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "El algoritmo (1)",
    "text": "El algoritmo (1)\n\nEl primer paso, que es la propagación hacia adelante, consiste en aplicar el ejemplo a la red y obtener los valores de salida (línea 3 ).\nA continuación, el método inicia la propagación hacia atrás, empezando por la capa de salida. Para cada neurona \\(j\\) de la capa de salida:\n\n2.1 Se calcula, en primera instancia, el valor \\(\\delta^{j}\\) basado en el valor de salida de la red para la neurona \\(j\\) \\(\\left(y^{j}\\right)\\), el valor de la clase de la instancia \\(\\left(c^{j}\\right)\\) y la derivada de la función sigmoide \\(\\left(\\sigma^{\\prime}\\left(z^{j}\\right)\\right)\\) (línea 5).\n2.2 A continuación, se modifica el vector de pesos de la neurona de la capa de salida, a partir de la tasa de aprendizaje \\((\\eta)\\), el valor delta de la neurona calculado en el paso anterior \\(\\left(\\delta^{j}\\right)\\) y el factor \\(x_{i}^{j}\\) que indica la responsabilidad de la entrada \\(i\\) de la neurona \\(j\\) en el error (línea 6 ).",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#el-algoritmo-2",
    "href": "C3.1-Introduction_to_ANN-Slides.html#el-algoritmo-2",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "El algoritmo (2)",
    "text": "El algoritmo (2)\n\nFinalmente, la propagación hacia atrás se aplica a las capas ocultas de la red. Para cada neurona \\(k\\) de las capas ocultas:\n\n3.1 En primer lugar, se calcula el valor \\(\\delta^{k}\\) basado en la derivada de la función sigmoide \\(\\left(\\sigma^{\\prime}\\left(z^{k}\\right)\\right)\\) y el sumatorio del producto de la delta calculada en el paso anterior \\(\\left(\\delta^{k}\\right)\\) por el valor \\(w_{k}^{j}\\), que indica el peso de la conexión entre la neurona \\(k\\) y la neurona \\(j\\) (línea 9). El conjunto \\(S_{k}\\) está formado por todos los nodos de salida a los que se encuentra conectada la neurona \\(k\\).\n3.2 En el último paso de la iteración, se modifica el vector de pesos de la neurona \\(k\\), a partir de la tasa de aprendizaje \\((\\eta)\\), el valor delta de la neurona calculado en el paso anterior \\(\\left(\\delta^{k}\\right)\\) y el factor \\(x_{i}^{k}\\) que indica la responsabilidad de la entrada \\(k\\) de la neurona \\(j\\) en el error (línea 10).",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#el-algoritmo",
    "href": "C3.1-Introduction_to_ANN-Slides.html#el-algoritmo",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "El algoritmo",
    "text": "El algoritmo",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#forward-propagation-example",
    "href": "C3.1-Introduction_to_ANN-Slides.html#forward-propagation-example",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Forward propagation example",
    "text": "Forward propagation example",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#backpropagation-illustrated",
    "href": "C3.1-Introduction_to_ANN-Slides.html#backpropagation-illustrated",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Backpropagation illustrated",
    "text": "Backpropagation illustrated",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#learning-optimization",
    "href": "C3.1-Introduction_to_ANN-Slides.html#learning-optimization",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Learning optimization",
    "text": "Learning optimization\n\nThe learning porocess such as it has been derived may be improved in different ways.\n\nPredictions can be be bad and require improvement.\nComputations may be inefficent or slow.\nThe network may overfit and lack generalizability.\n\nThis can be partially soved applying distinct approaches.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#network-architechture",
    "href": "C3.1-Introduction_to_ANN-Slides.html#network-architechture",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Network architechture",
    "text": "Network architechture\n\nNetwork performance is affected by many hyperparameters\n\nNetwork topology\nNumber of layers\nNumber of neurons per layer\nActivation function(s)\nWeights initialization procedure\netc.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#hyperparameter-tuning",
    "href": "C3.1-Introduction_to_ANN-Slides.html#hyperparameter-tuning",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\n\nHyperparameters selection and tuning may be hard, due simply to dimensionality.\nStandard approaches to search for best parameters combinations are used.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#how-many-hidden-layers",
    "href": "C3.1-Introduction_to_ANN-Slides.html#how-many-hidden-layers",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "How many (hidden) layers",
    "text": "How many (hidden) layers\n\nTraditionally considered that one layer may be enough\n\nShallow Networks\n\nPosterior research showed that adding more layers increases efficency\n\nNumber of neurons per layer decreases exponentially\n\nAlthough there is also risk of overfitting",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#epochs-and-iterations",
    "href": "C3.1-Introduction_to_ANN-Slides.html#epochs-and-iterations",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Epochs and iterations",
    "text": "Epochs and iterations\n\nIt has been shown that using the whole training set only once may not be enough for training an ANN.\nOne iteration of the training set is known as an epoch.\nThe number of epochs \\(N_E\\), defines how many times we iterate along the whole training set.\n\\(N_E\\) can be fixed, determined by cross-validation or left open and stop the training when it does not improve anymore.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#iterations-and-batches",
    "href": "C3.1-Introduction_to_ANN-Slides.html#iterations-and-batches",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Iterations and batches",
    "text": "Iterations and batches\n\nA complementary strategy to increasing the number of epochs is decreasing the number of instances in each iteration.\nThat is, the training set is broken in a number of batches that are trained separately.\n\nBatch learning allows weights to be updated more frequently per epoch.\nThe advantage of batch learning is related to the gradient descent approach used.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#training-in-batches",
    "href": "C3.1-Introduction_to_ANN-Slides.html#training-in-batches",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Training in batches",
    "text": "Training in batches",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent-drawbacks",
    "href": "C3.1-Introduction_to_ANN-Slides.html#gradient-descent-drawbacks",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Gradient descent drawbacks",
    "text": "Gradient descent drawbacks\n\nWith many parameters and many training points, computing the gradient vector at every iteration of the steepest descent method can be time consuming.\n\nIt is mainly due to that we have to sum across all training points.\nWith big data this becomes prohibitive.\n\nSeveral alternatives exist (many indeed).",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#stochastic-gradient",
    "href": "C3.1-Introduction_to_ANN-Slides.html#stochastic-gradient",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Stochastic Gradient",
    "text": "Stochastic Gradient\n\nA much cheaper alternative is to replace the mean of the individual gradients over all training points by the gradient at a single, randomly chosen, point.\nThis leads to the simplest form of the stochastic gradient method:\nChoose an integer \\(i\\) uniformly at random from \\(\\{1,...,n\\}\\) and update \\[\\begin{equation}\\label{g4}\n    \\theta_j=\\theta_j-\\eta\\frac{\\partial}{\\partial\\theta_j}J(\\theta;x^{(i)})\n    \\end{equation}\\]\n\\(x^{(i)}\\) incuded in the notation of \\(J(\\theta;x^{(i)})\\) to remark the dependence.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#rationale-for-sgd",
    "href": "C3.1-Introduction_to_ANN-Slides.html#rationale-for-sgd",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Rationale for SGD",
    "text": "Rationale for SGD\n\nAt each step, the SGD method uses one randomly chosen training point to represent the full training set.\nAs the iteration proceeds, the method sees more training points.\nSo there is some hope that this dramatic reduction in cost-per-iteration will be worthwhile overall.\nNote that, even for very small \\(\\eta\\), the update is not guaranteed to reduce the overall cost function because we traded the mean for a single sample.\nHence, although the phrase stochastic gradient descent is widely used, we prefer to use stochastic gradient.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#batch-gradient-descent",
    "href": "C3.1-Introduction_to_ANN-Slides.html#batch-gradient-descent",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Batch Gradient Descent",
    "text": "Batch Gradient Descent\n\nBatch Gradient Descent computes the error for each example in the training dataset but the model is updated only after evaluating all examples.\nAs a benefits there is computational efficiency, which produces a stable error gradient and a stable convergence.\nAs drawbacks, the stable error gradient can sometimes result in a state of convergence that isn’t the best the model can achieve. It also requires the entire training dataset to be in memory and available to the algorithm.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#mini-batch-gd",
    "href": "C3.1-Introduction_to_ANN-Slides.html#mini-batch-gd",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Mini-Batch GD",
    "text": "Mini-Batch GD\n\nMini-batch Gradient Descent (MBGD) divides the training dataset into small batches to compute error and update model coefficients.\nMBGD balances the robustness of stochastic gradient descent with the efficiency of batch gradient descent, making it a prevalent implementation in deep learning.\nA common batch size for MBGD is 32, although it can vary based on the specific problem and data.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#types-of-gradient-descent",
    "href": "C3.1-Introduction_to_ANN-Slides.html#types-of-gradient-descent",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Types of Gradient Descent",
    "text": "Types of Gradient Descent\n\nSource: Gradient Descent and its Types",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#alternative-optimizers",
    "href": "C3.1-Introduction_to_ANN-Slides.html#alternative-optimizers",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Alternative optimizers",
    "text": "Alternative optimizers\n\nGradient descent can be improved adopting different strategies\nOther optimizers exist that may make the training faster with strategies such as accumulating velocity in relevant directions or adjusting learning rates based on parameter frequency.\nOverall these optimizers enable quicker convergence and better handling of various data characteristics, making them favorable choices over traditional gradient descent.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#momentum",
    "href": "C3.1-Introduction_to_ANN-Slides.html#momentum",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Momentum",
    "text": "Momentum\n\nAccelerates SGD by accumulating momentum in relevant directions, akin to a rolling ball gaining speed downhill.\nIncreases acceleration for dimensions with consistent gradients, reducing updates for changing gradients, leading to faster convergence and reduced oscillation.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#adagrad",
    "href": "C3.1-Introduction_to_ANN-Slides.html#adagrad",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Adagrad",
    "text": "Adagrad\n\nAdapts learning rate based on parameter frequency, making smaller updates for frequent features and larger updates for rare ones, suitable for sparse data.\nEfficient for handling sparse data due to its adaptive learning rates.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#adadelta",
    "href": "C3.1-Introduction_to_ANN-Slides.html#adadelta",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Adadelta",
    "text": "Adadelta\n\nAn extension of Adagrad aiming to mitigate its aggressive, monotonically decreasing learning rate.\nRestricts accumulated past gradients to a fixed-size window, avoiding over-accumulation.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#adam",
    "href": "C3.1-Introduction_to_ANN-Slides.html#adam",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Adam",
    "text": "Adam\n\nCombines momentum optimization with adaptive learning rates, tracking both past gradients’ exponential decay and a moving average of gradients.\nRecent studies suggest caution with adaptive optimization methods like Adam due to potential generalization issues, prompting exploration of alternatives such as Momentum optimization.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#optimizing-the-training-speed",
    "href": "C3.1-Introduction_to_ANN-Slides.html#optimizing-the-training-speed",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Optimizing the training speed",
    "text": "Optimizing the training speed\n\nTraining speed optimization can be based on\n\nWeight inizialization\nChanging learning rate\nUsing efficient cost functions",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#optimizing-to-avoid-overfitting",
    "href": "C3.1-Introduction_to_ANN-Slides.html#optimizing-to-avoid-overfitting",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Optimizing to avoid overfitting",
    "text": "Optimizing to avoid overfitting\nIn order to fight overfitting distinct strategies are common\n\nL2 regularization\nEarly stopping\nDropout\nData augmentation",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#summarizing-optimization",
    "href": "C3.1-Introduction_to_ANN-Slides.html#summarizing-optimization",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Summarizing optimization",
    "text": "Summarizing optimization\n\nSource: Introducción al Deep Learning (UOC)",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#a-predictive-ann",
    "href": "C3.1-Introduction_to_ANN-Slides.html#a-predictive-ann",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "A predictive ANN",
    "text": "A predictive ANN\nWe use the neuralnet package to build a simple neural network to predict if a type of stock pays dividends or not.\n\nif (!require(neuralnet)) \n  install.packages(\"neuralnet\", dep=TRUE)",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#data-for-the-example",
    "href": "C3.1-Introduction_to_ANN-Slides.html#data-for-the-example",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Data for the example",
    "text": "Data for the example\nAnd use the dividendinfo.csv dataset from https://github.com/MGCodesandStats/datasets\n\nmydata &lt;- read.csv(\"https://raw.githubusercontent.com/MGCodesandStats/datasets/master/dividendinfo.csv\")\nstr(mydata)\n\n'data.frame':   200 obs. of  6 variables:\n $ dividend       : int  0 1 1 0 1 1 1 0 1 1 ...\n $ fcfps          : num  2.75 4.96 2.78 0.43 2.94 3.9 1.09 2.32 2.5 4.46 ...\n $ earnings_growth: num  -19.25 0.83 1.09 12.97 2.44 ...\n $ de             : num  1.11 1.09 0.19 1.7 1.83 0.46 2.32 3.34 3.15 3.33 ...\n $ mcap           : int  545 630 562 388 684 621 656 351 658 330 ...\n $ current_ratio  : num  0.924 1.469 1.976 1.942 2.487 ...",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#data-pre-processing",
    "href": "C3.1-Introduction_to_ANN-Slides.html#data-pre-processing",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Data pre-processing",
    "text": "Data pre-processing\n\nnormalize &lt;- function(x) {\n  return ((x - min(x)) / (max(x) - min(x)))\n}\nnormData &lt;- as.data.frame(lapply(mydata, normalize))",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#test-and-training-sets",
    "href": "C3.1-Introduction_to_ANN-Slides.html#test-and-training-sets",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Test and training sets",
    "text": "Test and training sets\nFinally we break our data in a test and a training set:\n\nperc2Train &lt;- 2/3\nssize &lt;- nrow(normData)\nset.seed(12345)\ndata_rows &lt;- floor(perc2Train *ssize)\ntrain_indices &lt;- sample(c(1:ssize), data_rows)\ntrainset &lt;- normData[train_indices,]\ntestset &lt;- normData[-train_indices,]",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#training-a-neural-network",
    "href": "C3.1-Introduction_to_ANN-Slides.html#training-a-neural-network",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Training a neural network",
    "text": "Training a neural network\nWe train a simple NN with two hidden layers, with 4 and 2 neurons respectively.\n\n#Neural Network\nlibrary(neuralnet)\nnn &lt;- neuralnet(dividend ~ fcfps + earnings_growth + de + mcap + current_ratio, \n                data=trainset, \n                hidden=c(2,1), \n                linear.output=FALSE, \n                threshold=0.01)",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#network-plot",
    "href": "C3.1-Introduction_to_ANN-Slides.html#network-plot",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Network plot",
    "text": "Network plot\nThe output of the procedure is a neural network with estimated weights\n\nplot(nn, rep = \"best\")",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#predictions",
    "href": "C3.1-Introduction_to_ANN-Slides.html#predictions",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Predictions",
    "text": "Predictions\n\ntemp_test &lt;- subset(testset, select =\n                      c(\"fcfps\",\"earnings_growth\", \n                        \"de\", \"mcap\", \"current_ratio\"))\nnn.results &lt;- compute(nn, temp_test)\nresults &lt;- data.frame(actual = \n                  testset$dividend, \n                  prediction = nn.results$net.result)\nhead(results)\n\n   actual   prediction\n9       1 0.9919213885\n19      1 0.9769206123\n22      0 0.0002187144\n26      0 0.6093330933\n27      1 0.7454164893\n29      1 0.9515431416",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C3.1-Introduction_to_ANN-Slides.html#model-evaluation",
    "href": "C3.1-Introduction_to_ANN-Slides.html#model-evaluation",
    "title": "ARTIFICIAL NEURAL NETWORKS",
    "section": "Model evaluation",
    "text": "Model evaluation\n\nroundedresults&lt;-sapply(results,round,digits=0)\nroundedresultsdf=data.frame(roundedresults)\nattach(roundedresultsdf)\ntable(actual,prediction)\n\n      prediction\nactual  0  1\n     0 33  3\n     1  4 27",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Neural Networks"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#outline",
    "href": "C2.2-Ensemble_Methods-Slides.html#outline",
    "title": "Ensemble Methods",
    "section": "Outline",
    "text": "Outline\n\nIntroduction to Ensembles\nBagging and the Bootstrap\nRandom Forests\nBoosting and its variants",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#weak-learners",
    "href": "C2.2-Ensemble_Methods-Slides.html#weak-learners",
    "title": "Ensemble Methods",
    "section": "Weak learners",
    "text": "Weak learners\n\n\nModels that perform only slightly better than random guessing are called weak learners (Bishop 2007).\nWeak learners low predictive accuracy may be due, to the predictor having high bias or high variance.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#trees-may-be-weak-learners",
    "href": "C2.2-Ensemble_Methods-Slides.html#trees-may-be-weak-learners",
    "title": "Ensemble Methods",
    "section": "Trees may be weak learners",
    "text": "Trees may be weak learners\n\nTrees use to be sensitive to small changes in training data which lead to very different tree structure.\nThis implies predictions are highly variable\nThis may be explained because they are greedy algorithms making locally optimal decisions at each node without considering the global optimal solution.\n\nThis can lead to suboptimal splits and ultimately a weaker predictive performance.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#theres-room-for-improvement",
    "href": "C2.2-Ensemble_Methods-Slides.html#theres-room-for-improvement",
    "title": "Ensemble Methods",
    "section": "There’s room for improvement",
    "text": "There’s room for improvement\n\nIn many situations trees may be a good option (e.g. for simplicity and interpretability)\nBut there are issues that, if solved, may improve performance.\nIt is to be noted, too, that these problems are not unique to trees.\n\nOther simple models such as linear regression may be considered as weakl learners in may situations.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#the-bias-variance-trade-off",
    "href": "C2.2-Ensemble_Methods-Slides.html#the-bias-variance-trade-off",
    "title": "Ensemble Methods",
    "section": "The bias-variance trade-off",
    "text": "The bias-variance trade-off\n\nWhen we try to improve weak learners we need to deal with the bias-variance trade-off.\n\n\nThe bias-variance trade-off",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#how-to-deal-with-such-trade-off",
    "href": "C2.2-Ensemble_Methods-Slides.html#how-to-deal-with-such-trade-off",
    "title": "Ensemble Methods",
    "section": "How to deal with such trade-off",
    "text": "How to deal with such trade-off\n\nHow can a model be made less variable or less biased without this increasing its bias or variance?\nThere are distinct appraches to deal with this problem\n\nRegularization,\nFeature engineering,\nModel selection\nEnsemble learning",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#ensemble-learners",
    "href": "C2.2-Ensemble_Methods-Slides.html#ensemble-learners",
    "title": "Ensemble Methods",
    "section": "Ensemble learners",
    "text": "Ensemble learners\n\nEnsemble learning takes a distinct based on “the wisdom of crowds”.\nPredictors, also called, ensembles are built by fitting repeated (weak learners) models on the same data and combining them to form a single result.\nAs a general rule, ensemble learners tend to improve the results obtained with the weak learners they are made of.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#ensemble-methods",
    "href": "C2.2-Ensemble_Methods-Slides.html#ensemble-methods",
    "title": "Ensemble Methods",
    "section": "Ensemble methods",
    "text": "Ensemble methods\n\nIf we rely on how they deal with the bias-variance trade-off we can consider distinct groups of ensemble methods:\n\nBagging\nBoosting\nHybrid learners\n\n\nEmnsemble methods cheatsheet",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#aggregating-trees",
    "href": "C2.2-Ensemble_Methods-Slides.html#aggregating-trees",
    "title": "Ensemble Methods",
    "section": "Aggregating Trees",
    "text": "Aggregating Trees\nBagging, Random Forests, and Random Patches reduce variance compared to individual decision trees by constructing multiple trees using different types of subsets:\n\nSubsets of observations (Bagging)\nSubsets of features (Random Forests)\nSubsets of both observations and features (Random Patches)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#boosting-stacking",
    "href": "C2.2-Ensemble_Methods-Slides.html#boosting-stacking",
    "title": "Ensemble Methods",
    "section": "Boosting & Stacking",
    "text": "Boosting & Stacking\n\nBoosting or Stacking combine distinct predictors to yield a model with an increasingly smaller error, and so reduce the bias.\nThey differ on if do the combination\n\nsequentially (1) or\nusing a meta-model (2) .",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#hybrid-techniques",
    "href": "C2.2-Ensemble_Methods-Slides.html#hybrid-techniques",
    "title": "Ensemble Methods",
    "section": "Hybrid Techniques",
    "text": "Hybrid Techniques\n\nHybrid techniques combine approaches in order to deal with both variance and bias.\nThe approach should be clear from their name:\n\nGradient Boosted Trees with Bagging\nStacked bagging",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#bagging-bootstrap-aggregation",
    "href": "C2.2-Ensemble_Methods-Slides.html#bagging-bootstrap-aggregation",
    "title": "Ensemble Methods",
    "section": "Bagging: bootstrap aggregation",
    "text": "Bagging: bootstrap aggregation\n\nDecision trees suffer from high variance when compared with other methods such as linear regression, especially when \\(n/p\\) is moderately large.\nGiven that this is intrinsic to trees, Breiman (1996) sugested to build multiple trees derived from the same dataset and, somehow, average them.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#averaging-decreases-variance",
    "href": "C2.2-Ensemble_Methods-Slides.html#averaging-decreases-variance",
    "title": "Ensemble Methods",
    "section": "Averaging decreases variance",
    "text": "Averaging decreases variance\n\nBagging relies, informally, on the idea that:\n\ngiven \\(X\\sim F()\\), s.t. \\(Var_F(X)=\\sigma^2\\),\ngiven a s.r.s. \\(X_1, ..., X_n\\) from \\(F\\) then\nif \\(\\overline{X}=\\frac{1}{N}\\sum_{i=1}^n X_i\\) then \\(var_F(\\overline{X})=\\sigma^2/n\\).\n\nThat is, relying on the sample mean instead of on simple observations, decreases variance by a factor of \\(n\\).\nBTW this idea is still (approximately) valid for more general statistics where the CLT applies.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#what-means-averaging-trees",
    "href": "C2.2-Ensemble_Methods-Slides.html#what-means-averaging-trees",
    "title": "Ensemble Methods",
    "section": "What means averaging trees?",
    "text": "What means averaging trees?\nTwo questions arise here:\n\nHow to go from \\(X\\) to \\(X_1, ..., X_n\\)?\n\n\nThis will be done using bootstrap resampling.\n\n\nWhat means “averaging” in this context.\n\n\nDepending on the type of tree:\n\nAverage predictions for regression trees.\nMajority voting for classification trees.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#bagging-bootstrap-aggregation-1",
    "href": "C2.2-Ensemble_Methods-Slides.html#bagging-bootstrap-aggregation-1",
    "title": "Ensemble Methods",
    "section": "BAGGING: Bootstrap Aggregation",
    "text": "BAGGING: Bootstrap Aggregation\n\nBreiman (1996) combined the ideas of:\n\nAveraging provides decreased variance estimates,\nBootstrap provides multiple (re)samples.\n\nHe suggested: bootstrap aggregating :\n\nTake resamples from the original training dataset\nLearn the model on each bootstrapped training set to get a prediction \\(\\hat f^{*b}(x)\\).\nAverage \\(\\hat f^{*b}(x)\\) to obtain improved prediction/classification.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#bagging-predictionclassifier",
    "href": "C2.2-Ensemble_Methods-Slides.html#bagging-predictionclassifier",
    "title": "Ensemble Methods",
    "section": "Bagging prediction/classifier",
    "text": "Bagging prediction/classifier\n\nFor regression (trees) the bagged estimate is the average prediction at \\(x\\) from these \\(B\\) trees. \\[\n\\hat f_{bag}(x)=\\frac 1B \\sum_{b=1}^B \\hat f^{*b}(x)\n\\]\nFor classification (trees) the bagged classifier selects the class with the most “votes” from the \\(B\\) trees: \\[\n\\hat G_{bag}(x) = \\arg \\max_k \\hat f_{bag}(x).\n\\]",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#resampling-based-estimators",
    "href": "C2.2-Ensemble_Methods-Slides.html#resampling-based-estimators",
    "title": "Ensemble Methods",
    "section": "Resampling based estimators",
    "text": "Resampling based estimators\n\nThe bootstrap was introduced as a way to provide standard error estimators.\nWhen used to compute \\(\\hat f_{bag}(x)\\) or \\(\\hat G_{bag}(x)\\), as described above, it provides direct estimators of a characteristic, not of their standard errors.\nHowever, the bagging process can also provide resampling based estimates of the precision of these estimators.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#out-of-bag-observations",
    "href": "C2.2-Ensemble_Methods-Slides.html#out-of-bag-observations",
    "title": "Ensemble Methods",
    "section": "Out-Of-Bag observations",
    "text": "Out-Of-Bag observations\n\nEvery time a resample is taken with replacement, some observations are omitted, due to the multiple occurring of others.\n\n\n\nThese out-of-bag (OOB) observations can be used to build an estimate of prediction error.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#out-of-bag-error-estimates",
    "href": "C2.2-Ensemble_Methods-Slides.html#out-of-bag-error-estimates",
    "title": "Ensemble Methods",
    "section": "Out-Of-Bag error estimates",
    "text": "Out-Of-Bag error estimates\nSince each out-of-bag set is not used to train the model, it can be used to evaluate performance.\n\nFind all trees that are not trained by the OOB instance.\nTake the majority vote of these trees for the OOB instance, compared to the true value of the OOB instance.\nCompile OOB error for all instances in the OOB dataset.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#illustration-of-oob-ee",
    "href": "C2.2-Ensemble_Methods-Slides.html#illustration-of-oob-ee",
    "title": "Ensemble Methods",
    "section": "Illustration of OOB EE",
    "text": "Illustration of OOB EE\n\nSource: https://www.baeldung.com/cs/random-forests-out-of-bag-error",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#bagging-in-r",
    "href": "C2.2-Ensemble_Methods-Slides.html#bagging-in-r",
    "title": "Ensemble Methods",
    "section": "Bagging in R",
    "text": "Bagging in R\n\nWe use the AmesHousing dataset on house prices in Ames, IA, to predict the “Sales price” of houses.\n\n\nStart by splitting the dataset in test/train subsets\nBuild a bagged tree on the train subset and evaluate it on the test subset.\nInterpret the results using “Variable importance”\n\n\nBagging is equivalent to RandomForest if each tree in a Random Forest considers all features at every split.\nSo we use randomForest package setting mtry to total number of features.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#bagging---prepare-data",
    "href": "C2.2-Ensemble_Methods-Slides.html#bagging---prepare-data",
    "title": "Ensemble Methods",
    "section": "Bagging - Prepare data",
    "text": "Bagging - Prepare data\n\n# Prepare \"clean\" dataset from raw data\names &lt;- AmesHousing::make_ames()\n# Scale response variable to improve readability\names$Sale_Price &lt;- ames$Sale_Price/1000\n# Split in test/training\nset.seed(123)\ntrain &lt;- sample(1:nrow(ames), nrow(ames)/2)\n# split &lt;- rsample::initial_split(ames, prop = 0.7, \n#                        strata = \"Sale_Price\")\names_train  &lt;- ames[train,]\names_test   &lt;- ames[-train,]",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#bagging---build-bag-of-trees",
    "href": "C2.2-Ensemble_Methods-Slides.html#bagging---build-bag-of-trees",
    "title": "Ensemble Methods",
    "section": "Bagging - Build bag of trees",
    "text": "Bagging - Build bag of trees\n\nlibrary(randomForest)\nset.seed(12543)\nbag.Ames &lt;- randomForest(Sale_Price ~ ., \n                         data = ames_train, \n                         mtry = ncol(ames_train-1), \n                         ntree = 100,\n                         importance = TRUE)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#bagging---results",
    "href": "C2.2-Ensemble_Methods-Slides.html#bagging---results",
    "title": "Ensemble Methods",
    "section": "Bagging - Results",
    "text": "Bagging - Results\n\nshow(bag.Ames )\n\nCall:\n randomForest(formula = Sale_Price ~ ., data = ames_train, mtry = ncol(ames_train -      1), ntree = 100, importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 100\nNo. of variables tried at each split: 80\n\n          Mean of squared residuals: 771498315\n                    % Var explained: 87.89",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#bagging---prediction-and-accuracy",
    "href": "C2.2-Ensemble_Methods-Slides.html#bagging---prediction-and-accuracy",
    "title": "Ensemble Methods",
    "section": "Bagging - Prediction and accuracy",
    "text": "Bagging - Prediction and accuracy\n\nyhat.bag &lt;- predict(bag.Ames, newdata = ames_test)\nMSE= mean((yhat.bag -ames_test$Sale_Price)^2)\nplot(yhat.bag, ames_test$Sale_Price, main=c(\"MSE= \", MSE))\nabline(0, 1)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#bagging---prediction-and-accuracy-1",
    "href": "C2.2-Ensemble_Methods-Slides.html#bagging---prediction-and-accuracy-1",
    "title": "Ensemble Methods",
    "section": "Bagging - Prediction and accuracy",
    "text": "Bagging - Prediction and accuracy",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#interpetability-the-achiles-heel",
    "href": "C2.2-Ensemble_Methods-Slides.html#interpetability-the-achiles-heel",
    "title": "Ensemble Methods",
    "section": "Interpetability: The “achiles heel”",
    "text": "Interpetability: The “achiles heel”\n\nTrees may have a straightforward interpretation,\n\nPlotting the tree provides information about\n\nwhich variables are important\nhow they act on the prediction\n\n\nEnsembles are less intuitive because\n\nthere is no consensus tree.\nnot clear which variables are most important.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#feature-importance-from-trees",
    "href": "C2.2-Ensemble_Methods-Slides.html#feature-importance-from-trees",
    "title": "Ensemble Methods",
    "section": "Feature importance from Trees",
    "text": "Feature importance from Trees\n\nTo measure feature importance, the reduction in the loss function (e.g., SSE) attributed to each variable at each split is tabulated.\nThe total reduction in the loss function across all splits by a variable are summed up and used as the total feature importance.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#feature-importance-for-ensembles",
    "href": "C2.2-Ensemble_Methods-Slides.html#feature-importance-for-ensembles",
    "title": "Ensemble Methods",
    "section": "Feature importance for Ensembles",
    "text": "Feature importance for Ensembles\n\nVariable importance measures can be extended to an ensemble simply by adding up variable importance over all trees built.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#random-forests-decorrelating-predictors",
    "href": "C2.2-Ensemble_Methods-Slides.html#random-forests-decorrelating-predictors",
    "title": "Ensemble Methods",
    "section": "Random forests: decorrelating predictors",
    "text": "Random forests: decorrelating predictors\n\nBagged trees, based on re-samples (of the same sample) tend to be highly correlated.\nLeo Breimann, again, introduced a modification to bagging, he called Random forests, that aims at decorrelating trees as follows:\n\nWhen growing a tree from one bootstrap sample,\nAt each split use only a randomly selected subset of predictors.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#split-variable-randomization",
    "href": "C2.2-Ensemble_Methods-Slides.html#split-variable-randomization",
    "title": "Ensemble Methods",
    "section": "Split variable randomization",
    "text": "Split variable randomization\n\nWhile growing a decision tree, during the bagging process,\nRandom forests perform split-variable randomization:\n\neach time a split is to be performed,\nthe search for the split variable is limited to a random subset of \\(m_{try}\\) of the original \\(p\\) features.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#random-forests-1",
    "href": "C2.2-Ensemble_Methods-Slides.html#random-forests-1",
    "title": "Ensemble Methods",
    "section": "Random forests",
    "text": "Random forests\n\nSource: AIML.com research",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#how-many-variables-per-split",
    "href": "C2.2-Ensemble_Methods-Slides.html#how-many-variables-per-split",
    "title": "Ensemble Methods",
    "section": "How many variables per split?",
    "text": "How many variables per split?\n\n\\(m\\) can be chosen using cross-validation, but\nThe usual recommendation for random selection of variables at each split has been studied by simulation:\n\nFor regression default value is \\(m=p/3\\)\nFor classification default value is \\(m=\\sqrt{p}\\).\n\nIf \\(m=p\\), we have bagging instead of RF.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#random-forest-algorithm-1",
    "href": "C2.2-Ensemble_Methods-Slides.html#random-forest-algorithm-1",
    "title": "Ensemble Methods",
    "section": "Random forest algorithm (1)",
    "text": "Random forest algorithm (1)\n\nRF Algorithm, from ch. 11 in (Boehmke and Greenwell 2020)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#random-forest-algorithm-2",
    "href": "C2.2-Ensemble_Methods-Slides.html#random-forest-algorithm-2",
    "title": "Ensemble Methods",
    "section": "Random forest algorithm (2)",
    "text": "Random forest algorithm (2)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#out-of-the-box-performance",
    "href": "C2.2-Ensemble_Methods-Slides.html#out-of-the-box-performance",
    "title": "Ensemble Methods",
    "section": "Out-of-the box performance",
    "text": "Out-of-the box performance\n\nRandom forests tend to provide very good out-of-the-box performance, that is:\n\nAlthough several hyperparameters can be tuned,\nDefault values tend to produce good results.\n\nMoreover, among the more popular machine learning algorithms, RFs have the least variability in their prediction accuracy when tuning (Probst, Wright, and Boulesteix 2019).",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#out-of-the-box-performance-1",
    "href": "C2.2-Ensemble_Methods-Slides.html#out-of-the-box-performance-1",
    "title": "Ensemble Methods",
    "section": "Out of the box performance",
    "text": "Out of the box performance\n\nA random forest trained with all hyperparameters set to their default values can yield an OOB RMSE that is better than many other classifiers, with or without tuning.\nThis combined with good stability and ease-of-use has made it the option of choice for many problems.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#out-of-the-box-performance-example",
    "href": "C2.2-Ensemble_Methods-Slides.html#out-of-the-box-performance-example",
    "title": "Ensemble Methods",
    "section": "Out of the box performance example",
    "text": "Out of the box performance example\n\n# number of features\nn_features &lt;- length(setdiff(names(ames_train), \"Sale_Price\"))\n\n# train a default random forest model\names_rf1 &lt;- ranger(\n  Sale_Price ~ ., \n  data = ames_train,\n  mtry = floor(n_features / 3),\n  respect.unordered.factors = \"order\",\n  seed = 123\n)\n\n# get OOB RMSE\n(default_rmse &lt;- sqrt(ames_rf1$prediction.error))\n## [1] 24859.27",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#tuning-hyperparameters",
    "href": "C2.2-Ensemble_Methods-Slides.html#tuning-hyperparameters",
    "title": "Ensemble Methods",
    "section": "Tuning hyperparameters",
    "text": "Tuning hyperparameters\nThere are several parameters that, appropriately tuned, can improve RF performance.\n\n\nNumber of trees in the forest.\nNumber of features to consider at any given split (\\(m_{try}\\)).\nComplexity of each tree.\nSampling scheme.\nSplitting rule to use during tree construction.\n\n\n1 & 2 usually have largest impact on predictive accuracy.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#number-of-trees",
    "href": "C2.2-Ensemble_Methods-Slides.html#number-of-trees",
    "title": "Ensemble Methods",
    "section": "1. Number of trees",
    "text": "1. Number of trees\n\n\n\nThe number of trees needs to be sufficiently large to stabilize the error rate.\nMore trees provide more robust and stable error estimates\nBut impact on computation time increases linearly with \\(n_tree\\)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#number-of-features-m_try.",
    "href": "C2.2-Ensemble_Methods-Slides.html#number-of-features-m_try.",
    "title": "Ensemble Methods",
    "section": "2. Number of features (\\(m_{try}\\)).",
    "text": "2. Number of features (\\(m_{try}\\)).\n\n\n\n\\(m_{try}\\) helps to balance low tree correlation with reasonable predictive strength.\nSensitive to total number of variables. If high /low, better increase/decrease it.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#complexity-of-each-tree.",
    "href": "C2.2-Ensemble_Methods-Slides.html#complexity-of-each-tree.",
    "title": "Ensemble Methods",
    "section": "3. Complexity of each tree.",
    "text": "3. Complexity of each tree.\n\n\n\nThe complexity of underlying trees influences RF performance.\nNode size has strong influence on error and time.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#sampling-scheme.",
    "href": "C2.2-Ensemble_Methods-Slides.html#sampling-scheme.",
    "title": "Ensemble Methods",
    "section": "4. Sampling scheme.",
    "text": "4. Sampling scheme.\n\n\n\nDefault: Bootstrap sampling with replacement on 100% observations. - Sampling size and replacement or not can affect diversity and reduce bias.\nNode size has strong influence on error and time.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#splitting-rule",
    "href": "C2.2-Ensemble_Methods-Slides.html#splitting-rule",
    "title": "Ensemble Methods",
    "section": "5. Splitting rule",
    "text": "5. Splitting rule\n\nDefault splitting rules favor features with many splits, potentially biasing towards certain variable types.\nConditional inference trees offer an alternative to reduce bias, but may not always improve predictive accuracy and have longer training times.\nRandomized splitting rules, like extremely randomized trees (which draw split points completely randomly), improve computational efficiency but may not enhance predictive accuracy and can even reduce it.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#tuning-hyperparameters-1",
    "href": "C2.2-Ensemble_Methods-Slides.html#tuning-hyperparameters-1",
    "title": "Ensemble Methods",
    "section": "Tuning hyperparameters",
    "text": "Tuning hyperparameters\n\nRF are a good example of a common situachion in ML:\n\nAs the number of parameter increases,\nfinding their optimal values requires more effort\nand can even become computationally unfeasible.\n\nAs more complex algorithms with greater number of hyperparameters are introduced tuning strategies should also be considered.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#tuning-strategies",
    "href": "C2.2-Ensemble_Methods-Slides.html#tuning-strategies",
    "title": "Ensemble Methods",
    "section": "Tuning strategies",
    "text": "Tuning strategies\n\n\nGrid Search: Systematically searches through (all possible combinations) a predefined grid of hyperparameter values to find the combination that maximizes performance.\nRandom Search: Randomly samples hyperparameter values from predefined distributions. Faster than Grid Search, but less prone to find optimum.\nModel-Based Optimization leverages probabilistic models, often Gaussian processes, to model the objective function and iteratively guide the search for optimal hyperparameters.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#random-forests-in-bioinformatics",
    "href": "C2.2-Ensemble_Methods-Slides.html#random-forests-in-bioinformatics",
    "title": "Ensemble Methods",
    "section": "Random forests in bioinformatics",
    "text": "Random forests in bioinformatics\n\nRandom forests have been thoroughly used in Bioinformatics (Boulesteix et al. 2012).\nBioinformatics data are often high dimensional with\n\ndozens or (less often) hundreds of samples/individuals\nthousands (or hundreds of thousands) of variables.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#application-of-random-forests",
    "href": "C2.2-Ensemble_Methods-Slides.html#application-of-random-forests",
    "title": "Ensemble Methods",
    "section": "Application of Random forests",
    "text": "Application of Random forests\n\nRandom forests provide robust classifiers for:\n\nDistinguishing cancer from non cancer,\nPredicting tumor type in cancer of unknown origin,\nSelecting variables (SNPs) in Genome Wide Association Studies…\n\nSome variation of Random forests are used only for variable selection",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#improving-predictors-iteratively",
    "href": "C2.2-Ensemble_Methods-Slides.html#improving-predictors-iteratively",
    "title": "Ensemble Methods",
    "section": "Improving predictors iteratively",
    "text": "Improving predictors iteratively\n\nThe idea of improving weak learners by aggregation has moved historically along two distinct lines:\n\nBuild similar learners on resamples from the original sample and average the predictions.\n\n\nThis entails Bagging and Random Forests\n\n\nBuild a learner progressively, improving it at every step using weak learners, until the desired possible quality is obtained.\n\n\nThis is what Boosting is about.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#bagging-vs-boosting",
    "href": "C2.2-Ensemble_Methods-Slides.html#bagging-vs-boosting",
    "title": "Ensemble Methods",
    "section": "Bagging vs Boosting",
    "text": "Bagging vs Boosting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Ensemble Learning: Bagging & Boosting",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#so-what-is-boosting",
    "href": "C2.2-Ensemble_Methods-Slides.html#so-what-is-boosting",
    "title": "Ensemble Methods",
    "section": "So what is Boosting",
    "text": "So what is Boosting\n\n\nIdea: create a model that is better than any of its individual components by combining their strengths and compensating for their weaknesses.\n\nMultiple weak models are trained sequentially.\nEach new model is trained to improve the errors made by the previous model.\n\nThe final model is a weighted combination of all the models where the weights are determined by the accuracy of each model.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#historical-background",
    "href": "C2.2-Ensemble_Methods-Slides.html#historical-background",
    "title": "Ensemble Methods",
    "section": "Historical background",
    "text": "Historical background\n\nThe boosting technique was proposed by Robert Schapire (Schapire 1989) and Yoav Freund in the 1990s.\nThey introduced AdaBoost, the first widely-used Boosting algorithm.\n\nIt achieved significant success in various applications, including face detection and handwriting recognition.\n\nOther Boosting algorithms have become popular: Gradient Boosting, XGBoost, and LightGBM.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#adaboost",
    "href": "C2.2-Ensemble_Methods-Slides.html#adaboost",
    "title": "Ensemble Methods",
    "section": "Adaboost",
    "text": "Adaboost\n\nAdaBoost (Adaptive Boosting) is a direct implementation of the “boosting” idea.\nIt trains models sequentially, new models focusing on correcting the errors of the previous ones.\nFinal Decision is made by combining models, giving more influence to the most reliable ones through\n\nA weighted majority vote (classification) or\nWeighted sum (regression).",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#running-adaboost",
    "href": "C2.2-Ensemble_Methods-Slides.html#running-adaboost",
    "title": "Ensemble Methods",
    "section": "Running Adaboost",
    "text": "Running Adaboost\n\nAdaboost proceeds iteratively by, at each iteration:\n\nFit the weak learner using initial weights.\nPredict and identify misclassified observations.\nUpdate observation weights:\n\ndecrease for correctly classified\nincrease for misclassified.\n\nAssign a learner weight to the weak learner based on its accuracy.\n\nThe final prediction is a weighted combination of all weak learners.\n\n\n\nAdaBoost implements a vector of weights to penalize those samples that were incorrectly inferred (by increasing the weight) and reward those that were correctly inferred (by decreasing the weight).\n\nUpdating this weight vector will generate a distribution where it will be more likely to extract those samples with higher weight (that is, those that were incorrectly inferred),\nThis sample will be introduced to the next base learner in the sequence and\nThis will be repeated until a stop criterion is met.\nLikewise, each base learner in the sequence will have assigned a weight, the higher the performance, the higher the weight and the greater the impact of this base learner for the final decision.\nFinally, to make a prediction,\n\neach base learner in the sequence will be fed with the test data,\neach of the predictions of each model will be voted (for the classification case) or averaged (for the regression case)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#adaboost-architecture",
    "href": "C2.2-Ensemble_Methods-Slides.html#adaboost-architecture",
    "title": "Ensemble Methods",
    "section": "Adaboost Architecture",
    "text": "Adaboost Architecture\n\n Source: Ensemble Learning: Bagging & Boosting",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#adaboost-pseudo-code",
    "href": "C2.2-Ensemble_Methods-Slides.html#adaboost-pseudo-code",
    "title": "Ensemble Methods",
    "section": "Adaboost pseudo-code",
    "text": "Adaboost pseudo-code\n\n\nInitialize sample weights: \\(w_i = 1/N\\), where \\(N\\) = # of training samples.\nFor each iteration \\(t=1,2,\\dots,T\\) do:\n\nTrain weak classifier \\(h_t(x)\\) on training set weighted by \\(w_i\\).\nCompute the weighted error rate: \\[{\\small \\left. \\epsilon_t = \\sum_{i=1}^N w_i I(y_i \\neq h_t(x_i)) \\right/ \\left. \\sum_{i=1}^N w_i\\right. }\\]\nCompute the classifier weight: \\(\\alpha_t = \\frac{1}{2}\\log\\frac{1-\\epsilon_t}{\\epsilon_t}\\).\nUpdate the sample weights: \\(w_i \\leftarrow w_i^{-\\alpha_t I(y_i \\neq h_t(x_i))}\\).\nNormalize the sample weights: \\(w_i \\leftarrow \\frac{w_i}{\\sum_{j=1}^N w_j}\\).\n\nOutput the final classifier: \\(H(x) = \\text{sign}\\left(\\sum_{t=1}^T \\alpha_t h_t(x)\\right)\\).",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#adaboost-applications",
    "href": "C2.2-Ensemble_Methods-Slides.html#adaboost-applications",
    "title": "Ensemble Methods",
    "section": "Adaboost applications",
    "text": "Adaboost applications\n\n source: Evaluating the AdaBoost Algorithm for Biometric-Based Face Recognition",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#adaboost-has-limitations",
    "href": "C2.2-Ensemble_Methods-Slides.html#adaboost-has-limitations",
    "title": "Ensemble Methods",
    "section": "Adaboost has limitations",
    "text": "Adaboost has limitations\n\nAdaboost was a breakthrough algorithm that significantly improved the accuracy of ML models.\nHowever, Adaboost has its limitations.\n\nIt does not handle continuous variables very well.\nCan be sensitive to noisy data and outliers,\nMay not perform well with complex datasets.\nIts performance can reach a “plateau”: it will no longer improve after a certain number of iterations.\n\nIn order to deal with some of these drawbacks different variants of boosting have been proposed.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#gradient-boosting",
    "href": "C2.2-Ensemble_Methods-Slides.html#gradient-boosting",
    "title": "Ensemble Methods",
    "section": "Gradient Boosting",
    "text": "Gradient Boosting\n\nDeveloped to overcome the limitations of Adaboost.\nTakes a different approach that can be linked with Optimization by Gradient Descent.\nSeveral advantages over Adaboost\n\nCan handle continuous variables much better,\nIt is more robust to noisy data and outliers.\nCan handle complex datasets and\nCan continue to improve its accuracy even after Adaboost’s performance has “plateaued”.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#gradient-boosting-algorithm",
    "href": "C2.2-Ensemble_Methods-Slides.html#gradient-boosting-algorithm",
    "title": "Ensemble Methods",
    "section": "Gradient Boosting Algorithm",
    "text": "Gradient Boosting Algorithm\n\nTrain a first weak learner \\(f_1\\), which predicts the response variable \\(y\\), and calculate the residuals \\(y - f_1(x)\\).\nNext, train a new model \\(f_2\\), to predict the residuals of the previous model, that is, to correct the errors made by model \\(f_1\\).\n\n\\(f_1(x) \\approx y\\)\n\\(f_2(x) \\approx y - f_1(x)\\)\n\nIterate calculating residuals of the two models together \\(y - f_1(x) - f_2(x)\\) and train a third model \\(f_3\\) to correct them.\n\n\\(f_3(x) \\approx y - f_1(x) - f_2(x)\\)\n\nRepeat the process \\(M\\) times, so that each new model minimizes the residuals (errors) of the previous one.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#gradient-boosting-may-overfit",
    "href": "C2.2-Ensemble_Methods-Slides.html#gradient-boosting-may-overfit",
    "title": "Ensemble Methods",
    "section": "Gradient Boosting may overfit",
    "text": "Gradient Boosting may overfit\n\nSince the goal of Gradient Boosting is to minimize the residuals iteration by iteration, it is susceptible to overfitting.\nOne way to avoid this problem is by using a regularization value, also known as the learning rate (\\(\\lambda\\)), which limits the influence of each model on the ensemble.\nAs a result of this regularization, more models are needed to form the ensemble, but better results are achieved.\n\n\\(f_1(x) \\approx y\\)\n\\(f_2(x) \\approx y - \\lambda f_1(x)\\)\n\\(f_3(x) \\approx y - \\lambda f_1(x) - \\lambda f_2(x)\\)\n\\(y \\approx \\lambda f_1(x) + \\lambda f_2(x) + \\lambda f_3(x) + \\ldots + \\lambda f_m(x)\\)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#gradient-boosting-architechture",
    "href": "C2.2-Ensemble_Methods-Slides.html#gradient-boosting-architechture",
    "title": "Ensemble Methods",
    "section": "Gradient boosting architechture",
    "text": "Gradient boosting architechture\n\n Source: Ensemble Learning: Bagging & Boosting",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#gradient-boosting-pseudo-code",
    "href": "C2.2-Ensemble_Methods-Slides.html#gradient-boosting-pseudo-code",
    "title": "Ensemble Methods",
    "section": "Gradient Boosting pseudo-code",
    "text": "Gradient Boosting pseudo-code\n\nInitialize the model with a constant value: \\(f_0(x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} y_i\\)\nFor \\(t = 1\\) to \\(T\\):\n\nCompute the negative gradient of the loss function at the current fit: \\(r_{ti} = -\\frac{\\partial L(y_i, f_{t-1}(x_i))}{\\partial f_{t-1}(x_i)}\\)\nTrain a new model to predict the negative gradient values: \\(h(x; \\theta_t) = \\arg\\min\\limits_{h} \\sum\\limits_{i=1}^{n} (r_{ti} - h(x_i; \\theta))^2\\)\nCompute the optimal step size: \\(\\gamma_t = \\arg\\min\\limits_{\\gamma} \\sum\\limits_{i=1}^{n} L(y_i, f_{t-1}(x_i) + \\gamma h(x_i; \\theta_t))\\)\nUpdate the model: \\(f_t(x) = f_{t-1}(x) + \\gamma_t h(x; \\theta_t)\\)\n\nOutput the final model: \\(F(x) = f_T(x)\\)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#relation-with-gradient-descent",
    "href": "C2.2-Ensemble_Methods-Slides.html#relation-with-gradient-descent",
    "title": "Ensemble Methods",
    "section": "Relation with Gradient Descent",
    "text": "Relation with Gradient Descent\n\nGradient Boosting can be seen as an extension of Gradient Descent, a popular optimization algorithm used to find the minimum of a function.\n\nIn Gradient Descent, the weights of the model are updated in the opposite direction of the gradient of the cost function.\nIn Gradient Boosting, the new model is trained on the negative gradient of the loss function, which is equivalent to minimizing the loss function in the direction of steepest descent.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#gradient-boosting-variations",
    "href": "C2.2-Ensemble_Methods-Slides.html#gradient-boosting-variations",
    "title": "Ensemble Methods",
    "section": "Gradient Boosting Variations",
    "text": "Gradient Boosting Variations\n\nMultiple extensions from Gradient Boosting.\nXGBoost\n\nOptimized implementation that uses regularization to control overfitting and provide better accuracy.\nWon many competitions.\n\nLightGBM\n\nRelies on a technique to reduce the number of samples used in each iteration.\nFaster training, good for large datasets.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#boosting-applications",
    "href": "C2.2-Ensemble_Methods-Slides.html#boosting-applications",
    "title": "Ensemble Methods",
    "section": "Boosting applications",
    "text": "Boosting applications\n\nFraud Detection\nImage and Speech Recognition\nAnomaly Detection\nMedical Diagnosis\nAmazon’s recommendation engine\nModels that predict protein structures from amino acid sequences\nPattern identification in fMRI brain scans.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#advantages-of-boosting",
    "href": "C2.2-Ensemble_Methods-Slides.html#advantages-of-boosting",
    "title": "Ensemble Methods",
    "section": "Advantages of Boosting",
    "text": "Advantages of Boosting\n\nBoosting, like other Ensemble methods, improves the accuracy of weak learners and achieve better predictive performance than individual models.\nBoosting also reduces overfitting by improving the generalization ability of models.\nAvailable in many flavors,\nCan be parallelized\nStrong experience in Real world applications and industry.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#limitations-of-boosting",
    "href": "C2.2-Ensemble_Methods-Slides.html#limitations-of-boosting",
    "title": "Ensemble Methods",
    "section": "Limitations of Boosting",
    "text": "Limitations of Boosting\n\nCan be computationally expensive, especially when dealing with large datasets and complex models.\nCan be sensitive to noisy data and outliers,\nMay not work well with certain types of data distributions.\nNot so good as “out-of-the-box”: Requires careful tuning of hyperparameters to achieve optimal performance, which can be time-consuming and challenging.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#boosting-application-with-r-and-python",
    "href": "C2.2-Ensemble_Methods-Slides.html#boosting-application-with-r-and-python",
    "title": "Ensemble Methods",
    "section": "Boosting application with R and Python",
    "text": "Boosting application with R and Python\n\nMany R packages implement the many variations of boosting:\n\nada,\nadabag,\nmboost,\ngbm,\nxgboost\nAn interesting option is to rely on the caret package which allows to run the distinct methods with a common interface.\n\nInpython we usually rely on scikit-learn libraries although there are many alternative implementations\n\nScikit-learn ensemble\nhttps://python-course.eu/machine-learning/boosting-algorithm-in-python.php",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#the-bootstrap",
    "href": "C2.2-Ensemble_Methods-Slides.html#the-bootstrap",
    "title": "Ensemble Methods",
    "section": "The bootstrap",
    "text": "The bootstrap\n\nBootstrap methods were introduced by Efron (1979) to estimate the standard error of a statistic.\nThe success of the idea lied in that the procedure was presented as automatic, that is:\n\ninstead of having to do complex calculations,\nit allowed to approximate them using computer simulation.\n\nSome called it the end of mathematical statistics.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#bootstrap-applications",
    "href": "C2.2-Ensemble_Methods-Slides.html#bootstrap-applications",
    "title": "Ensemble Methods",
    "section": "Bootstrap Applications",
    "text": "Bootstrap Applications\n\nThe bootstrap has been applied to almost any problem in Statistics.\n\nComputing standard errors, Bias, Quantiles,\nBuilding Confidence intervals,\nDoing Significance tests, …\n\nWe illustrate it with the simplest case: estimating the standard error of an estimator.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#empirical-distribution-function",
    "href": "C2.2-Ensemble_Methods-Slides.html#empirical-distribution-function",
    "title": "Ensemble Methods",
    "section": "Empirical Distribution Function",
    "text": "Empirical Distribution Function\n\n\nLet \\(X\\) be a random variable with distribution function \\(F\\),\nLet \\(\\mathbf{X}=X_1,\\ldots,X_n\\) be an i.i.d random sample of \\(F\\) and,\nlet \\(x_1,\\dots, x_n\\) be a realization of \\(\\mathbf{X}\\).\nThe Empirical Cumulative Distribution Function (ECDF) \\[\nF_n(x) = \\frac{1}{n} \\#\\{x_i\\le x: i=1\\dots n\\} = \\frac{1}{n}\n\\sum_{i=1}^n I_{(-\\infty,x]}(x_i),\n\\] is the function that assigns to each real number \\(x\\) the proportion of observed values that are less or equal than \\(x\\).",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#the-ecdf-in-r",
    "href": "C2.2-Ensemble_Methods-Slides.html#the-ecdf-in-r",
    "title": "Ensemble Methods",
    "section": "The ECDF in R",
    "text": "The ECDF in R\n\n\n\nx&lt;- c(2,5,0,11,-1)\nFn &lt;- ecdf(x)\nknots(Fn)\n\n[1] -1  0  2  5 11\n\ncat(\"Fn(3) = \", Fn(3))\n\nFn(3) =  0.6\n\ncat(\"Fn(-2) = \", Fn(-2))\n\nFn(-2) =  0\n\n\n\n\nplot(Fn)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#ecdf-has-good-properties",
    "href": "C2.2-Ensemble_Methods-Slides.html#ecdf-has-good-properties",
    "title": "Ensemble Methods",
    "section": "ECDF has good properties",
    "text": "ECDF has good properties\n\n\\(F_n (x)\\) is a cumulative distribution function.\n\\(F_n()\\) is an important DF because it comes to be the best approximation that one can find for the theoretical (population) distribution function, that is: \\[\nF_n(x) \\longrightarrow F(x), \\text { uniformly in } x \\text{ as } n \\rightarrow \\infty.\n\\]\nThis is stated in the Glivenko-Cantelli theorem also know as the Central Theorem of Statistics.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#the-sample-distribution-function",
    "href": "C2.2-Ensemble_Methods-Slides.html#the-sample-distribution-function",
    "title": "Ensemble Methods",
    "section": "The Sample distribution function",
    "text": "The Sample distribution function\n\n\nGiven a sample, \\(\\mathbf{X}\\) from a certain distribution, \\(X\\),\nConsider it a discrete random variable \\(X_e\\) that sets mass \\(1/n\\) to each of the observed \\(n\\) points \\(x_i\\):\n\n\n\n\n\n\n\n\\(F_n\\) is the distribution (function) of \\(X_e\\).\nFrom here, we notice that generating samples from \\(F_n\\) can be done by randomly taking values from \\(\\mathbf{X}\\) with probability \\(1/n\\)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#plug-in-estimators",
    "href": "C2.2-Ensemble_Methods-Slides.html#plug-in-estimators",
    "title": "Ensemble Methods",
    "section": "Plug-in estimators",
    "text": "Plug-in estimators\n\nAssume we want to estimate some parameter \\(\\theta\\), that can be expressed as \\(\\theta (F)\\), where \\(F\\) is the distribution function of each \\(X_i\\) in \\((X_1,X_2,...,X_n)\\).\nFor example, \\(\\theta = E_F(X)=\\theta (F)\\).\nA natural way to estimate \\(\\theta(F)\\) may be to rely on plug-in estimators, where \\(F\\) in \\(\\hat \\theta (F)\\) is substituted by some approximation (estimate) to \\(F\\).",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#plug-in-estimators-1",
    "href": "C2.2-Ensemble_Methods-Slides.html#plug-in-estimators-1",
    "title": "Ensemble Methods",
    "section": "Plug-in estimators",
    "text": "Plug-in estimators\n\nA plug-in estimator is obtained by substituting \\(F\\) by an approximation to \\(F\\), call it \\(\\hat F\\) in \\(\\theta(F)\\): \\[\n\\widehat {\\theta (F)}=\\theta(\\hat{F})\n\\]\nGiven that \\(F_n\\) is the best approximation to \\(F\\) a reasonable plug-in estimator is \\(\\theta(F_n)\\).\nCommon sample estimators are, indeed, plug-in estimators.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#some-plug-in-estimators",
    "href": "C2.2-Ensemble_Methods-Slides.html#some-plug-in-estimators",
    "title": "Ensemble Methods",
    "section": "Some plug-in estimators",
    "text": "Some plug-in estimators\n\nMany estimators we usally work with are plug-in estimators.\n\n\n\\[\\begin{eqnarray*}\n\\theta_1 &=& E_F(X)=\\theta (F) \\\\\n\\hat{\\theta_1}&=&\\overline{X}=\\int XdF_n(x)=\\frac\n1n\\sum_{i=1}^nx_i=\\theta (F_n)\n\\\\\n\\\\\n\\theta_2 &=& Med(X)=\\{m:P_F(X\\leq m)=1/2\\}=\n\\theta (F),\n\\\\\n\\hat{\\theta_2}&=&\\widehat{Med}(X)=\\{m:\\frac{\\#x_i\\leq m}n=1/2\\}=\\theta (F_n)\n\\end{eqnarray*}\\]",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#precision-of-an-estimate",
    "href": "C2.2-Ensemble_Methods-Slides.html#precision-of-an-estimate",
    "title": "Ensemble Methods",
    "section": "Precision of an estimate",
    "text": "Precision of an estimate\n\nAn key point, when computing an estimator \\(\\hat \\theta\\) of a parameter \\(\\theta\\), is how precise is \\(\\hat \\theta\\) as an estimator of \\(\\theta\\)?\n\nWith the sample mean, \\(\\overline{X}\\), the standard error estimation is immediate because the variance of the estimator is known: \\(\\sigma_\\overline{X}=\\frac{\\sigma (X)}{\\sqrt{n}}\\)\nSo, a natural estimator of the standard error of \\(\\overline{X}\\) is: \\(\\hat\\sigma_\\overline{X}=\\frac{\\hat{\\sigma}(X)}{\\sqrt{n}}\\)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#precision-of-an-estimate-2",
    "href": "C2.2-Ensemble_Methods-Slides.html#precision-of-an-estimate-2",
    "title": "Ensemble Methods",
    "section": "Precision of an estimate (2)",
    "text": "Precision of an estimate (2)\n\nIf, as in this case, the variance of \\(X\\) (and, here, that of \\(\\overline{X}\\)) is a functional of \\(F\\):\n\n\n\\[\n\\sigma _{\\overline{X}}=\\frac{\\sigma (X)}{\\sqrt{n}}=\\frac{\\sqrt{\\int\n[x-\\int x\\,dF(x)]^2 dF(x)}}{\\sqrt{n}}=\\sigma _{\\overline{X}}(F)\n\\]\n\nthen, the standard error estimator is the same functional applied on \\(F_n\\), that is:\n\n\\[\n\\hat{\\sigma}_{\\overline{X}}=\\frac{\\hat{\\sigma}(X)}{\\sqrt{n}}=\\frac{\\sqrt{1/n\\sum_{i=1}^n(x_i-\\overline{x})^2}}{\\sqrt{n}}=\\sigma\n_{\\overline{X}}(F_n).\n\\]",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#standard-error-estimation",
    "href": "C2.2-Ensemble_Methods-Slides.html#standard-error-estimation",
    "title": "Ensemble Methods",
    "section": "Standard error estimation",
    "text": "Standard error estimation\n\nThus, a way to obtain a standard error estimator \\(\\widehat{\\sigma}_{\\widehat{\\theta}}\\) of an estimator \\(\\widehat{\\theta}\\) consists on replacing \\(F\\) with \\(F_n\\) in the ``population’’ standard error expression of \\(\\hat \\theta\\), \\(\\displaystyle{\\sigma_{\\hat\n\\theta}= \\sigma_{\\hat \\theta}(F)}\\), whenever it is known.\nIn a schematic form: \\[\n\\sigma_{\\hat \\theta}= \\sigma_{\\hat \\theta}(F) \\Longrightarrow\n\\sigma_{\\hat \\theta}(F_n)= \\widehat{\\sigma}_{\\hat \\theta}.\n\\] That is, the process consists of “plugging-in” \\(F_n\\) in the (known) functional form, \\(\\sigma_{\\hat \\theta}(F)\\) that defines \\(\\sigma_{\\hat \\theta}\\)}.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#the-bootstrap-1",
    "href": "C2.2-Ensemble_Methods-Slides.html#the-bootstrap-1",
    "title": "Ensemble Methods",
    "section": "The bootstrap (1)",
    "text": "The bootstrap (1)\n\nThe previous approach, \\(F\\simeq F_n \\Longrightarrow \\sigma_{\\hat \\theta}(F) \\simeq \\sigma_{\\hat \\theta}(F_n)\\) presents the obvious drawback that, when the functional form \\(\\sigma _{\\hat{\\theta}}(F)\\) is unknown, it is not possible to carry out the substitution of \\(F\\) by \\(F_n\\).\nThis is, for example, the case of standard error of the median or that of the correlation coefficient.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#the-bootstrap-2",
    "href": "C2.2-Ensemble_Methods-Slides.html#the-bootstrap-2",
    "title": "Ensemble Methods",
    "section": "The bootstrap (2)",
    "text": "The bootstrap (2)\n\nThe bootstrap method makes it possible to do the desired approximation: \\[\\hat{\\sigma}_{\\hat\\theta} \\simeq \\sigma _{\\hat\\theta}(F_n)\\] without having to to know the form of \\(\\sigma_{\\hat\\theta}(F)\\).\nTo do this,the bootstrap estimates, or directly approaches \\(\\sigma_{\\hat{\\theta}}(F_n)\\) over the sample.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#bootstrap-sampling-resampling",
    "href": "C2.2-Ensemble_Methods-Slides.html#bootstrap-sampling-resampling",
    "title": "Ensemble Methods",
    "section": "Bootstrap sampling (resampling)",
    "text": "Bootstrap sampling (resampling)\n\nThe bootstrap allows to estimate the standard error from samples of \\(F_n\\)\n\n\n\\[\\begin{eqnarray*}\n&&\\mbox{Instead of: } \\\\\n&& \\quad F\\stackrel{s.r.s}{\\longrightarrow }{\\bf X} =\n(X_1,X_2,\\dots, X_n) \\, \\quad (\\hat \\sigma_{\\hat\\theta} =\\underbrace{\\sigma_\\theta(F_n)}_{unknown})\n\\\\\n&& \\mbox{It is done: } \\\\\n&& \\quad F_n\\stackrel{s.r.s}{\\longrightarrow }\\quad {\\bf X^{*}}=(X_1^{*},X_2^{*},\n\\dots ,X_n^{*}) \\\\\n&& \\quad (\\hat \\sigma_{\\hat\\theta}= \\hat \\sigma_{\\hat \\theta}^* \\simeq \\sigma_{\\hat \\theta}^*=\\sigma_{\\hat \\theta}(F_n)).\n\n\\end{eqnarray*}\\]\n\n\n\n\\(\\sigma_{\\hat \\theta}^*\\) is the bootstrap standard error of \\(\\hat \\theta\\) and\n\\(\\hat \\sigma_{\\hat \\theta}^*\\) the bootstrap estimate of the standard error of \\(\\hat \\theta\\).",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#the-resampling-process",
    "href": "C2.2-Ensemble_Methods-Slides.html#the-resampling-process",
    "title": "Ensemble Methods",
    "section": "The resampling process",
    "text": "The resampling process\n\n\nResampling consists of extracting samples of size \\(n\\) of \\(F_n\\): \\({\\bf X_b^{*}}\\) is a random sample of size \\(n\\) obtained with replacement from the original sample \\({\\bf X}\\).\nSamples \\({\\bf X^*_1, X^*_2, ..., X^*_B }\\), obtained through this procedure are called bootstrap samples or re-samples.\nOn each resample the statistic of interest \\(\\hat \\theta\\) can be computed yielding a bootstrap estimate \\(\\hat \\theta^*_b= s(\\mathbf{x^*_b})\\).\nThe collection of bootstrap estimates obtained form the resampled samples can be used to estimate distinct characteristics of \\(\\hat \\theta\\) such as its standard error, its bias, etc.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#resampling-illustrated",
    "href": "C2.2-Ensemble_Methods-Slides.html#resampling-illustrated",
    "title": "Ensemble Methods",
    "section": "Resampling illustrated",
    "text": "Resampling illustrated",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#boot.-monte-carlo-algorithm",
    "href": "C2.2-Ensemble_Methods-Slides.html#boot.-monte-carlo-algorithm",
    "title": "Ensemble Methods",
    "section": "Boot. Monte Carlo Algorithm",
    "text": "Boot. Monte Carlo Algorithm\n\n\nDraw a bootstrap sample, \\({\\bf x}_1^{*}\\) from \\(F_n\\) and compute \\(\\hat{\\theta}({\\bf x}_1^{*})\\).\nRepeat (1) \\(B\\) times yielding \\(\\hat{\\theta}({\\bf x}_2^{*})\\), \\(\\dots\\), \\(\\hat{\\theta}({\\bf x}_B^{*})\\) estimates.\nCompute: \\[\\begin{equation*}\n\\hat{\\sigma}_B (\\hat\\theta)= \\sqrt{\n\\frac{\n     \\sum_{b=1}^B\\left( \\hat{\\theta}(%\n     {\\bf x^{*}_i})-\\overline{\\hat{\\theta}^*}\\right) ^2\n     }{\n     (B-1)   \n     }\n}, \\quad \\overline{\\hat{\\theta}^*}\\equiv \\frac 1B\\sum_{b=1}^B\\hat{\\theta}\\left( {\\bf x}%\n_b^{*}\\right)\n\\end{equation*}\\]",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#bootstrap-estimates-of-se",
    "href": "C2.2-Ensemble_Methods-Slides.html#bootstrap-estimates-of-se",
    "title": "Ensemble Methods",
    "section": "Bootstrap Estimates of SE",
    "text": "Bootstrap Estimates of SE\n\n\nIdea behind the bootstrap: the standard error of \\(\\hat\\theta\\), \\(\\sigma(\\hat\\theta)\\) can be approximated by the bootstrap estimator of the standard error, \\(\\sigma_B (\\hat\\theta)\\) which:\n\nCoincides with \\(\\sigma_{\\hat\\theta}(F_n)\\), that cannot be evaluated, if the functional form of \\(\\sigma_{\\hat\\theta}(F)\\) is unkown.\nCan be approximated by the Monte Carlo Estimator, \\(\\hat\\sigma_{\\hat\\theta}(F_n)\\), which is evaluated by resampling.\n\n\n\\[\n\\hat{\\sigma}_B(\\hat\\theta)\\left (\\simeq \\sigma_B(\\hat\\theta)=\\sigma_{\\hat\\theta}(F_n)\\right )\\simeq\\hat \\sigma_{\\hat\\theta}(F_n).\n\\]",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#summary",
    "href": "C2.2-Ensemble_Methods-Slides.html#summary",
    "title": "Ensemble Methods",
    "section": "Summary",
    "text": "Summary\nFrom real world to bootstrap world:\n\n\nAquesta imatge no mostra el càlcul del error estandar. Canviar-la per una que si ho faci",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "C2.2-Ensemble_Methods-Slides.html#references",
    "href": "C2.2-Ensemble_Methods-Slides.html#references",
    "title": "Ensemble Methods",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nBishop, Christopher M. 2007. Pattern Recognition and Machine Learning (Information Science and Statistics). 1st ed. Springer. http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738%3FSubscriptionId%3D13CT5CVB80YFWJEPWS02%26tag%3Dws%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0387310738.\n\n\nBoehmke, Bradley, and Brandon Greenwell. 2020. The r Series Hands-on Machine Learning with r. CRC Press. https://www.routledge.com/Hands-On-Machine-Learning-with-R/Boehmke-Greenwell/p/book/9781138495685.\n\n\nBoulesteix, Anne Laure, Silke Janitza, Jochen Kruppa, and Inke R. König. 2012. “Overview of Random Forest Methodology and Practical Guidance with Emphasis on Computational Biology and Bioinformatics.” Undefined 2 (November): 493–507. https://doi.org/10.1002/WIDM.1072.\n\n\nBreiman, Leo. 1996. “Bagging Predictors.” Machine Learning 24: 123–40. https://doi.org/10.1007/BF00058655/METRICS.\n\n\nEfron, B. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1): 1–26. https://doi.org/10.1214/aos/1176344552.\n\n\nProbst, Philipp, Marvin N. Wright, and Anne-Laure Boulesteix. 2019. “Hyperparameters and Tuning Strategies for Random Forest.” WIREs Data Mining and Knowledge Discovery 9 (3): e1301. https://doi.org/https://doi.org/10.1002/widm.1301.\n\n\nSchapire, Robert E. 1989. “The Strength of Weak Learnability (Extended Abstract).” In 30th Annual Symposium on Foundations of Computer Science, Research Triangle Park, North Carolina, USA, 30 October - 1 November 1989, 28–33. IEEE Computer Society. https://doi.org/10.1109/SFCS.1989.63451.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This page is intended to provide support materials (slides, scripts datasets etc) in an agile way for one part of the Statistical Learning course at the UPC-UB MSc in Statistics and Operations Research (MESIO).\nThis part of the course has an introduction and two blocks, each with two parts."
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#assessing-model-performance",
    "href": "C1.2-Model_validation_and_Resampling.html#assessing-model-performance",
    "title": "Model validation and Resampling",
    "section": "Assessing model performance",
    "text": "Assessing model performance\n\n\nError estimation and, in general, performance assessment in predictive models is a complex process.\nA key challenge is that the true error of a model on new data is typically unknown, and using the training error as a proxy leads to an optimistic evaluation.\nResampling methods, such as cross-validation and the bootstrap, allow us to approximate test error and assess model variability using only the available data.\nWhat is best it can be proven that, well performed, they provide reliable estimates of a model’s performance.\nThis section introduces these techniques and discusses their practical implications in model assessment.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#prediction-generalization-error",
    "href": "C1.2-Model_validation_and_Resampling.html#prediction-generalization-error",
    "title": "Model validation and Resampling",
    "section": "Prediction (generalization) error",
    "text": "Prediction (generalization) error\n\nWe are interested the prediction or generalization error, the error that will appear when predicting a new observation using a model fitted from some dataset.\nAlthough we don’t know it, it can be estimated using either the training error or the test error estimators.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#training-error-vs-test-error",
    "href": "C1.2-Model_validation_and_Resampling.html#training-error-vs-test-error",
    "title": "Model validation and Resampling",
    "section": "Training Error vs Test error",
    "text": "Training Error vs Test error\n\nThe test error is the average error that results from using a statistical learning method to predict the response on a new observation, one that was not used in training the method.\nThe training error is calculated from the difference among the predictions of a model and the observations used to train it.\nTraining error rate often is quite different from the test error rate, and in particular the former can dramatically underestimate the latter.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#the-three-errors",
    "href": "C1.2-Model_validation_and_Resampling.html#the-three-errors",
    "title": "Model validation and Resampling",
    "section": "The three errors",
    "text": "The three errors\n\n\nGeneralization Error. True expected test error (unknown). No bias \\[\\mathcal{E}(f) =  \\mathbb{E}_{X_0, Y_0} [ L(Y_0, f(X_0)) ]\\]\nTest Error Estimator. Estimate of generalization error. Small bias. \\[\\hat{\\mathcal{E}}_{\\text{test}}=\\frac{1}{m} \\sum_{j=1}^{m} L(Y_j^{\\text{test}},  f(X_j^{\\text{test}}))\\]\nTraining Error Estimator. Measures fit to training data (optimistic). High bias \\[\\hat{\\mathcal{E}}_{\\text{train}}: \\frac{1}{n} \\sum_{i=1}^{n} L(Y_i^{\\text{train}}, f(X_i^{\\text{train}}))\\]",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#training--versus-test-set-performance",
    "href": "C1.2-Model_validation_and_Resampling.html#training--versus-test-set-performance",
    "title": "Model validation and Resampling",
    "section": "Training- versus Test-Set Performance",
    "text": "Training- versus Test-Set Performance",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#prediction-error-estimates",
    "href": "C1.2-Model_validation_and_Resampling.html#prediction-error-estimates",
    "title": "Model validation and Resampling",
    "section": "Prediction-error estimates",
    "text": "Prediction-error estimates\n\nIdeal: a large designated test set. Often not available\nSome methods make a mathematical adjustment to the training error rate in order to estimate the test error rate: \\(Cp\\) statistic, \\(AIC\\) and \\(BIC\\).\nInstead, we consider a class of methods that\n\nEstimate test error by holding out a subset of the training observations from the fitting process, and\nApply learning method to held out observations",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#validation-set-approach",
    "href": "C1.2-Model_validation_and_Resampling.html#validation-set-approach",
    "title": "Model validation and Resampling",
    "section": "Validation-set approach",
    "text": "Validation-set approach\n\nRandomly divide the available samples into two parts: a training set and a validation or hold-out set.\nThe model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.\nThe resulting validation-set error provides an estimate of the test error. This is assessed using:\n\nMSE in the case of a quantitative response and\nMisclassification rate in qualitative response.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#the-validation-process",
    "href": "C1.2-Model_validation_and_Resampling.html#the-validation-process",
    "title": "Model validation and Resampling",
    "section": "The Validation process",
    "text": "The Validation process\n\nA random splitting into two halves: left part is training set, right part is validation set",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#example-automobile-data",
    "href": "C1.2-Model_validation_and_Resampling.html#example-automobile-data",
    "title": "Model validation and Resampling",
    "section": "Example: automobile data",
    "text": "Example: automobile data\n\nGoal: compare linear vs higher-order polynomial terms in a linear regression\nMethod: randomly split the 392 observations into two sets,\n\nTraining set containing 196 of the data points,\nValidation set containing the remaining 196 observations.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#example-automobile-data-plot",
    "href": "C1.2-Model_validation_and_Resampling.html#example-automobile-data-plot",
    "title": "Model validation and Resampling",
    "section": "Example: automobile data (plot)",
    "text": "Example: automobile data (plot)\n\nLeft panel single split; Right panel shows multiple splits",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#drawbacks-of-the-vs-approach",
    "href": "C1.2-Model_validation_and_Resampling.html#drawbacks-of-the-vs-approach",
    "title": "Model validation and Resampling",
    "section": "Drawbacks of the (VS) approach",
    "text": "Drawbacks of the (VS) approach\n\n\nIn the validation approach, only a subset of the observations -those that are included in the training set rather than in the validation set- are used to fit the model.\nThe validation estimate of the test error can be highly variable, depending on which observations are included in the training set and which are included in the validation set.\nThis suggests that validation set error may tend to over-estimate the test error for the model fit on the entire data set.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#k-fold-cross-validation",
    "href": "C1.2-Model_validation_and_Resampling.html#k-fold-cross-validation",
    "title": "Model validation and Resampling",
    "section": "\\(K\\)-fold Cross-validation",
    "text": "\\(K\\)-fold Cross-validation\n\nWidely used approach for estimating test error.\nEstimates give an idea of the test error of the final chosen model\nEstimates can be used to select best model,",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#k-fold-cv-mechanism",
    "href": "C1.2-Model_validation_and_Resampling.html#k-fold-cv-mechanism",
    "title": "Model validation and Resampling",
    "section": "\\(K\\)-fold CV mechanism",
    "text": "\\(K\\)-fold CV mechanism\n\nRandomly divide the data into \\(K\\) equal-sized parts.\nRepeat for each part \\(k=1,2, \\ldots K\\),\n\nLeave one part, \\(k\\), apart.\nFit the model to the combined remaining \\(K-1\\) parts,\nThen obtain predictions for the left-out \\(k\\)-th part.\n\nCombine the results to obtain the crossvalidation estimate of tthe error.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#k-fold-cross-validation-in-detail",
    "href": "C1.2-Model_validation_and_Resampling.html#k-fold-cross-validation-in-detail",
    "title": "Model validation and Resampling",
    "section": "\\(K\\)-fold Cross-validation in detail",
    "text": "\\(K\\)-fold Cross-validation in detail\n\n\nA schematic display of 5-fold CV. A set of n observations is randomly split into fve non-overlapping groups. Each of these ffths acts as a validation set (shown in beige), and the remainder as a training set (shown in blue). The test error is estimated by averaging the fve resulting MSE estimates",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#the-details",
    "href": "C1.2-Model_validation_and_Resampling.html#the-details",
    "title": "Model validation and Resampling",
    "section": "The details",
    "text": "The details\n\n\nLet the \\(K\\) parts be \\(C_{1}, C_{2}, \\ldots C_{K}\\), where \\(C_{k}\\) denotes the indices of the observations in part \\(k\\). There are \\(n_{k}\\) observations in part \\(k\\) : if \\(N\\) is a multiple of \\(K\\), then \\(n_{k}=n / K\\).\nCompute \\[\n\\mathrm{CV}_{(K)}=\\sum_{k=1}^{K} \\frac{n_{k}}{n} \\mathrm{MSE}_{k}\n\\] where \\(\\mathrm{MSE}_{k}=\\sum_{i \\in C_{k}}\\left(y_{i}-\\hat{y}_{i}\\right)^{2} / n_{k}\\), and \\(\\hat{y}_{i}\\) is the fit for observation \\(i\\), obtained from the data with part \\(k\\) removed.\n\\(K=n\\) yields \\(n\\)-fold or leave-one out cross-validation (LOOCV).",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#auto-data-revisited",
    "href": "C1.2-Model_validation_and_Resampling.html#auto-data-revisited",
    "title": "Model validation and Resampling",
    "section": "Auto data revisited",
    "text": "Auto data revisited",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#issues-with-cross-validation",
    "href": "C1.2-Model_validation_and_Resampling.html#issues-with-cross-validation",
    "title": "Model validation and Resampling",
    "section": "Issues with Cross-validation",
    "text": "Issues with Cross-validation\n\nSince each training set is only \\((K-1) / K\\) as big as the original training set, the estimates of prediction error will typically be biased upward. Why?\nThis bias is minimized when \\(K=n\\) (LOOCV), but this estimate has high variance, as noted earlier.\n\\(K=5\\) or 10 provides a good compromise for this bias-variance tradeoff.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#cv-for-classification-problems",
    "href": "C1.2-Model_validation_and_Resampling.html#cv-for-classification-problems",
    "title": "Model validation and Resampling",
    "section": "CV for Classification Problems",
    "text": "CV for Classification Problems\n\n\nDivide the data into \\(K\\) roughly equal-sized parts \\(C_{1}, C_{2}, \\ldots C_{K}\\).\n\n\n\nThere are \\(n_{k}\\) observations in part \\(k\\) and \\(n_{k}\\simeq n / K\\).\nCompute \\[\n\\mathrm{CV}_{K}=\\sum_{k=1}^{K} \\frac{n_{k}}{n} \\operatorname{Err}_{k}\n\\] where \\(\\operatorname{Err}_{k}=\\sum_{i \\in C_{k}} I\\left(y_{i} \\neq \\hat{y}_{i}\\right) / n_{k}\\).",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#standard-error-of-cv-estimate",
    "href": "C1.2-Model_validation_and_Resampling.html#standard-error-of-cv-estimate",
    "title": "Model validation and Resampling",
    "section": "Standard error of CV estimate",
    "text": "Standard error of CV estimate\n\nThe estimated standard deviation of \\(\\mathrm{CV}_{K}\\) is:\n\n\\[\n\\widehat{\\mathrm{SE}}\\left(\\mathrm{CV}_{K}\\right)=\\sqrt{\\frac{1}{K} \\sum_{k=1}^{K} \\frac{\\left(\\operatorname{Err}_{k}-\\overline{\\operatorname{Err}_{k}}\\right)^{2}}{K-1}}\n\\]\n\nThis is a useful estimate, but strictly speaking, not quite valid. Why not?",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#why-is-this-an-issue",
    "href": "C1.2-Model_validation_and_Resampling.html#why-is-this-an-issue",
    "title": "Model validation and Resampling",
    "section": "Why is this an issue?",
    "text": "Why is this an issue?\n\n\nIn (K)-fold CV, the same dataset is used repeatedly for training and testing across different folds.\nThis introduces correlations between estimated errors in different folds because each fold’s training set overlaps with others.\nThe assumption underlying this estimation of the standard error is that \\(\\operatorname{Err}_{k}\\) values are independent, which does not hold here.\nThe dependence between folds leads to underestimation of the true variability in \\(\\mathrm{CV}_K\\), meaning that the reported standard error is likely too small, giving a misleading sense of precision in the estimate of the test error.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#cv-right-and-wrong",
    "href": "C1.2-Model_validation_and_Resampling.html#cv-right-and-wrong",
    "title": "Model validation and Resampling",
    "section": "CV: right and wrong",
    "text": "CV: right and wrong\n\n\nConsider a classifier applied to some 2-class data:\n\nStart with 5000 predictors & 50 samples and find the 100 predictors most correlated with the class labels.\nWe then apply a classifier such as logistic regression, using only these 100 predictors.\n\nIn order to estimate the test set performance of this classifier, ¿can we apply cross-validation in step 2, forgetting about step 1?",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#cv-the-wrong-and-the-right-way",
    "href": "C1.2-Model_validation_and_Resampling.html#cv-the-wrong-and-the-right-way",
    "title": "Model validation and Resampling",
    "section": "CV the Wrong and the Right way",
    "text": "CV the Wrong and the Right way\n\n\nApplying CV only to Step 2 ignores the fact that in Step 1, the procedure has already used the labels of the training data.\nThis is a form of training and must be included in the validation process.\n\nWrong way: Apply cross-validation in step 2.\nRight way: Apply cross-validation to steps 1 and 2.\n\nThis error has happened in many high profile papers, mainly due to a misunderstanding of what CV means and does.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#wrong-way",
    "href": "C1.2-Model_validation_and_Resampling.html#wrong-way",
    "title": "Model validation and Resampling",
    "section": "Wrong Way",
    "text": "Wrong Way",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#right-way",
    "href": "C1.2-Model_validation_and_Resampling.html#right-way",
    "title": "Model validation and Resampling",
    "section": "Right Way",
    "text": "Right Way",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#introducing-the-bootstrap",
    "href": "C1.2-Model_validation_and_Resampling.html#introducing-the-bootstrap",
    "title": "Model validation and Resampling",
    "section": "Introducing the Bootstrap",
    "text": "Introducing the Bootstrap\n\nA flexible and powerful statistical tool that can be used to quantify the uncertainty associated with an estimator or a statistical learning method.\nIt can provide estimates of the standard error or confidence intervals for that estimator/method.\nIndeed, it can be applied to any (or most) situations where one needs to deal with variability, that the method can approximate using resampling.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#where-does-the-name-came-from",
    "href": "C1.2-Model_validation_and_Resampling.html#where-does-the-name-came-from",
    "title": "Model validation and Resampling",
    "section": "Where does the name came from?",
    "text": "Where does the name came from?\n\nThe term derives from the phrase “to pull oneself up by one’s bootstraps”, thought to be based on the XVIIIth century book ” The Surprising Adventures of Baron Munchausen”.\nIt is not the same as the term “bootstrap” used in computer science meaning to “boot” a computer from a set of core instructions.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#origins-of-the-bootstrap",
    "href": "C1.2-Model_validation_and_Resampling.html#origins-of-the-bootstrap",
    "title": "Model validation and Resampling",
    "section": "Origins of the bootstrap",
    "text": "Origins of the bootstrap",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#a-simple-example",
    "href": "C1.2-Model_validation_and_Resampling.html#a-simple-example",
    "title": "Model validation and Resampling",
    "section": "A simple example",
    "text": "A simple example\n\nSuppose that we wish to invest a fixed sum of money in two financial assets that yield returns of \\(X\\) and \\(Y\\), respectively, where \\(X\\) and \\(Y\\) are random quantities.\nWe will invest a fraction \\(\\alpha\\) of our money in \\(X\\), and will invest the remaining \\(1-\\alpha\\) in \\(Y\\).\nWe wish to choose \\(\\alpha\\) to minimize the total risk, or variance, of our investment. In other words, we want to minimize \\(\\operatorname{Var}(\\alpha X+(1-\\alpha) Y)\\).",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#example-continued",
    "href": "C1.2-Model_validation_and_Resampling.html#example-continued",
    "title": "Model validation and Resampling",
    "section": "Example continued",
    "text": "Example continued\n\nThe value that minimizes the risk is: \\[\n\\alpha=\\frac{\\sigma_{Y}^{2}-\\sigma_{X Y}}{\\sigma_{X}^{2}+\\sigma_{Y}^{2}-2 \\sigma_{X Y}}\n\\] where:\n\n\\(\\sigma_{X}^{2}=\\operatorname{Var}(X)\\)\n\\(\\sigma_{Y}^{2}=\\operatorname{Var}(Y)\\) and\n\\(\\sigma_{X Y}=\\operatorname{Cov}(X, Y)\\).",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#example-continued-1",
    "href": "C1.2-Model_validation_and_Resampling.html#example-continued-1",
    "title": "Model validation and Resampling",
    "section": "Example continued",
    "text": "Example continued\n\nThe values of \\(\\sigma_{X}^{2}, \\sigma_{Y}^{2}\\), and \\(\\sigma_{X Y}\\) are unknown.\nWe can compute estimates for these quantities, \\(\\hat{\\sigma}_{X}^{2}, \\hat{\\sigma}_{Y}^{2}\\), and \\(\\hat{\\sigma}_{X Y}\\), using a data set that contains measurements for \\(X\\) and \\(Y\\).\nWe can then estimate the value of \\(\\alpha\\) that minimizes the variance of our investment using \\[\n\\hat{\\alpha}=\\frac{\\hat{\\sigma}_{Y}^{2}-\\hat{\\sigma}_{X Y}}{\\hat{\\sigma}_{X}^{2}+\\hat{\\sigma}_{Y}^{2}-2 \\hat{\\sigma}_{X Y}}\n\\]",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#example-continued-2",
    "href": "C1.2-Model_validation_and_Resampling.html#example-continued-2",
    "title": "Model validation and Resampling",
    "section": "Example continued",
    "text": "Example continued\n\nEach panel displays 100 simulated returns for investments \\(X\\) and \\(Y\\).  From left to right and top to bottom, the resulting estimates for alpha are 0.576, 0.532, 0.657, and 0.651",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#example-how-variable-is-hat-alpha",
    "href": "C1.2-Model_validation_and_Resampling.html#example-how-variable-is-hat-alpha",
    "title": "Model validation and Resampling",
    "section": "Example: How variable is \\(\\hat \\alpha\\)?",
    "text": "Example: How variable is \\(\\hat \\alpha\\)?\n\n\nThe standard deviation of \\(\\hat{\\alpha}\\), can be estimated as:\n\n\nRepeat 1 … 1000 times\n\n\n1.1 Simulate 100 paired observations of \\(X\\) and \\(Y\\).\n1.2 Estimate \\(\\alpha: \\hat{\\alpha}_{1}, \\hat{\\alpha}_{2}, \\ldots, \\hat{\\alpha}_{1000}\\).\n\n\nCompute the standard deviation of \\(\\alpha_i\\), \\(s_{1000}(\\hat{\\alpha}_i)\\) and use it as an estimator of \\(SE (\\hat{\\alpha})\\)\n\n\nThe sample mean of all \\(\\alpha_i\\), \\(\\overline{\\hat{\\alpha}_i}\\) is also a monte carlo estimate of \\(\\alpha\\), although we are not interested in it.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#example-simlation-results",
    "href": "C1.2-Model_validation_and_Resampling.html#example-simlation-results",
    "title": "Model validation and Resampling",
    "section": "Example: Simlation results",
    "text": "Example: Simlation results\n\n\nLeft: A histogram of the estimates of \\(\\alpha\\) obtained by generating 1,000 simulated data sets from the true population. Center: A histogram of the estimates of \\(\\alpha\\) obtained from 1,000 bootstrap samples from a single data set. Right: The estimates of \\(\\alpha\\) displayed in the left and center panels are shown as boxplots.\nFor these simulations the parameters were set to \\(\\sigma_{X}^{2}=1, \\sigma_{Y}^{2}=1.25\\), and \\(\\sigma_{X Y}=0.5\\), and so we know that the true value of \\(\\alpha\\) is 0.6 (indicated by the red line).",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#example-continued-3",
    "href": "C1.2-Model_validation_and_Resampling.html#example-continued-3",
    "title": "Model validation and Resampling",
    "section": "Example continued",
    "text": "Example continued\n\n\nThe mean over all 1,000 estimates for \\(\\alpha\\) is\n\n\\[\n\\bar{\\alpha}=\\frac{1}{1000} \\sum_{r=1}^{1000} \\hat{\\alpha}_{r}=0.5996,\\quad \\text { very close to $\\alpha=0.6$.}\n\\]\n\nThe standard deviation of the estimates computed on the simulated samples: \\[\n\\mathrm{SE}_{Sim}(\\hat{\\alpha})= \\sqrt{\\frac{1}{1000-1} \\sum_{r=1}^{1000}\\left(\\hat{\\alpha}_{r}-\\bar{\\alpha}\\right)^{2}}=0.083\n\\]\nThis gives an idea of the accuracy of \\(\\hat{\\alpha}\\) : \\(\\mathrm{SE}(\\hat{\\alpha}) \\approx \\mathrm{SE}_{Sim}(\\hat{\\alpha})= 0.083\\).\nSo roughly speaking, for a random sample from the population, we would expect \\(\\hat{\\alpha}\\) to differ from \\(\\alpha\\) by approximately 0.08 , on average.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#back-to-the-real-world",
    "href": "C1.2-Model_validation_and_Resampling.html#back-to-the-real-world",
    "title": "Model validation and Resampling",
    "section": "Back to the real world",
    "text": "Back to the real world\n\nThe procedure outlined above, Monte Carlo Sampling, cannot be applied, because for real data we cannot generate new samples from the original population.\nHowever, the bootstrap approach allows us to use a computer to mimic the process of obtaining new data sets, so that we can estimate the variability of our estimate without generating additional samples.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#sampling-the-sample-resampling",
    "href": "C1.2-Model_validation_and_Resampling.html#sampling-the-sample-resampling",
    "title": "Model validation and Resampling",
    "section": "Sampling the sample = resampling",
    "text": "Sampling the sample = resampling\n\nRather than repeatedly obtaining independent data sets from the population, we may obtain distinct data sets by repeatedly sampling observations from the original data set with replacement.\nThis generates a list of “bootstrap data sets” of the same size as our original dataset.\nAs a result some observations may appear more than once in a given bootstrap data set and some not at all.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#resampling-illustrated",
    "href": "C1.2-Model_validation_and_Resampling.html#resampling-illustrated",
    "title": "Model validation and Resampling",
    "section": "Resampling illustrated",
    "text": "Resampling illustrated",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#boot.-estimate-of-std-error",
    "href": "C1.2-Model_validation_and_Resampling.html#boot.-estimate-of-std-error",
    "title": "Model validation and Resampling",
    "section": "Boot. estimate of std error",
    "text": "Boot. estimate of std error\n\nThe standard error of \\(\\alpha\\) can be approximated by the the standard deviation taken on all of these bootstrap estimates using the usual formula: \\[\n\\mathrm{SE}_{B}(\\hat{\\alpha})=\\sqrt{\\frac{1}{B-1} \\sum_{r=1}^{B}\\left(\\hat{\\alpha}^{* r}-\\overline{\\hat{\\alpha^{*}}}\\right)^{2}}\n\\]\nThis quantity, called bootstrap estimate of standard error serves as an estimate of the standard error of \\(\\hat{\\alpha}\\) estimated from the original data set. \\[\n\\mathrm{SE}_{B}(\\hat{\\alpha}) \\approx \\mathrm{SE}(\\hat{\\alpha})\n\\]\nFor this example \\(\\mathrm{SE}_{B}(\\hat{\\alpha})=0.087\\).",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#a-general-picture-for-the-bootstrap",
    "href": "C1.2-Model_validation_and_Resampling.html#a-general-picture-for-the-bootstrap",
    "title": "Model validation and Resampling",
    "section": "A general picture for the bootstrap",
    "text": "A general picture for the bootstrap",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#the-bootstrap-in-general",
    "href": "C1.2-Model_validation_and_Resampling.html#the-bootstrap-in-general",
    "title": "Model validation and Resampling",
    "section": "The bootstrap in general",
    "text": "The bootstrap in general\n\nIn more complex data situations, figuring out the appropriate way to generate bootstrap samples can require some thought.\nFor example, if the data is a time series, we can’t simply sample the observations with replacement (why not?).\nWe can instead create blocks of consecutive observations, and sample those with replacements. Then we paste together sampled blocks to obtain a bootstrap dataset.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#other-uses-of-the-bootstrap",
    "href": "C1.2-Model_validation_and_Resampling.html#other-uses-of-the-bootstrap",
    "title": "Model validation and Resampling",
    "section": "Other uses of the bootstrap",
    "text": "Other uses of the bootstrap\n\nPrimarily used to obtain standard errors of an estimate.\nAlso provides approximate confidence intervals for a population parameter. For example, looking at the histogram in the middle panel of the Figure on slide 29, the \\(5 \\%\\) and \\(95 \\%\\) quantiles of the 1000 values is (.43, .72 ).\nThis represents an approximate \\(90 \\%\\) confidence interval for the true \\(\\alpha\\). How do we interpret this confidence interval?",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#other-uses-of-the-bootstrap-1",
    "href": "C1.2-Model_validation_and_Resampling.html#other-uses-of-the-bootstrap-1",
    "title": "Model validation and Resampling",
    "section": "Other uses of the bootstrap",
    "text": "Other uses of the bootstrap\n\nPrimarily used to obtain standard errors of an estimate.\nAlso provides approximate confidence intervals for a population parameter. For example, looking at the histogram in the middle panel of the Figure on slide 29, the \\(5 \\%\\) and \\(95 \\%\\) quantiles of the 1000 values is (.43, .72 ).\nThis represents an approximate \\(90 \\%\\) confidence interval for the true \\(\\alpha\\). How do we interpret this confidence interval?\nThe above interval is called a Bootstrap Percentile confidence interval. It is the simplest method (among many approaches) for obtaining a confidence interval from the bootstrap.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#can-the-bootstrap-estimate-prediction-error",
    "href": "C1.2-Model_validation_and_Resampling.html#can-the-bootstrap-estimate-prediction-error",
    "title": "Model validation and Resampling",
    "section": "Can the bootstrap estimate prediction error?",
    "text": "Can the bootstrap estimate prediction error?\n\nIn cross-validation, each of the \\(K\\) validation folds is distinct from the other \\(K-1\\) folds used for training: there is no overlap. This is crucial for its success. Why?\nTo estimate prediction error using the bootstrap, we could think about using each bootstrap dataset as our training sample, and the original sample as our validation sample.\nBut each bootstrap sample has significant overlap with the original data. About two-thirds of the original data points appear in each bootstrap sample. Can you prove this?\nThis will cause the bootstrap to seriously underestimate the true prediction error. Why?\nThe other way around- with original sample = training sample, bootstrap dataset \\(=\\) validation sample - is worse!",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#removing-the-overlap",
    "href": "C1.2-Model_validation_and_Resampling.html#removing-the-overlap",
    "title": "Model validation and Resampling",
    "section": "Removing the overlap",
    "text": "Removing the overlap\n\nCan partly fix this problem by only using predictions for those observations that did not (by chance) occur in the current bootstrap sample.\nBut the method gets complicated, and in the end, cross-validation provides a simpler, more attractive approach for estimating prediction error.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#pre-validation",
    "href": "C1.2-Model_validation_and_Resampling.html#pre-validation",
    "title": "Model validation and Resampling",
    "section": "Pre-validation",
    "text": "Pre-validation\n\nIn microarray and other genomic studies, an important problem is to compare a predictor of disease outcome derived from a large number of “biomarkers” to standard clinical predictors.\nComparing them on the same dataset that was used to derive the biomarker predictor can lead to results strongly biased in favor of the biomarker predictor.\nPre-validation can be used to make a fairer comparison between the two sets of predictors.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#motivating-example",
    "href": "C1.2-Model_validation_and_Resampling.html#motivating-example",
    "title": "Model validation and Resampling",
    "section": "Motivating example",
    "text": "Motivating example\nAn example of this problem arose in the paper of van’t Veer et al. Nature (2002). Their microarray data has 4918 genes measured over 78 cases, taken from a study of breast cancer. There are 44 cases in the good prognosis group and 34 in the poor prognosis group. A “microarray” predictor was constructed as follows:\n\n70 genes were selected, having largest absolute correlation with the 78 class labels.\nUsing these 70 genes, a nearest-centroid classifier \\(C(x)\\) was constructed.\nApplying the classifier to the 78 microarrays gave a dichotomous predictor \\(z_{i}=C\\left(x_{i}\\right)\\) for each case \\(i\\).",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#results",
    "href": "C1.2-Model_validation_and_Resampling.html#results",
    "title": "Model validation and Resampling",
    "section": "Results",
    "text": "Results\nComparison of the microarray predictor with some clinical predictors, using logistic regression with outcome prognosis:\n\n\n\nModel\nCoef\nStand. Err.\nZ score\np-value\n\n\n\n\nRe-use\n\n\n\n\n\n\nmicroarray\n4.096\n1.092\n3.753\n0.000\n\n\nangio\n1.208\n0.816\n1.482\n0.069\n\n\ner\n-0.554\n1.044\n-0.530\n0.298\n\n\ngrade\n-0.697\n1.003\n-0.695\n0.243\n\n\npr\n1.214\n1.057\n1.149\n0.125\n\n\nage\n-1.593\n0.911\n-1.748\n0.040\n\n\nsize\n1.483\n0.732\n2.026\n0.021\n\n\n\nPre-validated\n\n\n\n\n\n\n\n\n\n\n\n\nmicroarray\n1.549\n0.675\n2.296\n0.011\n\n\nangio\n1.589\n0.682\n2.329\n0.010\n\n\ner\n-0.617\n0.894\n-0.690\n0.245\n\n\ngrade\n0.719\n0.720\n0.999\n0.159\n\n\npr\n0.537\n0.863\n0.622\n0.267\n\n\nage\n-1.471\n0.701\n-2.099\n0.018\n\n\nsize\n0.998\n0.594\n1.681\n0.046",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#idea-behind-pre-validation",
    "href": "C1.2-Model_validation_and_Resampling.html#idea-behind-pre-validation",
    "title": "Model validation and Resampling",
    "section": "Idea behind Pre-validation",
    "text": "Idea behind Pre-validation\n\nDesigned for comparison of adaptively derived predictors to fixed, pre-defined predictors.\nThe idea is to form a “pre-validated” version of the adaptive predictor: specifically, a “fairer” version that hasn’t “seen” the response \\(y\\).",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#pre-validation-process",
    "href": "C1.2-Model_validation_and_Resampling.html#pre-validation-process",
    "title": "Model validation and Resampling",
    "section": "Pre-validation process",
    "text": "Pre-validation process",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#pre-validation-in-detail-for-this-example",
    "href": "C1.2-Model_validation_and_Resampling.html#pre-validation-in-detail-for-this-example",
    "title": "Model validation and Resampling",
    "section": "Pre-validation in detail for this example",
    "text": "Pre-validation in detail for this example\n\nDivide the cases up into \\(K=13\\) equal-sized parts of 6 cases each.\nSet aside one of parts. Using only the data from the other 12 parts, select the features having absolute correlation at least .3 with the class labels, and form a nearest centroid classification rule.\nUse the rule to predict the class labels for the 13th part\nDo steps 2 and 3 for each of the 13 parts, yielding a “pre-validated” microarray predictor \\(\\tilde{z}_{i}\\) for each of the 78 cases.\nFit a logistic regression model to the pre-validated microarray predictor and the 6 clinical predictors.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C1.2-Model_validation_and_Resampling.html#the-bootstrap-versus-permutation-tests",
    "href": "C1.2-Model_validation_and_Resampling.html#the-bootstrap-versus-permutation-tests",
    "title": "Model validation and Resampling",
    "section": "The Bootstrap versus Permutation tests",
    "text": "The Bootstrap versus Permutation tests\n\nThe bootstrap samples from the estimated population, and uses the results to estimate standard errors and confidence intervals.\nPermutation methods sample from an estimated null distribution for the data, and use this to estimate p-values and False Discovery Rates for hypothesis tests.\nThe bootstrap can be used to test a null hypothesis in simple situations. Eg if \\(\\theta=0\\) is the null hypothesis, we check whether the confidence interval for \\(\\theta\\) contains zero.\nCan also adapt the bootstrap to sample from a null distribution (See Efron and Tibshirani book “An Introduction to the Bootstrap” (1993), chapter 16) but there’s no real advantage over permutations.",
    "crumbs": [
      "Home",
      "Introduction",
      "Model Validation and Resampling"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#motivation",
    "href": "C2.1-DecisionTrees-Slides.html#motivation",
    "title": "Tree based methods",
    "section": "Motivation",
    "text": "Motivation\n\nIn many real-world applications, decisions need to be made based on complex, multi-dimensional data.\nOne goal of statistical analysis is to provide insights and guidance to support these decisions.\nDecision trees provide a way to organize and summarize information in a way that is easy to understand and use in decision-making.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#examples",
    "href": "C2.1-DecisionTrees-Slides.html#examples",
    "title": "Tree based methods",
    "section": "Examples",
    "text": "Examples\n\nA bank needs to have a way to decide if/when a customer can be granted a loan.\nA doctor may need to decide if a patient has to undergo a surgery or a less aggressive treatment.\nA company may need to decide about investing in new technologies or stay with the traditional ones.\n\nIn all those cases a decision tree may provide a structured approach to decision-making that is based on data and can be easily explained and justified.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#an-intuitive-approach",
    "href": "C2.1-DecisionTrees-Slides.html#an-intuitive-approach",
    "title": "Tree based methods",
    "section": "An intuitive approach",
    "text": "An intuitive approach\n\n\nDecisions are often based on asking several questions on available information whose answers induce binary splits on data that end up with some grouping or classification.\n\n\n\n\n\n\nA doctor may classify patients at high or low cardiovascular risk using some type of decision tree",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#introducing-decision-trees",
    "href": "C2.1-DecisionTrees-Slides.html#introducing-decision-trees",
    "title": "Tree based methods",
    "section": "Introducing decision trees",
    "text": "Introducing decision trees\n\nA decision tree is a graphical representation of a series of decisions and their potential outcomes.\nIt is obtained by recursively stratifying or segmenting the feature space into a number of simple regions.\nEach region (decision) corresponds to a node in the tree, and each potential outcome to a branch.\nThe tree structure can be used to guide decision-making based on data.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#types-of-decision-trees",
    "href": "C2.1-DecisionTrees-Slides.html#types-of-decision-trees",
    "title": "Tree based methods",
    "section": "Types of decision trees",
    "text": "Types of decision trees\n\nDecision trees are simple yet reliable predictors that can be use for both classification or prediction\n\nClassification Trees are classifiers, used when the response variable is categorical\nRegression Trees, are regression models used to predict the values of a numerical response variable.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tree-building-with-r",
    "href": "C2.1-DecisionTrees-Slides.html#tree-building-with-r",
    "title": "Tree based methods",
    "section": "Tree building with R",
    "text": "Tree building with R\n\n\n\n\n\nPackage\nAlgorithm\nDataset size\nMissing data handling\nVisual repr\n\n\n\n\nrpart\nRPART\nMedium to large\nPoor\nYes\n\n\ncaret\nVarious\nVarious\nDepends on algorithm\nDepends on algorithm\n\n\ntree\nCART\nSmall to medium\nPoor\nYes",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tree-building-with-python",
    "href": "C2.1-DecisionTrees-Slides.html#tree-building-with-python",
    "title": "Tree based methods",
    "section": "Tree building with Python",
    "text": "Tree building with Python\n\n\n\n\nPackage\nAlgorithm\nDataset size\nMissing data handling\nVisual repr\n\n\n\n\nscikit-learn\nCART, ID3, C4.5\nSmall to medium\nPoor (requires preprocessing)\nYes (plot_tree())\n\n\nxgboost\nGradient Boosted Trees\nMedium to large\nGood (handles missing values natively)\nLimited (requires external tools)\n\n\nlightgbm\nGradient-based One-Side Sampling (GOSS)\nLarge\nGood (handles missing values natively)\nLimited (requires external tools)\n\n\ncatboost\nOblivious Decision Trees\nLarge\nExcellent (handles categorical and missing values natively)\nLimited (built-in tools available)\n\n\npydotplus\nVisualization tool\nN/A\nN/A\nYes (graph-based visualization)",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#building-the-trees",
    "href": "C2.1-DecisionTrees-Slides.html#building-the-trees",
    "title": "Tree based methods",
    "section": "Building the trees",
    "text": "Building the trees\n\nAs with any model, we aim not only at construting trees.\nWe wish to build good trees and, if possible, optimal trees in some sense we decide.\nIn order to build good trees we must decide\n\nHow to construct a tree?\nHow to optimize the tree?\nHow to evaluate it?",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#decision-trees-are-supervised-learners",
    "href": "C2.1-DecisionTrees-Slides.html#decision-trees-are-supervised-learners",
    "title": "Tree based methods",
    "section": "Decision Trees are Supervised Learners",
    "text": "Decision Trees are Supervised Learners\n\nClassification / Regression: Supervised Learning tasks:\nThere is a learning set \\(\\mathcal{L}=\\{(\\mathbf{X_i,Y_i})\\}_{i=1}^n\\)\nAnd depending of \\(\\mathbf{Y}\\) we have:\n\nClassification: \\(\\mathbf{X}\\in\\mathbb{R}^d,\\quad Y\\in\\{-1,+1\\}\\)\nRegression \\(\\mathbf{X}\\in\\mathbb{R}^d,\\quad Y\\in\\mathbb{R}\\).",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#trees-vs-decision-trees",
    "href": "C2.1-DecisionTrees-Slides.html#trees-vs-decision-trees",
    "title": "Tree based methods",
    "section": "Trees vs Decision Trees",
    "text": "Trees vs Decision Trees\n\n\n\n\nA tree is a set of nodes and edges organized in a hierarchical fashion. In contrast to a graph, in a tree there are no loops. \n\n\n\nA decision tree is a tree where each split node stores a boolean test function to be applied to the incoming data.  Each leaf stores the final answer (predictor)",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#additional-notation",
    "href": "C2.1-DecisionTrees-Slides.html#additional-notation",
    "title": "Tree based methods",
    "section": "Additional notation",
    "text": "Additional notation\n\n\n\n\nA node is denoted by \\(t\\).\nThe left and right child nodes are denoted by \\(t_{L}\\) and \\(t_{R}\\)\nThe collection of all nodes in the tree is denoted \\(T\\)\nThe collection of all the leaf nodes is denoted \\(\\tilde{T}\\)\nA split will be denoted by \\(s\\).\nThe set of all splits is denoted by \\(S\\).",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#building-a-tree",
    "href": "C2.1-DecisionTrees-Slides.html#building-a-tree",
    "title": "Tree based methods",
    "section": "Building a tree",
    "text": "Building a tree\n\nA binary decision tree is built by defining a series of (recursive) splits on the feature space.\nThe splits are decided in such a way that the associated learning task is attained\n\nby setting thresholds on the variables values,\nthat induce paths in the tree,\n\nThe ultimate goal of the tree is to be able to use a combination of the splits to accomplish the learning task with as small an error as possible.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#trees-partition-the-space",
    "href": "C2.1-DecisionTrees-Slides.html#trees-partition-the-space",
    "title": "Tree based methods",
    "section": "Trees partition the space",
    "text": "Trees partition the space\n\nA tree represents a recursive splitting of the space.\n\nEvery node of interest corresponds to one region in the original space.\nTwo child nodes occupy two different regions.\nTogether, they yield same region as that of the parent node.\n\nIn the end, every leaf node is assigned with a class (or value) and a test point is assigned with the class (or value) of the leaf node it lands in.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#the-tree-represents-the-splitting",
    "href": "C2.1-DecisionTrees-Slides.html#the-tree-represents-the-splitting",
    "title": "Tree based methods",
    "section": "The tree represents the splitting",
    "text": "The tree represents the splitting",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#different-splits-are-possible",
    "href": "C2.1-DecisionTrees-Slides.html#different-splits-are-possible",
    "title": "Tree based methods",
    "section": "Different splits are possible",
    "text": "Different splits are possible\n\nIt is always possible to split a space in distinct ways\n\n\n\nSome ways perform better than other for a given task, but rarely will they be perfect.\nSo we aim at combining splits to find a better rule.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#building-a-decision-tree",
    "href": "C2.1-DecisionTrees-Slides.html#building-a-decision-tree",
    "title": "Tree based methods",
    "section": "Building a decision tree",
    "text": "Building a decision tree\nTree building involves the following three elements:\n\nThe selection of the splits, i.e., how do we decide which node (region) to split and how to split it?\n\nHow to select from the pool of candidate splits?\nWhat are appropriate goodness of split criteria?\nwhen to declare a node terminal and stop splitting?\n\nHow to assign each terminal node to a class\nHow to ensure the Tree is the best possible we can build.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-1.1---split-selection",
    "href": "C2.1-DecisionTrees-Slides.html#tb-1.1---split-selection",
    "title": "Tree based methods",
    "section": "TB 1.1 - Split selection",
    "text": "TB 1.1 - Split selection\n\n\nTo build a Tree, questions have to be generated that induce splits based on the value of a single variable.\nOrdered variable \\(X_j\\):\n\nIs \\(X_j \\leq c\\)? for all possible thresholds \\(c\\).\nSplit lines: parallel to the coordinates.\n\nCategorical variables, \\(X_j \\in \\{1, 2, \\ldots, M\\}\\):\n\nIs \\(X_j \\in A\\)?, where \\(A \\subseteq M\\) .\n\nThe pool of candidate splits for all \\(p\\) variables is formed by combining all the generated questions.\nWith the pool of candidate splits, next step is to decide which one to use when constructing the decision tree.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-1.2.1---goodness-of-split",
    "href": "C2.1-DecisionTrees-Slides.html#tb-1.2.1---goodness-of-split",
    "title": "Tree based methods",
    "section": "TB 1.2.1 - Goodness of Split",
    "text": "TB 1.2.1 - Goodness of Split\n\nIntuitively, when we split the points we want the region corresponding to each leaf node to be “pure”.\nThat is, we aim at regions where most points belong to the same class.\nWith this goal in mind we select splits by measuring their “goodness of split” using some of the available Impurity fuctions introduced later.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-1.2.2---good-splits-vs-bad-splits",
    "href": "C2.1-DecisionTrees-Slides.html#tb-1.2.2---good-splits-vs-bad-splits",
    "title": "Tree based methods",
    "section": "TB 1.2.2 - Good splits vs bad splits",
    "text": "TB 1.2.2 - Good splits vs bad splits\n\n\n\n\n\n\n\n\n\n\n\n\nPurity not increased\n\n\n\n\n\n\n\n\n\n\n\nPurity increased",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-1.2.3---measuring-homogeneity",
    "href": "C2.1-DecisionTrees-Slides.html#tb-1.2.3---measuring-homogeneity",
    "title": "Tree based methods",
    "section": "TB 1.2.3 - Measuring homogeneity",
    "text": "TB 1.2.3 - Measuring homogeneity\n\nIn order to measure homogeneity,or as called here, purity, of splits we rely on different Impurity functions\nThese functions allow us to quantify the extent of homogeneity for a region containing data points from possibly different classes.\n\nA region will be more pure or more homogeneous the less variable is the set of points it contains.\nIn the image in TB 1.2.2 regions on the right of the image are homogeneous that is purer than the heterogeneous regions on the left.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-1.2.4---impurity-functions",
    "href": "C2.1-DecisionTrees-Slides.html#tb-1.2.4---impurity-functions",
    "title": "Tree based methods",
    "section": "TB 1.2.4 - Impurity functions",
    "text": "TB 1.2.4 - Impurity functions\n\nAn impurity function is a function \\(\\Phi\\) defined on the set of all \\(K\\)-tuples of numbers \\(\\mathbf{p}= \\left(p_{1}, \\cdots, p_{K}\\right)\\) s.t. \\(p_{j} \\geq 0, \\,  \\sum_{j=1}^K p_{j}=1\\), \\[\n\\Phi: \\left(p_{1}, \\cdots, p_{K}\\right) \\rightarrow [0,1]\n\\]\nwith the properties:\n\n\\(\\Phi\\) is maximum only for the uniform distribution, that is all the \\(p_{j}\\) are equal.\n\\(\\Phi\\) is minimum only at points \\((1,0, \\ldots, 0)\\),\\((0,1,0, \\ldots, 0)\\), \\(\\ldots,(0,0, \\ldots, 0,1)\\), i.e., when the probability of being in a certain class is 1 and 0 for all the other classes.\n\\(\\Phi\\) is a symmetric function of \\(p_{1}, \\cdots, p_{K}\\), i.e., if we permute \\(p_{j}\\), \\(\\Phi\\) remains constant.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-1.2.5---some-impurity-functions",
    "href": "C2.1-DecisionTrees-Slides.html#tb-1.2.5---some-impurity-functions",
    "title": "Tree based methods",
    "section": "TB 1.2.5 - Some Impurity Functions",
    "text": "TB 1.2.5 - Some Impurity Functions\nThe functions below are commonly used to measure impurity.\n\n\n\nEntropy: \\(\\Phi_E (\\mathbf{p}) = -\\sum_{j=1}^K p_j\\log (p_j)\\) .\nGini Index: \\(\\Phi_G (\\mathbf{p}) = 1-\\sum_{j=1}^K p_j^2\\).\nMisclassification rate: \\(\\Phi_M (\\mathbf{p}) = \\sum_{i=1}^K p_j(1-p_j)\\).\n\n\n\nIn practice, only the first two are recommended because misclassification rate is not sensitive enough to differences in class probabilies.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-1.2.5-impurity-functions-behavior",
    "href": "C2.1-DecisionTrees-Slides.html#tb-1.2.5-impurity-functions-behavior",
    "title": "Tree based methods",
    "section": "TB 1.2.5 Impurity functions behavior",
    "text": "TB 1.2.5 Impurity functions behavior\n\n\n\n\nNode impurity functions for the two-class case. The entropy function (rescaled) is the red curve, the Gini index is the green curve, and the resubstitution estimate of the misclassification rate is the blue curve.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-1.2.6---impurity-measure-of-a-split",
    "href": "C2.1-DecisionTrees-Slides.html#tb-1.2.6---impurity-measure-of-a-split",
    "title": "Tree based methods",
    "section": "TB 1.2.6 - Impurity measure of a split",
    "text": "TB 1.2.6 - Impurity measure of a split\n\nGiven an impurity function \\(\\Phi\\), a node \\(t\\), and given \\(p(j \\mid t)\\), the estimated posterior probability of class \\(j\\) given node \\(t\\), the impurity measure of \\(t\\), \\(i(t)\\), is:\n\n\\[\ni(t)=\\phi\\left (p(1 \\mid t), p(2 \\mid t), \\ldots, p(K \\mid t)\\right)\n\\]\n\nThat is, the impurity measure of a split (or a node) is the impurity function computed on probabilities associated (conditional) with a node.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-1.2.7---goodness-of-a-split",
    "href": "C2.1-DecisionTrees-Slides.html#tb-1.2.7---goodness-of-a-split",
    "title": "Tree based methods",
    "section": "TB 1.2.7 - Goodness of a split",
    "text": "TB 1.2.7 - Goodness of a split\n\nOnce we have defined \\(i(t)\\), we define the goodness of split \\(s\\) for node \\(t\\), denoted by \\(\\Phi(s, t)\\) :\n\n\\[\n\\Phi(s, t)=\\Delta i(s, t)=i(t)-p_{R} i\\left(t_{R}\\right)-p_{L} i\\left(t_{L}\\right)\n\\]\n\nThe best split for the single variable \\(X_{j}\\) is the one that has the largest value of \\(\\Phi(s, t)\\) over all \\(s \\in \\mathcal{S}_{j}\\), the set of possible distinct splits for \\(X_{j}\\).",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-1.2.8---impurity-score-for-a-node",
    "href": "C2.1-DecisionTrees-Slides.html#tb-1.2.8---impurity-score-for-a-node",
    "title": "Tree based methods",
    "section": "TB 1.2.8 - Impurity score for a node",
    "text": "TB 1.2.8 - Impurity score for a node\n\nThe impurity, \\(i(t)\\), of a node is based solely on the estimated posterior probabilities of the classes\n\nThat is, it doesn’t account for the size of \\(t\\).\n\nThis is done by the impurity score of \\(t\\), defined as \\(I(t)=i(t)\\cdot p(t)\\), a weighted impurity measure of node \\(t\\) that takes into account:\n\nThe estimated posterior probabilities of the classes,\nThe estimated proportion of data that go to node \\(t\\).",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-1.2.9---applications-of-it",
    "href": "C2.1-DecisionTrees-Slides.html#tb-1.2.9---applications-of-it",
    "title": "Tree based methods",
    "section": "TB 1.2.9 - Applications of \\(I(t)\\)",
    "text": "TB 1.2.9 - Applications of \\(I(t)\\)\n\n\\(I(t)\\) can be used to:\n\nDefine the aggregated impurity of a tree, by adding the impurity scores of all terminal leaves.\nProvide a weighted measure of impurity decrease for a split: \\(\\Delta I(s, t)=p(t) \\Delta i(s, t)\\).\nDefine a criteria for stop splitting a tree (see below).",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-1.2.10---entropy-as-an-impurity-measure",
    "href": "C2.1-DecisionTrees-Slides.html#tb-1.2.10---entropy-as-an-impurity-measure",
    "title": "Tree based methods",
    "section": "TB 1.2.10 - Entropy as an impurity measure",
    "text": "TB 1.2.10 - Entropy as an impurity measure\n\nThe entropy of a node, \\(t\\), that is split in \\(n\\) child nodes \\(t_1\\), \\(t_2\\), …, \\(t_n\\), is:\n\n\\[\n\\Phi_E (\\mathbf{p}) = H(\\mathbf{t})=-\\sum_{i=1}^{n} \\underbrace{P\\left(t_{i}\\right)}_{p_i} \\log _{2} P\\left(t_{i}\\right)\n\\]",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-1.2.11---goodness-of-split-based-on-entropy",
    "href": "C2.1-DecisionTrees-Slides.html#tb-1.2.11---goodness-of-split-based-on-entropy",
    "title": "Tree based methods",
    "section": "TB 1.2.11 - Goodness of split based on entropy",
    "text": "TB 1.2.11 - Goodness of split based on entropy\n\nFrom here, an information gain (that is impurity decrease) measure can be introduced.\nInformation theoretic approach that compares\n\nthe entropy of the parent node before the split to\nthat of a weighted sum of the child nodes after the split where the weights are proportional to the number of observations in each node.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-1.2.12---information-gain",
    "href": "C2.1-DecisionTrees-Slides.html#tb-1.2.12---information-gain",
    "title": "Tree based methods",
    "section": "TB 1.2.12 - Information gain",
    "text": "TB 1.2.12 - Information gain\n\nFor a split \\(s\\) and a set of observations (a node) \\(t\\), information gain is defined as:\n\n\\[\n\\begin{aligned}\n& IG(t, s)=\\text { (original entr.) }-(\\text { entr. after split) } \\\\\n& IG(t, s)=H(t)-\\sum_{i=1}^{n} \\frac{\\left|t_{i}\\right|}{t} H\\left(x_{i}\\right)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#example-1-pears-vs-apples",
    "href": "C2.1-DecisionTrees-Slides.html#example-1-pears-vs-apples",
    "title": "Tree based methods",
    "section": "Example 1: Pears vs Apples",
    "text": "Example 1: Pears vs Apples\n\n\nConsider the problem of designing an algorithm to automatically differentiate between apples and pears (class labels) given only their width and height measurements (features).\n\n\n\n\n\nWidth\nHeight\nFruit\n\n\n\n\n7.1\n7.3\nApple\n\n\n7.9\n7.5\nApple\n\n\n7.4\n7.0\nApple\n\n\n8.2\n7.3\nApple\n\n\n7.6\n6.9\nApple\n\n\n7.8\n8.0\nApple\n\n\n7.0\n7.5\nPear\n\n\n7.1\n7.9\nPear\n\n\n6.8\n8.0\nPear\n\n\n6.6\n7.7\nPear\n\n\n7.3\n8.2\nPear\n\n\n7.2\n7.9\nPear",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#example-1.-entropy-calculation",
    "href": "C2.1-DecisionTrees-Slides.html#example-1.-entropy-calculation",
    "title": "Tree based methods",
    "section": "Example 1. Entropy Calculation",
    "text": "Example 1. Entropy Calculation",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#example-1.-information-gain",
    "href": "C2.1-DecisionTrees-Slides.html#example-1.-information-gain",
    "title": "Tree based methods",
    "section": "Example 1. Information Gain",
    "text": "Example 1. Information Gain",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb.-1.3.1---when-to-stop-growing",
    "href": "C2.1-DecisionTrees-Slides.html#tb.-1.3.1---when-to-stop-growing",
    "title": "Tree based methods",
    "section": "TB. 1.3.1 - When to stop growing",
    "text": "TB. 1.3.1 - When to stop growing\n\nMaximizing information gain is one possible criteria to choose among splits.\nIn order to avoid excessive complexity it is usually decided to stop splitting when information gain does not compensate for increase in complexity.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-1.3.2-stop-splitting-criteria",
    "href": "C2.1-DecisionTrees-Slides.html#tb-1.3.2-stop-splitting-criteria",
    "title": "Tree based methods",
    "section": "TB 1.3.2 Stop splitting criteria",
    "text": "TB 1.3.2 Stop splitting criteria\n\nIn practice, stop splitting is decided when: \\[\n\\max _{s \\in S} \\Delta I(s, t)&lt;\\beta,\n\\]where:\n\n\\(\\Delta I\\) represents the information gain associated with an optimal split \\(s\\) and a node \\(t\\),\nand \\(\\beta\\) is a pre-determined threshold.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-1.-summary",
    "href": "C2.1-DecisionTrees-Slides.html#tb-1.-summary",
    "title": "Tree based methods",
    "section": "TB 1. Summary",
    "text": "TB 1. Summary\n\n\nDecision trees are built by iteratively partitioning data into smaller regions based on feature values.\nSplits are aimed at producing purer nodes, that contains mostly data from one class.\nHomogeneity is measured by impurity functions such as Entropy, Gini Index or Misclassification rate\nThe best split is chosen from candidate splits as the one that maximizes impurity reduction for example through Information Gain\nTree growth continues until impurity reduction is minimal, or a predefined depth or node size threshold is reached.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#example-2.-the-pima-database",
    "href": "C2.1-DecisionTrees-Slides.html#example-2.-the-pima-database",
    "title": "Tree based methods",
    "section": "Example 2. The PIMA database",
    "text": "Example 2. The PIMA database\n\nThe Pima Indian Diabetes dataset contains 768 individuals (female) and 9 clinical variables.\n\n\n\nRows: 768\nColumns: 9\n$ pregnant &lt;dbl&gt; 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, 0, 7, 1, 1…\n$ glucose  &lt;dbl&gt; 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168, 139,…\n$ pressure &lt;dbl&gt; 72, 66, 64, 66, 40, 74, 50, NA, 70, 96, 92, 74, 80, 60, 72, N…\n$ triceps  &lt;dbl&gt; 35, 29, NA, 23, 35, NA, 32, NA, 45, NA, NA, NA, NA, 23, 19, N…\n$ insulin  &lt;dbl&gt; NA, NA, NA, 94, 168, NA, 88, NA, 543, NA, NA, NA, NA, 846, 17…\n$ mass     &lt;dbl&gt; 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, NA, 37.…\n$ pedigree &lt;dbl&gt; 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.134, 0.158…\n$ age      &lt;dbl&gt; 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 59, 51, 3…\n$ diabetes &lt;fct&gt; pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg, pos, n…",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#example-2.-looking-at-the-data",
    "href": "C2.1-DecisionTrees-Slides.html#example-2.-looking-at-the-data",
    "title": "Tree based methods",
    "section": "Example 2. Looking at the data",
    "text": "Example 2. Looking at the data\n\nThese Variables are known to be related with cardiovascular diseases.\nIt seems intuitive to use these variables to decide if a person is affected by diabetes\n\n\n\n             p0      p25      p50       p75   p100  hist\ndiabetes     NA       NA       NA        NA     NA  &lt;NA&gt;\npregnant  0.000  1.00000   3.0000   6.00000  17.00 ▇▃▂▁▁\nglucose  44.000 99.00000 117.0000 141.00000 199.00 ▁▇▇▃▂\npressure 24.000 64.00000  72.0000  80.00000 122.00 ▁▃▇▂▁\ntriceps   7.000 22.00000  29.0000  36.00000  99.00 ▆▇▁▁▁\ninsulin  14.000 76.25000 125.0000 190.00000 846.00 ▇▂▁▁▁\nmass     18.200 27.50000  32.3000  36.60000  67.10 ▅▇▃▁▁\npedigree  0.078  0.24375   0.3725   0.62625   2.42 ▇▃▁▁▁\nage      21.000 24.00000  29.0000  41.00000  81.00 ▇▃▁▁▁",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#ex.-2.-building-a-classification-tree",
    "href": "C2.1-DecisionTrees-Slides.html#ex.-2.-building-a-classification-tree",
    "title": "Tree based methods",
    "section": "Ex. 2. Building a classification tree",
    "text": "Ex. 2. Building a classification tree\n\nWe wish to predict the probability of individuals in being diabete-positive or negative.\n\nWe start building a tree with all the variables\n\n\n\nlibrary(rpart)\nmodel1 &lt;- rpart(diabetes ~., data = PimaIndiansDiabetes2)",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#ex.2.-plotting-the-tree-1",
    "href": "C2.1-DecisionTrees-Slides.html#ex.2.-plotting-the-tree-1",
    "title": "Tree based methods",
    "section": "Ex.2. Plotting the tree (1)",
    "text": "Ex.2. Plotting the tree (1)\n\n\n\n\n\nEven without domain expertise the model seems reasonable",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#ex.-2.-plotting-the-tree-nicer",
    "href": "C2.1-DecisionTrees-Slides.html#ex.-2.-plotting-the-tree-nicer",
    "title": "Tree based methods",
    "section": "Ex. 2. Plotting the tree (Nicer)",
    "text": "Ex. 2. Plotting the tree (Nicer)\n\nThe tree plotted with the rpart.plot package.\nEach node shows: (1) the predicted class (‘neg’ or ‘pos’), (2) the predicted probability, (3) the percentage of observations in the node.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-2---class-assignment",
    "href": "C2.1-DecisionTrees-Slides.html#tb-2---class-assignment",
    "title": "Tree based methods",
    "section": "TB 2 - Class Assignment",
    "text": "TB 2 - Class Assignment\n\nThe decision tree classifies new data points as follows.\n\nWe let a data point pass down the tree and see which leaf node it lands in.\nThe class of the leaf node is assigned to the new data point. Basically, all the points that land in the same leaf node will be given the same class.\nThis is similar to k-means or any prototype method.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-2.1---class-assignment-rules",
    "href": "C2.1-DecisionTrees-Slides.html#tb-2.1---class-assignment-rules",
    "title": "Tree based methods",
    "section": "TB 2.1 - Class Assignment Rules",
    "text": "TB 2.1 - Class Assignment Rules\n\n\nA class assignment rule assigns a class \\(j=1, \\cdots, K\\) to every terminal (leaf) node \\(t \\in \\tilde{T}\\).\nThe class is assigned to node \\(t\\) is denoted by \\(\\kappa(t)\\),\n\nE.g., if \\(\\kappa(t)=2\\), all the points in node \\(t\\) would be assigned to class 2.\n\nIf we use 0-1 loss, the class assignment rule picks the class with maximum posterior probability: \\[\n\\kappa(t)=\\arg \\max _{j} p(j \\mid t)\n\\]",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-2.2.-estimating-the-error-rate-1",
    "href": "C2.1-DecisionTrees-Slides.html#tb-2.2.-estimating-the-error-rate-1",
    "title": "Tree based methods",
    "section": "TB 2.2. Estimating the error rate (1)",
    "text": "TB 2.2. Estimating the error rate (1)\n\nLet’s assume we have built a tree and have the classes assigned for the leaf nodes.\nGoal: estimate the classification error rate for this tree.\nWe use the resubstitution estimate \\(r(t)\\) for the probability of misclassification, given that a case falls into node \\(t\\). This is:\n\n\\[\nr(t)=1-\\max _{j} p(j \\mid t)=1-p(\\kappa(t) \\mid t)\n\\]",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-2.3.-estimating-the-error-rate-2",
    "href": "C2.1-DecisionTrees-Slides.html#tb-2.3.-estimating-the-error-rate-2",
    "title": "Tree based methods",
    "section": "TB 2.3. Estimating the error rate (2)",
    "text": "TB 2.3. Estimating the error rate (2)\n\nDenote \\(R(t)=r(t) p(t)\\), that is the miscclassification error rate weighted by the probability of the node.\nThe resubstitution estimation for the overall misclassification rate \\(R(T)\\) of the tree classifier \\(T\\) is:\n\n\\[\nR(T)=\\sum_{t \\in \\tilde{T}} R(t)\n\\]",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#example-2-individual-prediction",
    "href": "C2.1-DecisionTrees-Slides.html#example-2-individual-prediction",
    "title": "Tree based methods",
    "section": "Example 2: Individual prediction",
    "text": "Example 2: Individual prediction\nConsider individuals 521 and 562\n\n\n    pregnant glucose pressure triceps insulin mass pedigree age diabetes\n521        2      68       70      32      66 25.0    0.187  25      neg\n562        0     198       66      32     274 41.3    0.502  28      pos\n\n\n\n\n521 562 \nneg pos \nLevels: neg pos\n\n\n\nIf we follow individuals 521 and 562 along the tree, we reach the same prediction.\nThe tree provides not only a classification but also an explanation.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#example-2-how-accurate-is-the-model",
    "href": "C2.1-DecisionTrees-Slides.html#example-2-how-accurate-is-the-model",
    "title": "Tree based methods",
    "section": "Example 2: How accurate is the model?",
    "text": "Example 2: How accurate is the model?\n\nIt is straightforward to obtain a simple performance measure.\n\n\npredicted.classes&lt;- predict(model1, PimaIndiansDiabetes2, \"class\")\nmean(predicted.classes == PimaIndiansDiabetes2$diabetes)\n\n[1] 0.8294271",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#example-2-is-the-tree-optimal",
    "href": "C2.1-DecisionTrees-Slides.html#example-2-is-the-tree-optimal",
    "title": "Tree based methods",
    "section": "Example 2: Is the Tree optimal?",
    "text": "Example 2: Is the Tree optimal?\n\nThe question becomes harder when we go back and ask if we obtained the best possible tree.\nIn order to answer this question we must study tree construction in more detail.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-3.1-optimizing-the-tree",
    "href": "C2.1-DecisionTrees-Slides.html#tb-3.1-optimizing-the-tree",
    "title": "Tree based methods",
    "section": "TB 3.1 Optimizing the Tree",
    "text": "TB 3.1 Optimizing the Tree\n\nTrees obtained by looking for optimal splits tend to overfit: good for the data in the tree, but generalize badly and tend to fail more in predictions.\nIn order to reduce complexity and overfitting, while keeping the tree as good as possible, tree pruning may be applied.\nPruning works removing branches that are unlikely to improve the accuracy of the model on new data.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-3.2-pruning-methods",
    "href": "C2.1-DecisionTrees-Slides.html#tb-3.2-pruning-methods",
    "title": "Tree based methods",
    "section": "TB 3.2 Pruning methods",
    "text": "TB 3.2 Pruning methods\n\nThere are different pruning methods, but the most common one is the cost-complexity pruning algorithm, also known as the weakest link pruning.\nThe algorithm works by adding a penalty term to the misclassification rate of the terminal nodes:\n\n\\[\nR_\\alpha(T) =R(T)+\\alpha|T|\n\\] where \\(\\alpha\\) is the parameter that controls the trade-off between tree complexity and accuracy.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tb-3.3-cost-complexity-pruning",
    "href": "C2.1-DecisionTrees-Slides.html#tb-3.3-cost-complexity-pruning",
    "title": "Tree based methods",
    "section": "TB 3.3 Cost complexity pruning",
    "text": "TB 3.3 Cost complexity pruning\n\nStart by building a large tree that overfits the data.\nThen, use cross-validation to estimate the optimal value of alpha that minimizes the generalization error.\nFinally, prune the tree by removing the branches that have a smaller improvement in impurity than the penalty term multiplied by alpha.\nIterate the process until no more branches can be pruned, or until a minimum tree size is reached.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#regression-modelling-with-trees",
    "href": "C2.1-DecisionTrees-Slides.html#regression-modelling-with-trees",
    "title": "Tree based methods",
    "section": "Regression modelling with trees",
    "text": "Regression modelling with trees\n\nWhen the response variable is numeric, decision trees are regression trees.\nOption of choice for distinct reasons\n\nThe relation between response and potential explanatory variables is not linear.\nPerform automatic variable selection.\nEasy to interpret, visualize, explain.\nRobust to outliers and can handle missing data",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#classification-vs-regression-trees",
    "href": "C2.1-DecisionTrees-Slides.html#classification-vs-regression-trees",
    "title": "Tree based methods",
    "section": "Classification vs Regression Trees",
    "text": "Classification vs Regression Trees\n\n\n\n\n\n\n\n\n\nAspect\nRegression Trees\nClassification Trees\n\n\n\n\nOutcome type\nContinuous\nCategorical\n\n\nGoal\nPredict numerical value\nPredict class label\n\n\nSplitting criteria\nMean Squared Error, Mean Abs. Error\nGini Impurity, Entropy, etc.\n\n\nLeaf node  prediction\nMean/median of target variable in that region\nMode/majority class of target variable …\n\n\nUse cases\nPredicting housing prices, predicting stock prices\nPredicting customer churn, predicting high/low risk in diease\n\n\nEvaluation metric\nMean Squared Error, Mean Absolute Error, R-square\nAccuracy, Precision, Recall, F1-score, etc.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#regression-tree-example",
    "href": "C2.1-DecisionTrees-Slides.html#regression-tree-example",
    "title": "Tree based methods",
    "section": "Regression tree example",
    "text": "Regression tree example\n\nThe airquality dataset from the datasets package contains daily air quality measurements in New York from May through September of 1973 (153 days).\nThe main variables include:\n\nOzone: the mean ozone (in parts per billion) …\nSolar.R: the solar radiation (in Langleys) …\nWind: the average wind speed (in mph) …\nTemp: the maximum daily temperature (ºF) …\n\nMain goal : Predict ozone concentration.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#non-linear-relationships",
    "href": "C2.1-DecisionTrees-Slides.html#non-linear-relationships",
    "title": "Tree based methods",
    "section": "Non linear relationships!",
    "text": "Non linear relationships!\n\naq &lt;- datasets::airquality\ncolor &lt;- adjustcolor(\"forestgreen\", alpha.f = 0.5)\nps &lt;- function(x, y, ...) {  # custom panel function\n  panel.smooth(x, y, col = color, col.smooth = \"black\", cex = 0.7, lwd = 2)\n}\npairs(aq, cex = 0.7, upper.panel = ps, col = color)",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#building-the-tree-1-splitting",
    "href": "C2.1-DecisionTrees-Slides.html#building-the-tree-1-splitting",
    "title": "Tree based methods",
    "section": "Building the tree (1): Splitting",
    "text": "Building the tree (1): Splitting\n\n\nConsider:\n\nall predictors \\(X_1, \\dots, X_n\\), and\nall values of cutpoint \\(s\\) for each predictor and\n\nFor each predictor find boxes \\(R_1, \\ldots, R_J\\) that minimize the Resiudal Sum of Squares, RSS:\n\n\\[\n\\text{RSS}= \\sum_{j=1}^J \\sum_{i \\in R_j}\\left(y_i-\\hat{y}_{R_j}\\right)^2\n\\]\n\\(\\hat{y}_{R_j}\\): mean response for the training observations within the \\(j\\) th box.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#building-the-tree-2-splitting",
    "href": "C2.1-DecisionTrees-Slides.html#building-the-tree-2-splitting",
    "title": "Tree based methods",
    "section": "Building the tree (2): Splitting",
    "text": "Building the tree (2): Splitting\n\n\nTo do this, define the pair of half-planes\n\n\\[\nR_1(j, s)=\\left\\{X \\mid X_j&lt;s\\right\\} \\text { and } R_2(j, s)=\\left\\{X \\mid X_j \\geq s\\right\\}\n\\]\nand seek the value of \\(j\\) and \\(s\\) that minimize the equation:\n\\[\n\\sum_{i: x_i \\in R_1(j, s)}\\left(y_i-\\hat{y}_{R_1}\\right)^2+\\sum_{i: x_i \\in R_2(j, s)}\\left(y_i-\\hat{y}_{R_2}\\right)^2.\n\\]",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#building-the-tree-3-prediction",
    "href": "C2.1-DecisionTrees-Slides.html#building-the-tree-3-prediction",
    "title": "Tree based methods",
    "section": "Building the tree (3): Prediction",
    "text": "Building the tree (3): Prediction\n\n\n\nOnce the regions have been created we predict the response using the mean of the trainig observations in the region to which that observation belongs.\nIn the example, for an observation belonging to the shaded region, the prediction would be:\n\n\\[\n\\hat{y} =\\frac{1}{4}(y_2+y_3+y_5+y_9)\n\\]",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#example-a-regression-tree",
    "href": "C2.1-DecisionTrees-Slides.html#example-a-regression-tree",
    "title": "Tree based methods",
    "section": "Example: A regression tree",
    "text": "Example: A regression tree\n\nset.seed(123)\ntrain &lt;- sample(1:nrow(aq), size = nrow(aq)*0.7)\naq_train &lt;- aq[train,]\naq_test  &lt;- aq[-train,]\naq_regresion &lt;- tree::tree(formula = Ozone ~ ., \n                           data = aq_train, split = \"deviance\")\nsummary(aq_regresion)\n\n\nRegression tree:\ntree::tree(formula = Ozone ~ ., data = aq_train, split = \"deviance\")\nVariables actually used in tree construction:\n[1] \"Temp\"    \"Wind\"    \"Solar.R\" \"Day\"    \nNumber of terminal nodes:  8 \nResidual mean deviance:  285.6 = 21420 / 75 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-58.2000  -7.9710  -0.4545   0.0000   5.5290  81.8000",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#example-plot-the-tree",
    "href": "C2.1-DecisionTrees-Slides.html#example-plot-the-tree",
    "title": "Tree based methods",
    "section": "Example: Plot the tree",
    "text": "Example: Plot the tree",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#prunning-the-tree-1",
    "href": "C2.1-DecisionTrees-Slides.html#prunning-the-tree-1",
    "title": "Tree based methods",
    "section": "Prunning the tree (1)",
    "text": "Prunning the tree (1)\n\nAs before, cost-complexity prunning can be applied\nWe consider a sequence of trees indexed by a nonnegative tuning parameter \\(\\alpha\\).\nFor each value of \\(\\alpha\\) there corresponds a subtree \\(T \\subset T_0\\) such that:\n\n\n\\[\n\\sum_{m=1}^{|T|} \\sum_{y_i \\in R_m}\n\\left(y_i -\\hat{y}_{R_m}\\right)^2+\n\\alpha|T|\\quad (*)\n\\label{prunning}\n\\]\n\nis as small as possible.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#tuning-parameter-alpha",
    "href": "C2.1-DecisionTrees-Slides.html#tuning-parameter-alpha",
    "title": "Tree based methods",
    "section": "Tuning parameter \\(\\alpha\\)",
    "text": "Tuning parameter \\(\\alpha\\)\n\n\\(\\alpha\\) controls a trade-off between the subtree’s complexity and its fit to the training data.\nWhen \\(\\alpha=0\\), then the subtree \\(T\\) will simply equal \\(T_0\\).\nAs \\(\\alpha\\) increases, there is a price to pay for having a tree with many terminal nodes, and so (*) will tend to be minimized for a smaller subtree.\nEquation (*1) is reminiscent of the lasso.\n\\(\\alpha\\) can be chosen by cross-validation .",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#optimizing-the-tree-alpha",
    "href": "C2.1-DecisionTrees-Slides.html#optimizing-the-tree-alpha",
    "title": "Tree based methods",
    "section": "Optimizing the tree (\\(\\alpha\\))",
    "text": "Optimizing the tree (\\(\\alpha\\))\n\nUse recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.\nApply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of \\(\\alpha\\).\nUse K-fold cross-validation to choose \\(\\alpha\\). That is, divide the training observations into \\(K\\) folds. For each \\(k=1, \\ldots, K\\) :\n\nRepeat Steps 1 and 2 on all but the \\(k\\) th fold of the training data.\nEvaluate the mean squared prediction error on the data in the left-out \\(k\\) th fold, as a function of \\(\\alpha\\).\n\n\nAverage the results for each value of \\(\\alpha\\). Pick \\(\\alpha\\) to minimize the average error.\n\nReturn the subtree from Step 2 that corresponds to the chosen value of \\(\\alpha\\).",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#example-prune-the-tree",
    "href": "C2.1-DecisionTrees-Slides.html#example-prune-the-tree",
    "title": "Tree based methods",
    "section": "Example: Prune the tree",
    "text": "Example: Prune the tree\n\ncv_aq &lt;- tree::cv.tree(aq_regresion, K = 5)\noptimal_size &lt;-  rev(cv_aq$size)[which.min(rev(cv_aq$dev))]\naq_final_tree &lt;- tree::prune.tree(\n                 tree = aq_regresion,\n                 best = optimal_size\n               )\nsummary(aq_final_tree)\n\n\nRegression tree:\ntree::tree(formula = Ozone ~ ., data = aq_train, split = \"deviance\")\nVariables actually used in tree construction:\n[1] \"Temp\"    \"Wind\"    \"Solar.R\" \"Day\"    \nNumber of terminal nodes:  8 \nResidual mean deviance:  285.6 = 21420 / 75 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-58.2000  -7.9710  -0.4545   0.0000   5.5290  81.8000 \n\n\nIn this example pruning does not improve the tree.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#trees-have-many-advantages",
    "href": "C2.1-DecisionTrees-Slides.html#trees-have-many-advantages",
    "title": "Tree based methods",
    "section": "Trees have many advantages",
    "text": "Trees have many advantages\n\nTrees are very easy to explain to people.\nDecision trees may be seen as good mirrors of human decision-making.\nTrees can be displayed graphically, and are easily interpreted even by a non-expert.\nTrees can easily handle qualitative predictors without the need to create dummy variables.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#but-they-come-at-a-price",
    "href": "C2.1-DecisionTrees-Slides.html#but-they-come-at-a-price",
    "title": "Tree based methods",
    "section": "But they come at a price",
    "text": "But they come at a price\n\nTrees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches.\nAdditionally, trees can be very non-robust: a small change in the data can cause a large change in the final estimated tree.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#references",
    "href": "C2.1-DecisionTrees-Slides.html#references",
    "title": "Tree based methods",
    "section": "References",
    "text": "References\n\nA. Criminisi, J. Shotton and E. Konukoglu (2011) Decision Forests for Classifcation, Regression … Microsoft Research technical report TR-2011-114\nEfron, B., Hastie T. (2016) Computer Age Statistical Inference. Cambridge University Press. Web site\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: Data mining, inference, and prediction. Springer.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112). Springer. Web site",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#complementary-references",
    "href": "C2.1-DecisionTrees-Slides.html#complementary-references",
    "title": "Tree based methods",
    "section": "Complementary references",
    "text": "Complementary references\n\nBreiman, L., Friedman, J., Stone, C. J., & Olshen, R. A. (1984). Classification and regression trees. CRC press.\nBrandon M. Greenwell (202) Tree-Based Methods for Statistical Learning in R. 1st Edition. Chapman and Hall/CRC DOI:https://doi.org/10.1201/9781003089032\nGenuer R., Poggi, J.M. (2020) Random Forests with R. Springer ed. (UseR!)",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C2.1-DecisionTrees-Slides.html#resources",
    "href": "C2.1-DecisionTrees-Slides.html#resources",
    "title": "Tree based methods",
    "section": "Resources",
    "text": "Resources\n\nApplied Data Mining and Statistical Learning (Penn Statte-University)\nR for statistical learning\nCART Model: Decision Tree Essentials\nAn Introduction to Recursive Partitioning Using the RPART Routines",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#session-outline",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#session-outline",
    "title": "Introduction to Deep Learning",
    "section": "Session Outline",
    "text": "Session Outline\n\nTowards Deep learning\nLearning visual features\n \nConvolutional Neural Networks  \nBuilding and Training CNNs  \nApplications of CNNs",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#deep-neural-networks",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#deep-neural-networks",
    "title": "Introduction to Deep Learning",
    "section": "Deep Neural Networks",
    "text": "Deep Neural Networks\n\nNeural Networks may have distinct levels of complexity.\n\n\nSource: ‘Deep Learning’ course, by Andrew Ng in Coursera & deeplearning.ai",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#from-shallow-to-deep-nns",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#from-shallow-to-deep-nns",
    "title": "Introduction to Deep Learning",
    "section": "From Shallow to Deep NNs",
    "text": "From Shallow to Deep NNs\n\n“Deep Neural networks” are NNs with several hidden layers.\nThe real shift, from Shallow to Deep NNs, is not (only) the number of layers.\nThe difference comes from realizing that\n\nSome tasks as digit recognition, could be solved decently well using a “brute force” approach\nOther more complex tasks, such as distinguishing a human face in an image, where hard to solve witht that “brute” force approach.\n\nThis is often associated to working with structured vs unstructured data",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#structured-unstructured-data",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#structured-unstructured-data",
    "title": "Introduction to Deep Learning",
    "section": "Structured-Unstructured data",
    "text": "Structured-Unstructured data\n\n‘Source: Generative Deep Learning. David Foster (Fig. 2.1)’",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#images-are-unstructured-data",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#images-are-unstructured-data",
    "title": "Introduction to Deep Learning",
    "section": "Images are unstructured data",
    "text": "Images are unstructured data\nTask: Distinguish human from non-human in an image\n\nSource: ‘Neural Networkls and Deep Learning’ course, by Michael Nielsen",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#face-recognition-problem",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#face-recognition-problem",
    "title": "Introduction to Deep Learning",
    "section": "Face recognition problem",
    "text": "Face recognition problem\n\nThis can be attacked as the digit recognition problem (output of “yes” and “no”), although the cost of training the network would be much higher.\nAn alternative approach may be to try to solve the problem hierarchically.\n\nWe start by tying to find edges in the figure\nIn the parts with edges we “look around” to find face pieces, a nose, an eye, an eyebrow …\nAs we locate the pieces we look for their optimal combination.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#a-hierarchy-of-complexity",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#a-hierarchy-of-complexity",
    "title": "Introduction to Deep Learning",
    "section": "A hierarchy of complexity",
    "text": "A hierarchy of complexity\n\nSource: ‘Deep Learning’ course, by Andrew Ng in Coursera & deeplearning.ai",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#a-hierarchy-of-complexity-1",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#a-hierarchy-of-complexity-1",
    "title": "Introduction to Deep Learning",
    "section": "A hierarchy of complexity",
    "text": "A hierarchy of complexity\n\nEach layer has a more complex task, but it receives better information.\n\nIf we can solve the sub-problems using ANNs,\nPerhaps we can build a neural network for face-detection, by combining the networks for the sub-problems.\n\n\n\n\n\nNetworks with this kind of many-layer structure - two or more hidden layers - are called deep neural networks.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#automatic-tuning",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#automatic-tuning",
    "title": "Introduction to Deep Learning",
    "section": "Automatic tuning",
    "text": "Automatic tuning\n\nIn order for these networks to succeed it is important not having to hand-craft the complicated structure of weights and biases required for such hierarchy of layers and functions.\nIn 2006 techniques enabling learning in Deep Neural Nets were developed.\nThese deep learning techniques are based on stochastic gradient descent and backpropagation, but also introduce new ideas.\nIt turns out that equiped with such techniques, deep neural networks perform much better on many problems than shallow neural networks.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#shallow-vs-deep-nns",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#shallow-vs-deep-nns",
    "title": "Introduction to Deep Learning",
    "section": "Shallow vs Deep NNs",
    "text": "Shallow vs Deep NNs\n\nSource: ‘Deep Learning’ course, by Andrew Ng in Coursera & deeplearning.ai",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#we-want-computers-that-can-see",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#we-want-computers-that-can-see",
    "title": "Introduction to Deep Learning",
    "section": "We want computers that can see",
    "text": "We want computers that can see\nGoal: Computer systems able to see what is present in the world, but also to predict and anticipate events.\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#dnn-in-computer-vision-systems",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#dnn-in-computer-vision-systems",
    "title": "Introduction to Deep Learning",
    "section": "DNN in computer vision systems",
    "text": "DNN in computer vision systems\nDeep Learning enables many systems to undertake a variety of computer vision related tasks.\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#facial-detection-and-recognition",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#facial-detection-and-recognition",
    "title": "Introduction to Deep Learning",
    "section": "Facial detection and recognition",
    "text": "Facial detection and recognition\nIn particular it enables automatic feature extraction.\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#autonomous-driving",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#autonomous-driving",
    "title": "Introduction to Deep Learning",
    "section": "Autonomous driving",
    "text": "Autonomous driving\nAutonomus Driving would not be possible without the possibility of performing Automatic Feature Extraction\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#medicine-biology-self-care",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#medicine-biology-self-care",
    "title": "Introduction to Deep Learning",
    "section": "Medicine, biology, self care",
    "text": "Medicine, biology, self care\nNeither would systems for automatic disease detection be able to distinguish healthy from affected people though images.\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#images-are-numbers",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#images-are-numbers",
    "title": "Introduction to Deep Learning",
    "section": "Images are numbers",
    "text": "Images are numbers\n\nTo a computer images, of course, are numbers.\nAn RGB image is a NxNx3 matrix of numbers [0,255]\n\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#main-tasks-in-computer-vision",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#main-tasks-in-computer-vision",
    "title": "Introduction to Deep Learning",
    "section": "Main tasks in Computer Vision:",
    "text": "Main tasks in Computer Vision:\n\nRegression: Output variable takes continuous value. E.g. Distance to target\nClassification: Output variable takes class labels. E.g. Probability of belonging to a class\n\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#high-level-feature-detection",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#high-level-feature-detection",
    "title": "Introduction to Deep Learning",
    "section": "High level feature detection",
    "text": "High level feature detection\n\nA different set of features characterize each image\nBefore attempting to build a computer vision system, we need to be aware of what feature keys are in our data that need to be identified and detected.\n\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#how-to-do-feature-extraction",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#how-to-do-feature-extraction",
    "title": "Introduction to Deep Learning",
    "section": "How to do feature extraction",
    "text": "How to do feature extraction\n\nManual feature extraction is hard!\nFeature characterization needs to define a hierarchy of features allowing an increasing level of detail.\nDeep Neural networks can do this in a hierarchichal fashion!\n\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#feature-extraction-with-dense-nn",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#feature-extraction-with-dense-nn",
    "title": "Introduction to Deep Learning",
    "section": "Feature extraction with dense NN",
    "text": "Feature extraction with dense NN\n\nFully connected NN could, in principle, be used to learn visual features\n\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#accounting-for-spatial-structure",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#accounting-for-spatial-structure",
    "title": "Introduction to Deep Learning",
    "section": "Accounting for spatial structure",
    "text": "Accounting for spatial structure\n\nImages have a spatial structure.\nHow can this be used to inform the architecture of the Network?\n\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#extending-the-idea-with-patches",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#extending-the-idea-with-patches",
    "title": "Introduction to Deep Learning",
    "section": "Extending the idea with patches",
    "text": "Extending the idea with patches\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#use-filters-to-extract-features",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#use-filters-to-extract-features",
    "title": "Introduction to Deep Learning",
    "section": "Use filters to extract features",
    "text": "Use filters to extract features\n\nFilters can be used to extract local features\n\nA filter is a set of weights\n\nDifferent filters can extract different .\nFilters that matter in one part of the input should matter elsewhere so:\n\nParameters of each filter are spatially shared.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#shifting-filters-for-extraction",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#shifting-filters-for-extraction",
    "title": "Introduction to Deep Learning",
    "section": "Shifting filters for Extraction",
    "text": "Shifting filters for Extraction\n\n\n\n\n\n\n\nsource: MIT Course, http://introtodeeplearning.com/, L3\n\n\n\n\n\n\n\nA 4x4: 16 distinct weights filter is applied to define the state of the neuron in the next layer.\nSame filter applied to 4x4 patches in input\nShift by 2 pixels for next patch.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#example-x-or-x",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#example-x-or-x",
    "title": "Introduction to Deep Learning",
    "section": "Example: “X or X”?",
    "text": "Example: “X or X”?\n\nsource: MIT Course, http://introtodeeplearning.com/, L3\nImages are represented by matrices of pixels, so\nLiterally speaking these images are different.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#what-are-the-features-of-x",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#what-are-the-features-of-x",
    "title": "Introduction to Deep Learning",
    "section": "What are the features of X",
    "text": "What are the features of X\n\n\nLook for a set of features that:\n\ncharacterize the images, and\nand are the same in both cases.\n\n\n\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#filters-can-detect-x-features",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#filters-can-detect-x-features",
    "title": "Introduction to Deep Learning",
    "section": "Filters can detect X features",
    "text": "Filters can detect X features\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#is-a-given-patch-in-the-image",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#is-a-given-patch-in-the-image",
    "title": "Introduction to Deep Learning",
    "section": "Is a given patch in the image?",
    "text": "Is a given patch in the image?\n\nThe key question is how to pick-up the operation that can take\n\na patch and\nan image and\n\nAn decide if the patch is in the image.\nThis operation is the convolution.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#the-convolution-operation",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#the-convolution-operation",
    "title": "Introduction to Deep Learning",
    "section": "The Convolution Operation",
    "text": "The Convolution Operation\n\nsource: MIT Course, http://introtodeeplearning.com/, L3\n\nConvolution matches the patch and the image by elementwise multiplication, followed by a sum.\nGiven the filters used (+1/-1) if there is absolute coincidence, as in the example, all multiplications will yield 1, and the sum will be 9.\nTwo completely different patches would add -9.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#the-convolution-operation-1",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#the-convolution-operation-1",
    "title": "Introduction to Deep Learning",
    "section": "The Convolution Operation",
    "text": "The Convolution Operation\n\nSuppose we want to compute the convolution of a 5x5 image and a 3x3 filter.\n\n\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#the-convolution-operation-2",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#the-convolution-operation-2",
    "title": "Introduction to Deep Learning",
    "section": "The Convolution Operation",
    "text": "The Convolution Operation\n\n\nSlide the 3x3 filter over the input image,\nElementwise multiply and\nAdd the outputs\n\n\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#the-convolution-operation-3",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#the-convolution-operation-3",
    "title": "Introduction to Deep Learning",
    "section": "The Convolution Operation",
    "text": "The Convolution Operation",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#a-filter-for-each-pattern",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#a-filter-for-each-pattern",
    "title": "Introduction to Deep Learning",
    "section": "A filter for each pattern?",
    "text": "A filter for each pattern?\n\n\nBy applying different filters, i.e. changing the weights,\nWe can achieve completely different results",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#can-filters-be-learned",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#can-filters-be-learned",
    "title": "Introduction to Deep Learning",
    "section": "Can filters be learned?",
    "text": "Can filters be learned?\n\nDifferent filters can be used to extract different characteristics from the image.\n\nBuilding filters by trial-and-error can be slow.\n\nIf a NN can learn these filters from the data, then\n\nThey can be used to classify new images.\n\nThis is what Convolutional Neural Networks is about.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#cnns-overview",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#cnns-overview",
    "title": "Introduction to Deep Learning",
    "section": "CNNs: Overview",
    "text": "CNNs: Overview\n\n\nConvolutional Neural Networks are a type of DNN that implement the ideas previously introduced\n\nUses convolutions to learn spatial features.\nIntended to identify increasingly complex traits by concatenating multiple convolutional layers.\nConvolutional layers combined with dense layers that perform “classification” as usual from the output of convolutional layers.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#another-cnn-outline",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#another-cnn-outline",
    "title": "Introduction to Deep Learning",
    "section": "Another CNN outline",
    "text": "Another CNN outline\n\nSource: Generative Deep Learning. David Foster (Fig. 2.13)",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#convolutional-layers",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#convolutional-layers",
    "title": "Introduction to Deep Learning",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\nThe layers implementing convolutions.\nBesides this, they proceed as usual for hidden layer\n\nWeighted linear combination of (convoluted) values \\[\nz^{(l)}_{p,j}=\\sum_{i=1}^4\\sum_{j=1}^4 w_{ij} x_{i+p,j+q}+b\n\\]\nFollowed by non-linear transformation (ReLU) \\[\na^{(l)}_{p,j} =max(0, z^(l)_{p,j}).\n\\]",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#filtering-increases-dimension",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#filtering-increases-dimension",
    "title": "Introduction to Deep Learning",
    "section": "Filtering increases dimension",
    "text": "Filtering increases dimension\n\nMultiple filters can be applied on the same image.\nAdding filters increases the dimensionality of the filtered output\n\nThink of the output as a volume.\n\n\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#pooling-decreases-dimension",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#pooling-decreases-dimension",
    "title": "Introduction to Deep Learning",
    "section": "Pooling decreases dimension",
    "text": "Pooling decreases dimension\n\nPooling downsamples feature maps to reduce the spatial dimensions of the feature maps while retaining the essential information.\n\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#pooling",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#pooling",
    "title": "Introduction to Deep Learning",
    "section": "Pooling",
    "text": "Pooling\nKey objectives of pooling in CNNs:\n\nDimensionality Reduction:\nTranslation Invariance:\nRobustness to Variations:\nExtraction of Salient Features:\nSpatial Hierarchy:",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#common-types-of-pooling",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#common-types-of-pooling",
    "title": "Introduction to Deep Learning",
    "section": "Common types of pooling",
    "text": "Common types of pooling\n\nMax pooling\n\nselects the maximum value within each pooling region,\n\nAverage pooling\n\ncalculates the average value.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#summary-cnns-for-classification",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#summary-cnns-for-classification",
    "title": "Introduction to Deep Learning",
    "section": "Summary: CNNs for classification",
    "text": "Summary: CNNs for classification\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#summary-cnns-for-classification-1",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#summary-cnns-for-classification-1",
    "title": "Introduction to Deep Learning",
    "section": "Summary: CNNs for classification",
    "text": "Summary: CNNs for classification\n\nsource: MIT Course, http://introtodeeplearning.com/, L3",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#tensors",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#tensors",
    "title": "Introduction to Deep Learning",
    "section": "Tensors",
    "text": "Tensors\n\nDeep learning is filled with the word “tensor”,\nWhat are Tensors any way?\n\nR users: familiar with vectors (1-d arrays) and matrices (2-d arrays).\nTensors extend this concept to higher dimensions.\nCan be seen as multi-dimensional arrays that generalize matrices.\n\nSee the Wikipedia for a nice article on tensors",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#why-tensors",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#why-tensors",
    "title": "Introduction to Deep Learning",
    "section": "Why tensors?",
    "text": "Why tensors?\n\nWorking with tensors has many benefits:\n\nGeneralization: Tensors generalize vectors and matrices to an arbitary number of dimensions,\nFlexibility: can hold a wide range of data types.\nSpeed: Use of tensors facilitates fast, parallel processing computations.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#one-and-two-dimensional-tensors",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#one-and-two-dimensional-tensors",
    "title": "Introduction to Deep Learning",
    "section": "One and two dimensional tensors",
    "text": "One and two dimensional tensors\n\n\nVectors:rank-1 tensors.\n\n\n\n\n\n\n\n\n\n\n\nMatrices: rank-2 tensors.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#rank-three-tensors",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#rank-three-tensors",
    "title": "Introduction to Deep Learning",
    "section": "Rank three tensors",
    "text": "Rank three tensors\n\n\n\nArrays in layers.\nTypic use: Sequence data\n\ntime series, text\ndim = (observations, seq steps, features)\n\n\nExamples\n\n\n250 days of high, low, and current stock price for 390 minutes of trading in a day;\n\ndim = c(250, 390, 3)\n\n1M tweets that can be 140 characters long and include 128 unique characters;\n\ndim = c(1M, 140, 128)",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#rank-four-tensors",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#rank-four-tensors",
    "title": "Introduction to Deep Learning",
    "section": "Rank four tensors",
    "text": "Rank four tensors\n\n\n\nLayers of groups of arrays\nTypic use: Image data\n\nRGB channels\ndim = (observations, height, width, color_depth)\nMNIST data could be seen as a 4D tensor where color_depth = 1",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#rank-five-tensors",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#rank-five-tensors",
    "title": "Introduction to Deep Learning",
    "section": "Rank five tensors",
    "text": "Rank five tensors\n\n\n\nTypic use: Video data\n\nsamples: 4 (each video is 1 minute long)\nframes: 240 (4 frames/second)\nwidth: 256 (pixels)\nheight: 144 (pixels)\nchannels: 3 (red, green, blue)\n\nTensor shape (4, 240, 256, 144, 3)",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#one-can-always-reshape",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#one-can-always-reshape",
    "title": "Introduction to Deep Learning",
    "section": "One can always reshape",
    "text": "One can always reshape\n\nEach DNN model has a given architecture which usually requires 2D/3D tensors.\nIf data is not in the expected form it can be reshaped.\n\n\nSee Deep learning with R for more.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#which-programs-for-dl",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#which-programs-for-dl",
    "title": "Introduction to Deep Learning",
    "section": "Which programs for DL?",
    "text": "Which programs for DL?\n\nTensorFlow\nPyTorch\nKeras\nMXNet\nCaffe\nTheano\nMicrosoft Cognitive Toolkit (CNTK)",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#tensorflow",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#tensorflow",
    "title": "Introduction to Deep Learning",
    "section": "TensorFlow",
    "text": "TensorFlow\n\n\n\nOpen-source DL framework developed by Google.\nFlexible model development\nOptimized for distributed computing and performance\nHigh level APIS like Keras, in Python and R.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#pytorch",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#pytorch",
    "title": "Introduction to Deep Learning",
    "section": "PyTorch",
    "text": "PyTorch\n\n\n\nOpen-source DL framework developed by facebook.\nUser-friendly interface and dynamic computational graph features with intuitive debugging and experimentation.\nHighly integrated with Numpy",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#keras",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#keras",
    "title": "Introduction to Deep Learning",
    "section": "Keras",
    "text": "Keras\n\n\n\nHigh-level neural networks API running on top of TensorFlow, CNTK, or Theano,\nEmphasizes simplicity and ease of use.\nAVailable in Python and R",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#the-keras-pipeline",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#the-keras-pipeline",
    "title": "Introduction to Deep Learning",
    "section": "The Keras pipeline",
    "text": "The Keras pipeline\n\nTraining and using a model in keras is intended to be done through the usual steps of a ML worfflow",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#a-keras-cheat-sheet",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#a-keras-cheat-sheet",
    "title": "Introduction to Deep Learning",
    "section": "A keras cheat sheet",
    "text": "A keras cheat sheet\n\nAvailable at rstudio github repo",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#the-mnist-dataset",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#the-mnist-dataset",
    "title": "Introduction to Deep Learning",
    "section": "The MNIST dataset",
    "text": "The MNIST dataset\n\nA popular dataset or handwritten numbers.\n\n\nlibrary(keras)\nmnist &lt;- dataset_mnist()\n\n\nMade of features (images) and target values (labels)\nDivided into a training and test set.\n\n\nx_train &lt;- mnist$train$x; y_train &lt;- mnist$train$y\nx_test &lt;- mnist$test$x; y_test &lt;- mnist$test$y\n\n\n(mnistDims &lt;- dim(x_train))\nimg_rows &lt;- mnistDims[2];  img_cols &lt;- mnistDims[3]",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#data-pre-processing-1-reshaping",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#data-pre-processing-1-reshaping",
    "title": "Introduction to Deep Learning",
    "section": "Data pre-processing (1): Reshaping",
    "text": "Data pre-processing (1): Reshaping\n\nThese images are not in the the requires shape, as the number of channels is missing.\nThis can be corrected using the array_reshape() function.\n\n\nx_train &lt;- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))\nx_test &lt;- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1)) \n\ninput_shape &lt;- c(img_rows, img_cols, 1)\n\ndim(x_train)",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#data-pre-processing-2-other-transforms",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#data-pre-processing-2-other-transforms",
    "title": "Introduction to Deep Learning",
    "section": "Data pre-processing (2): Other transforms",
    "text": "Data pre-processing (2): Other transforms\n\nData is first normalized (to values in [0,1])\n\n\nx_train &lt;- x_train / 255\nx_test &lt;- x_test / 255\n\n\nLabels are one-hot-encoded using the to_categorical() function.\n\n\nnum_classes = 10\ny_train &lt;- to_categorical(y_train, num_classes)\ny_test &lt;- to_categorical(y_test, num_classes)",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#modeling-1-definition",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#modeling-1-definition",
    "title": "Introduction to Deep Learning",
    "section": "Modeling (1): Definition",
    "text": "Modeling (1): Definition\n\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_conv_2d(filters = 16,\n                kernel_size = c(3,3),\n                activation = 'relu',\n                input_shape = input_shape) %&gt;% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;% \n  layer_dropout(rate = 0.25) %&gt;% \n  layer_flatten() %&gt;% \n  layer_dense(units = 10,\n              activation = 'relu') %&gt;% \n  layer_dropout(rate = 0.5) %&gt;% \n  layer_dense(units = num_classes,\n              activation = 'softmax')",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#modeling-1-model-summary",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#modeling-1-model-summary",
    "title": "Introduction to Deep Learning",
    "section": "Modeling (1): Model Summary",
    "text": "Modeling (1): Model Summary\n\nmodel %&gt;% summary()",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#modeling-2-compilation",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#modeling-2-compilation",
    "title": "Introduction to Deep Learning",
    "section": "Modeling (2): Compilation",
    "text": "Modeling (2): Compilation\n\nCategorical cross-entropy as loss function.\nAdadelta optimizes the gradient descent.\nAccuracy serves as metric.\n\n\nmodel %&gt;% compile(\n  loss = loss_categorical_crossentropy,\n  optimizer = optimizer_adadelta(),\n  metrics = c('accuracy')\n)",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#model-training",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#model-training",
    "title": "Introduction to Deep Learning",
    "section": "Model training",
    "text": "Model training\n\nA mini-batch1 size of 128 should allow the tensors to fit into the memory of most “normal” machines.\nThe model will run over 12 epochs,\nWith a validation split set at 0.2\n\n\nbatch_size &lt;- 128\nepochs &lt;- 12\n\nmodel %&gt;% fit(\n  x_train, y_train,\n  batch_size = batch_size,\n  epochs = epochs,\n  validation_split = 0.2\n)\n\n\nA batch is a collection of training examples processed together,\nA minibatch is a smaller subset of a batch used for memory efficiency\nAn epoch is a complete pass of the entire training dataset during model training.",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#model-evaluation",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#model-evaluation",
    "title": "Introduction to Deep Learning",
    "section": "Model evaluation",
    "text": "Model evaluation\n\nUse test data to evaluate the model.\n\n\nmodel %&gt;% evaluate(x_test, y_test)\npredictions &lt;- model %&gt;% predict(x_test) # Not shown",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#resources-1",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#resources-1",
    "title": "Introduction to Deep Learning",
    "section": "Resources (1)",
    "text": "Resources (1)\nCourses\n\nAn introduction to Deep Learning. Alex Amini. MIT\nCoursera: Deep Learning Specialization. Andrew Ng\n\nBooks\n\nNeural networks and Deep Learning\nDeep learning with R, 2nd edition. F. Chollet",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "C3.2-Introduction_to_Deep_Learning-Slides.html#resources-2",
    "href": "C3.2-Introduction_to_Deep_Learning-Slides.html#resources-2",
    "title": "Introduction to Deep Learning",
    "section": "Resources (2)",
    "text": "Resources (2)\nWorkshops\n\nDeep learning with R Summer course\nDeep learning with keras and Tensorflow in R (Rstudio conf. 2020)\n\nDocuments\n\nIntroduction to Convolutional Neural Networks (CNN)\nConvolutional Neural Networks in R",
    "crumbs": [
      "Home",
      "Artificial Neural Networks",
      "Deep Learning"
    ]
  },
  {
    "objectID": "labs/Decision_Trees-Lab_0.html",
    "href": "labs/Decision_Trees-Lab_0.html",
    "title": "The Pima Indians dataset",
    "section": "",
    "text": "The Pima Indian Diabetes data set (PimaIndiansDiabetes2) is available in the mlbench package.\nThe data contains 768 individuals (female) and 9 clinical variables for predicting the probability of individuals in being diabete-positive or negative:\n\npregnant: number of times pregnant\nglucose: plasma glucose concentration\npressure: diastolic blood pressure (mm Hg)\ntriceps: triceps skin fold thickness (mm)\ninsulin: 2-Hour serum insulin (mu U/ml)\nmass: body mass index (weight in kg/(height in m)^2)\npedigree: diabetes pedigree function\nage: age (years)\ndiabetes: class variable\n\nA typical classification/prediction problem is to build a model that can distinguish and predict diabetes using some or all the variables in the dataset.\nWe start by getting the data and loading it\n\nimport pandas as pd\nfrom sklearn import datasets\n\n# Cargar el dataset de Pima Indians Diabetes desde seaborn\nurl = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n\n# Cargar el dataset\ncolumn_names = [\"pregnancies\", \"glucose\", \"blood_pressure\", \"skin_thickness\", \n                \"insulin\", \"bmi\", \"diabetes_pedigree\", \"age\", \"outcome\"]\n\ndf = pd.read_csv(url, names=column_names)"
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html",
    "title": "Lab 2: Ensembles of Trees",
    "section": "",
    "text": "Code\n# Helper packages\nlibrary(dplyr)       # for data wrangling\nlibrary(ggplot2)     # for awesome plotting\nlibrary(modeldata)  # for parallel backend to foreach\nlibrary(foreach)     # for parallel processing with for loops\n\n# Modeling packages\n#library(caret)       # for general model fitting\nlibrary(rpart)       # for fitting decision trees\nlibrary(ipred)       # for fitting bagged decision trees",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Random Forest Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#the-dataset",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#the-dataset",
    "title": "Lab 2: Ensembles of Trees",
    "section": "The dataset",
    "text": "The dataset\nPackge AmesHousing contains the data jointly with some instructions to create the required dataset.\nWe will use, however data from the modeldata package where some preprocessing of the data has already been performed (see: https://www.tmwr.org/ames)\n\n\nCode\ndata(ames, package = \"modeldata\")\ndim(ames)\n\n\n[1] 2930   74",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Random Forest Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#exploratory-data-analysis-an-preprocessing",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#exploratory-data-analysis-an-preprocessing",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Exploratory Data Analysis an preprocessing",
    "text": "Exploratory Data Analysis an preprocessing\nThe dataset has 74 variables, and has already been prepared by the package modeldatamaintainers.\nIn any case it is always recommended to do some Exploratory Analysis.\n\n\nCode\nlibrary(skimr)\nskim(ames)\n\n\n\nData summary\n\n\nName\names\n\n\nNumber of rows\n2930\n\n\nNumber of columns\n74\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n40\n\n\nnumeric\n34\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nMS_SubClass\n0\n1\nFALSE\n16\nOne: 1079, Two: 575, One: 287, One: 192\n\n\nMS_Zoning\n0\n1\nFALSE\n7\nRes: 2273, Res: 462, Flo: 139, Res: 27\n\n\nStreet\n0\n1\nFALSE\n2\nPav: 2918, Grv: 12\n\n\nAlley\n0\n1\nFALSE\n3\nNo_: 2732, Gra: 120, Pav: 78\n\n\nLot_Shape\n0\n1\nFALSE\n4\nReg: 1859, Sli: 979, Mod: 76, Irr: 16\n\n\nLand_Contour\n0\n1\nFALSE\n4\nLvl: 2633, HLS: 120, Bnk: 117, Low: 60\n\n\nUtilities\n0\n1\nFALSE\n3\nAll: 2927, NoS: 2, NoS: 1\n\n\nLot_Config\n0\n1\nFALSE\n5\nIns: 2140, Cor: 511, Cul: 180, FR2: 85\n\n\nLand_Slope\n0\n1\nFALSE\n3\nGtl: 2789, Mod: 125, Sev: 16\n\n\nNeighborhood\n0\n1\nFALSE\n28\nNor: 443, Col: 267, Old: 239, Edw: 194\n\n\nCondition_1\n0\n1\nFALSE\n9\nNor: 2522, Fee: 164, Art: 92, RRA: 50\n\n\nCondition_2\n0\n1\nFALSE\n8\nNor: 2900, Fee: 13, Art: 5, Pos: 4\n\n\nBldg_Type\n0\n1\nFALSE\n5\nOne: 2425, Twn: 233, Dup: 109, Twn: 101\n\n\nHouse_Style\n0\n1\nFALSE\n8\nOne: 1481, Two: 873, One: 314, SLv: 128\n\n\nOverall_Cond\n0\n1\nFALSE\n9\nAve: 1654, Abo: 533, Goo: 390, Ver: 144\n\n\nRoof_Style\n0\n1\nFALSE\n6\nGab: 2321, Hip: 551, Gam: 22, Fla: 20\n\n\nRoof_Matl\n0\n1\nFALSE\n8\nCom: 2887, Tar: 23, WdS: 9, WdS: 7\n\n\nExterior_1st\n0\n1\nFALSE\n16\nVin: 1026, Met: 450, HdB: 442, Wd : 420\n\n\nExterior_2nd\n0\n1\nFALSE\n17\nVin: 1015, Met: 447, HdB: 406, Wd : 397\n\n\nMas_Vnr_Type\n0\n1\nFALSE\n5\nNon: 1775, Brk: 880, Sto: 249, Brk: 25\n\n\nExter_Cond\n0\n1\nFALSE\n5\nTyp: 2549, Goo: 299, Fai: 67, Exc: 12\n\n\nFoundation\n0\n1\nFALSE\n6\nPCo: 1310, CBl: 1244, Brk: 311, Sla: 49\n\n\nBsmt_Cond\n0\n1\nFALSE\n6\nTyp: 2616, Goo: 122, Fai: 104, No_: 80\n\n\nBsmt_Exposure\n0\n1\nFALSE\n5\nNo: 1906, Av: 418, Gd: 284, Mn: 239\n\n\nBsmtFin_Type_1\n0\n1\nFALSE\n7\nGLQ: 859, Unf: 851, ALQ: 429, Rec: 288\n\n\nBsmtFin_Type_2\n0\n1\nFALSE\n7\nUnf: 2499, Rec: 106, LwQ: 89, No_: 81\n\n\nHeating\n0\n1\nFALSE\n6\nGas: 2885, Gas: 27, Gra: 9, Wal: 6\n\n\nHeating_QC\n0\n1\nFALSE\n5\nExc: 1495, Typ: 864, Goo: 476, Fai: 92\n\n\nCentral_Air\n0\n1\nFALSE\n2\nY: 2734, N: 196\n\n\nElectrical\n0\n1\nFALSE\n6\nSBr: 2682, Fus: 188, Fus: 50, Fus: 8\n\n\nFunctional\n0\n1\nFALSE\n8\nTyp: 2728, Min: 70, Min: 65, Mod: 35\n\n\nGarage_Type\n0\n1\nFALSE\n7\nAtt: 1731, Det: 782, Bui: 186, No_: 157\n\n\nGarage_Finish\n0\n1\nFALSE\n4\nUnf: 1231, RFn: 812, Fin: 728, No_: 159\n\n\nGarage_Cond\n0\n1\nFALSE\n6\nTyp: 2665, No_: 159, Fai: 74, Goo: 15\n\n\nPaved_Drive\n0\n1\nFALSE\n3\nPav: 2652, Dir: 216, Par: 62\n\n\nPool_QC\n0\n1\nFALSE\n5\nNo_: 2917, Exc: 4, Goo: 4, Typ: 3\n\n\nFence\n0\n1\nFALSE\n5\nNo_: 2358, Min: 330, Goo: 118, Goo: 112\n\n\nMisc_Feature\n0\n1\nFALSE\n6\nNon: 2824, She: 95, Gar: 5, Oth: 4\n\n\nSale_Type\n0\n1\nFALSE\n10\nWD : 2536, New: 239, COD: 87, Con: 26\n\n\nSale_Condition\n0\n1\nFALSE\n6\nNor: 2413, Par: 245, Abn: 190, Fam: 46\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nLot_Frontage\n0\n1\n57.65\n33.50\n0.00\n43.00\n63.00\n78.00\n313.00\n▇▇▁▁▁\n\n\nLot_Area\n0\n1\n10147.92\n7880.02\n1300.00\n7440.25\n9436.50\n11555.25\n215245.00\n▇▁▁▁▁\n\n\nYear_Built\n0\n1\n1971.36\n30.25\n1872.00\n1954.00\n1973.00\n2001.00\n2010.00\n▁▂▃▆▇\n\n\nYear_Remod_Add\n0\n1\n1984.27\n20.86\n1950.00\n1965.00\n1993.00\n2004.00\n2010.00\n▅▂▂▃▇\n\n\nMas_Vnr_Area\n0\n1\n101.10\n178.63\n0.00\n0.00\n0.00\n162.75\n1600.00\n▇▁▁▁▁\n\n\nBsmtFin_SF_1\n0\n1\n4.18\n2.23\n0.00\n3.00\n3.00\n7.00\n7.00\n▃▂▇▁▇\n\n\nBsmtFin_SF_2\n0\n1\n49.71\n169.14\n0.00\n0.00\n0.00\n0.00\n1526.00\n▇▁▁▁▁\n\n\nBsmt_Unf_SF\n0\n1\n559.07\n439.54\n0.00\n219.00\n465.50\n801.75\n2336.00\n▇▅▂▁▁\n\n\nTotal_Bsmt_SF\n0\n1\n1051.26\n440.97\n0.00\n793.00\n990.00\n1301.50\n6110.00\n▇▃▁▁▁\n\n\nFirst_Flr_SF\n0\n1\n1159.56\n391.89\n334.00\n876.25\n1084.00\n1384.00\n5095.00\n▇▃▁▁▁\n\n\nSecond_Flr_SF\n0\n1\n335.46\n428.40\n0.00\n0.00\n0.00\n703.75\n2065.00\n▇▃▂▁▁\n\n\nGr_Liv_Area\n0\n1\n1499.69\n505.51\n334.00\n1126.00\n1442.00\n1742.75\n5642.00\n▇▇▁▁▁\n\n\nBsmt_Full_Bath\n0\n1\n0.43\n0.52\n0.00\n0.00\n0.00\n1.00\n3.00\n▇▆▁▁▁\n\n\nBsmt_Half_Bath\n0\n1\n0.06\n0.25\n0.00\n0.00\n0.00\n0.00\n2.00\n▇▁▁▁▁\n\n\nFull_Bath\n0\n1\n1.57\n0.55\n0.00\n1.00\n2.00\n2.00\n4.00\n▁▇▇▁▁\n\n\nHalf_Bath\n0\n1\n0.38\n0.50\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▁\n\n\nBedroom_AbvGr\n0\n1\n2.85\n0.83\n0.00\n2.00\n3.00\n3.00\n8.00\n▁▇▂▁▁\n\n\nKitchen_AbvGr\n0\n1\n1.04\n0.21\n0.00\n1.00\n1.00\n1.00\n3.00\n▁▇▁▁▁\n\n\nTotRms_AbvGrd\n0\n1\n6.44\n1.57\n2.00\n5.00\n6.00\n7.00\n15.00\n▁▇▂▁▁\n\n\nFireplaces\n0\n1\n0.60\n0.65\n0.00\n0.00\n1.00\n1.00\n4.00\n▇▇▁▁▁\n\n\nGarage_Cars\n0\n1\n1.77\n0.76\n0.00\n1.00\n2.00\n2.00\n5.00\n▅▇▂▁▁\n\n\nGarage_Area\n0\n1\n472.66\n215.19\n0.00\n320.00\n480.00\n576.00\n1488.00\n▃▇▃▁▁\n\n\nWood_Deck_SF\n0\n1\n93.75\n126.36\n0.00\n0.00\n0.00\n168.00\n1424.00\n▇▁▁▁▁\n\n\nOpen_Porch_SF\n0\n1\n47.53\n67.48\n0.00\n0.00\n27.00\n70.00\n742.00\n▇▁▁▁▁\n\n\nEnclosed_Porch\n0\n1\n23.01\n64.14\n0.00\n0.00\n0.00\n0.00\n1012.00\n▇▁▁▁▁\n\n\nThree_season_porch\n0\n1\n2.59\n25.14\n0.00\n0.00\n0.00\n0.00\n508.00\n▇▁▁▁▁\n\n\nScreen_Porch\n0\n1\n16.00\n56.09\n0.00\n0.00\n0.00\n0.00\n576.00\n▇▁▁▁▁\n\n\nPool_Area\n0\n1\n2.24\n35.60\n0.00\n0.00\n0.00\n0.00\n800.00\n▇▁▁▁▁\n\n\nMisc_Val\n0\n1\n50.64\n566.34\n0.00\n0.00\n0.00\n0.00\n17000.00\n▇▁▁▁▁\n\n\nMo_Sold\n0\n1\n6.22\n2.71\n1.00\n4.00\n6.00\n8.00\n12.00\n▅▆▇▃▃\n\n\nYear_Sold\n0\n1\n2007.79\n1.32\n2006.00\n2007.00\n2008.00\n2009.00\n2010.00\n▇▇▇▇▃\n\n\nSale_Price\n0\n1\n180796.06\n79886.69\n12789.00\n129500.00\n160000.00\n213500.00\n755000.00\n▇▇▁▁▁\n\n\nLongitude\n0\n1\n-93.64\n0.03\n-93.69\n-93.66\n-93.64\n-93.62\n-93.58\n▅▅▇▆▁\n\n\nLatitude\n0\n1\n42.03\n0.02\n41.99\n42.02\n42.03\n42.05\n42.06\n▂▂▇▇▇\n\n\n\n\n\nThe exploration shows that the data set is well formed with factor data types for categorical variables and no missings.\nIt can also be seen that tha response variabl, Sales_Price varies on a high range, as confirmed below.\n\n\nCode\nsummary(ames$Sale_Price)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12789  129500  160000  180796  213500  755000 \n\n\n\n\nCode\nboxplot(ames[,sapply(ames, is.numeric)], las=2, cex.axis=0.5)\n\n\n\n\n\n\n\n\n\nAlthough not strictly necessary, given that the variable that has the widest range of variation is the variable to predict we can consider transforming it. In this case the simplest transform seems to express the price in thousands instead of dollars. The distribution of the variable is asymetrical so we may also consider taking logarithm, but given that it would complicate the interpretation of the results, and that something like normality is not required by the methods we use, only division by 1000 is performed.\n\n\nCode\nrequire(dplyr)\names &lt;- ames %&gt;% mutate(Sale_Price = Sale_Price/1000)\nboxplot(ames)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Random Forest Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#spliting-the-data-into-testtrain",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#spliting-the-data-into-testtrain",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Spliting the data into test/train",
    "text": "Spliting the data into test/train\nWe split the data in separate test / training sets and do it in such a way that samplig is balanced for the response variable, Sale_Price.\n\n\nCode\nif(!require(rsample))\n  install.packages(\"rsample\", dep=TRUE)\n# Stratified sampling with the rsample package\nset.seed(123)\nsplit &lt;- rsample::initial_split(ames, prop = 0.7, \n                       strata = \"Sale_Price\")\names_train  &lt;- training(split)\names_test   &lt;- testing(split)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Random Forest Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#optimizing-the-tree",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#optimizing-the-tree",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Optimizing the tree",
    "text": "Optimizing the tree\nIn order to optimize the tree we ccan ompute the best cost complexity value\n\n\nCode\nset.seed(123)\ncv_ames_rt1 &lt;- tree::cv.tree(ames_rt1, K = 5)\n\noptSize &lt;- rev(cv_ames_rt1$size)[which.min(rev(cv_ames_rt1$dev))]\npaste(\"Optimal size obtained is:\", optSize)\n\n\n[1] \"Optimal size obtained is: 12\"\n\n\nThe best value of alpha is obtained with the same tree, which suggests that there will be no advantage in pruning.\nThis is confirmed by plotting tree size vs deviance which shows that the tree with the samllest error is the biggest one that can be obtained.\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggpubr)\n\nresultados_cv &lt;- data.frame(\n                   n_nodes  = cv_ames_rt1$size,\n                   deviance = cv_ames_rt1$dev,\n                   alpha    = cv_ames_rt1$k\n                 )\n\np1 &lt;- ggplot(data = resultados_cv, aes(x = n_nodes, y = deviance)) +\n      geom_line() + \n      geom_point() +\n      geom_vline(xintercept = optSize, color = \"red\") +\n      labs(title = \"Error vs tree size\") +\n      theme_bw() \n  \np2 &lt;- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +\n      geom_line() + \n      geom_point() +\n      labs(title = \"Error vs penalization (alpha)\") +\n      theme_bw() \n\nggarrange(p1, p2)\n\n\n\n\n\n\n\n\n\nWe could have tried to obtain a bigger tree with the hope that pruning might find a better tree. This can be done setting the tree parameters to minimal values.\n\n\nCode\nctlPars2 &lt;-tree.control(nobs=nrow(ames_train), mincut = 1, minsize = 2, mindev = 0)\names_rt2 &lt;-  tree::tree(\n                    formula = Sale_Price ~ .,\n                    data    = ames_train,\n                    split   = \"deviance\",\n                    control = ctlPars2)\nsummary(ames_rt2)\n\n\n\nRegression tree:\ntree::tree(formula = Sale_Price ~ ., data = ames_train, control = ctlPars2, \n    split = \"deviance\")\nVariables actually used in tree construction:\n [1] \"Neighborhood\"   \"First_Flr_SF\"   \"Garage_Cond\"    \"Gr_Liv_Area\"   \n [5] \"Central_Air\"    \"Garage_Area\"    \"Enclosed_Porch\" \"Lot_Area\"      \n [9] \"Longitude\"      \"Latitude\"       \"MS_SubClass\"    \"Lot_Frontage\"  \n[13] \"Overall_Cond\"   \"Year_Built\"     \"MS_Zoning\"      \"Alley\"         \n[17] \"Fireplaces\"     \"House_Style\"    \"BsmtFin_Type_1\" \"Bsmt_Exposure\" \n[21] \"Bsmt_Unf_SF\"    \"Electrical\"     \"Year_Remod_Add\" \"Exterior_1st\"  \n[25] \"Open_Porch_SF\"  \"Exterior_2nd\"   \"Functional\"     \"Mo_Sold\"       \n[29] \"Total_Bsmt_SF\"  \"Bsmt_Full_Bath\" \"Sale_Condition\" \"Heating_QC\"    \n[33] \"Mas_Vnr_Area\"   \"Garage_Cars\"    \"Land_Contour\"   \"Condition_1\"   \n[37] \"Mas_Vnr_Type\"   \"Second_Flr_SF\"  \"Lot_Config\"     \"Exter_Cond\"    \n[41] \"Fence\"          \"Lot_Shape\"      \"Bsmt_Cond\"      \"Bedroom_AbvGr\" \n[45] \"Wood_Deck_SF\"   \"Roof_Style\"     \"Sale_Type\"      \"BsmtFin_Type_2\"\n[49] \"Bldg_Type\"      \"Garage_Type\"    \"Year_Sold\"      \"Garage_Finish\" \n[53] \"Screen_Porch\"   \"Half_Bath\"      \"Foundation\"     \"BsmtFin_SF_1\"  \n[57] \"Full_Bath\"      \"Land_Slope\"     \"TotRms_AbvGrd\" \nNumber of terminal nodes:  1222 \nResidual mean deviance:  2.579 = 2133 / 827 \nDistribution of residuals:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -2.750  -0.500   0.000   0.000   0.500   2.943 \n\n\nThis bigger tree has indeed a smaller deviance but pruning provides no benefit:\n\n\nCode\nset.seed(123)\ncv_ames_rt2 &lt;- tree::cv.tree(ames_rt2, K = 5)\n\noptSize2 &lt;- rev(cv_ames_rt2$size)[which.min(rev(cv_ames_rt2$dev))]\npaste(\"Optimal size obtained is:\", optSize2)\n\n\n[1] \"Optimal size obtained is: 12\"\n\n\n\n\nCode\nprunedTree2 &lt;- tree::prune.tree(\n                  tree = ames_rt2,\n                  best = optSize2\n               )\nsummary(prunedTree2)\n\n\n\nRegression tree:\nsnip.tree(tree = ames_rt2, nodes = c(61L, 31L, 12L, 13L, 60L, \n18L, 19L, 14L, 23L, 10L, 22L, 8L))\nVariables actually used in tree construction:\n[1] \"Neighborhood\"  \"First_Flr_SF\"  \"Gr_Liv_Area\"   \"Total_Bsmt_SF\"\n[5] \"Second_Flr_SF\"\nNumber of terminal nodes:  12 \nResidual mean deviance:  1428 = 2909000 / 2037 \nDistribution of residuals:\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-227.1000  -21.2400   -0.2372    0.0000   19.0800  212.0000 \n\n\n\n\nCode\nres_cv2 &lt;- data.frame(\n                   n_nodes  = cv_ames_rt2$size,\n                   deviance = cv_ames_rt2$dev,\n                   alpha    = cv_ames_rt2$k\n                 )\n\np1 &lt;- ggplot(data = res_cv2, aes(x = n_nodes, y = deviance)) +\n      geom_line() + \n      geom_point() +\n      geom_vline(xintercept = optSize2, color = \"red\") +\n      labs(title = \"Error vs tree size\") +\n      theme_bw() \n  \np2 &lt;- ggplot(data = res_cv2, aes(x = alpha, y = deviance)) +\n      geom_line() + \n      geom_point() +\n      labs(title = \"Error vs penalization (alpha)\") +\n      theme_bw() \n\nggarrange(p1, p2)\n\n\n\n\n\n\n\n\n\nThe performance of the trees is hardly different between small or big tree in pruned or non-pruned version.\n\n\nCode\names_rt_pred1 &lt;- predict(ames_rt1, newdata = ames_test)\ntest_rmse1    &lt;- sqrt(mean((ames_rt_pred1 - ames_test$Sale_Price)^2))\npaste(\"Error test (rmse) for initial tree:\", round(test_rmse1,2))\n\n\n[1] \"Error test (rmse) for initial tree: 39.69\"\n\n\n\n\nCode\names_rt_pred2 &lt;- predict(ames_rt2, newdata = ames_test)\ntest_rmse2    &lt;- sqrt(mean((ames_rt_pred2 - ames_test$Sale_Price)^2))\npaste(\"Error test (rmse) for big tree:\", round(test_rmse2,2))\n\n\n[1] \"Error test (rmse) for big tree: 37.59\"\n\n\n\n\nCode\names_pruned_pred &lt;- predict(prunedTree2, newdata = ames_test)\ntest_rmse3    &lt;- sqrt(mean((ames_pruned_pred - ames_test$Sale_Price)^2))\npaste(\"Error test (rmse) for pruned tree:\", round(test_rmse3,2))\n\n\n[1] \"Error test (rmse) for pruned tree: 39.69\"\n\n\nCode\nimprovement &lt;- (test_rmse3-test_rmse2)/test_rmse2*100\n\n\nThe MSE for each model will be saved to facilitate comparison with other models\n\n\nCode\nerrTable &lt;- data.frame(Model=character(),  RMSE=double())\nerrTable[1, ] &lt;-  c(\"Default Regression Tree\", round(test_rmse1,2))\nerrTable[2, ] &lt;-  c(\"Big Regression Tree\", round(test_rmse2,2))\nerrTable[3, ] &lt;-  c(\"Optimally pruned Regression Tree\", round(test_rmse3,2))\n\n\n\n\nCode\n# kableExtra::kable(errTable) %&gt;% kableExtra::kable_styling()\nknitr::kable(errTable) \n\n\n\n\n\nModel\nRMSE\n\n\n\n\nDefault Regression Tree\n39.69\n\n\nBig Regression Tree\n37.59\n\n\nOptimally pruned Regression Tree\n39.69\n\n\n\n\n\nIn summary, what is illustrated by this example is that, for some datasets, it is very hard to obtain an optimal tree because there seems to be a minimum complexity which is very hard to decrease.\nBuilding a saturated tree only provides a slight improvement of less than 5% in RMSE at the cost of having to use 5 times more variables in a tree withh more than 1000 nodes.\nThis is a good point to consider using an ensemble instead of single trees.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Random Forest Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#feature-interpretation",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#feature-interpretation",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Feature interpretation",
    "text": "Feature interpretation\nTo measure feature importance, the reduction in the loss function (e.g., SSE) attributed to each variable at each split is tabulated.\nIn some instances, a single variable could be used multiple times in a tree; consequently, the total reduction in the loss function across all splits by a variable are summed up and used as the total feature importance.\nNot all packages store the informaticon required to compute variable importance. For instance, the treepackges does not, but rpartor caret do save it.\n\n\nCode\nvip(ames_rt1bis, num_features = 40, bar = FALSE)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Random Forest Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#distinct-error-rates",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#distinct-error-rates",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Distinct error rates",
    "text": "Distinct error rates\nThe following chunks of code show the fit between data and predictions for the train set, the test set and the out-of bag samples\n\nError estimated from train samples\n\n\nCode\nyhattrain.bag &lt;- predict(bag.Ames, newdata = ames_train)\n# train_mse_bag  &lt;- sqrt(mean(yhattrain.bag - ames_train$Sale_Price)^2)\ntrain_rmse_bag &lt;- sqrt(mean((yhattrain.bag - ames_train$Sale_Price)^2))\nshowError&lt;- paste(\"Error train (rmse) for bagged tree:\", round(train_rmse_bag,6))\nplot(yhattrain.bag, ames_train$Sale_Price, main=showError)\nabline(0, 1)\n\n\n\n\n\n\n\n\n\nTHis error is much smaller even than those from trees because of overfitting\n\n\nError estimated from test samples\n\n\nCode\nyhat.bag &lt;- predict(bag.Ames, newdata = ames_test)\n# test_mse_bag  &lt;- sqrt(mean(yhat.bag - ames_test$Sale_Price)^2)\ntest_rmse_bag  &lt;- sqrt(mean((yhat.bag - ames_test$Sale_Price)^2))\nshowError&lt;- paste(\"Error test (rmse) for bagged tree:\", round(test_rmse_bag,4))\nplot(yhat.bag, ames_test$Sale_Price, main=showError)\nabline(0, 1)\n\n\n\n\n\n\n\n\n\n\n\nError estimated from out-of-bag samples\nBagging allows computing an out-of bag error estimate.\nOut of bag error rate is reported in the output and can be computed from the predicted (“the predicted values of the input data based on out-of-bag samples”)\n\n\nCode\noob_err&lt;- sqrt(mean((bag.Ames$predicted-ames_train$Sale_Price)^2))\nshowError &lt;- paste(\"Out of bag error for bagged tree:\", round(oob_err,4))\nplot(bag.Ames$predicted, ames_train$Sale_Price, main=showError)\nabline(0, 1)\n\n\n\n\n\n\n\n\n\nInterestingly this may be not only bigger than the error estimated on the train set but also bigger than the error estimated on the test set.\nWe can collect error rates and compare to each other and also to those obtained from regression trees:\n\n\nCode\nerrTable &lt;- data.frame(Model=character(),  RMSE=double())\nerrTable[1, ] &lt;-  c(\"Default Regression Tree\", round(test_rmse1,2))\nerrTable[2, ] &lt;-  c(\"Big Regression Tree\", round(test_rmse2,2))\nerrTable[3, ] &lt;-  c(\"Optimally pruned Regression Tree\", round(test_rmse3,2))\nerrTable[4, ] &lt;-  c(\"Bagged Tree with Train Data\", round(train_rmse_bag,2))\nerrTable[5, ] &lt;-  c(\"Bagged Tree with Test Data\", round(test_rmse_bag,2))\nerrTable[6, ] &lt;-  c(\"Bagged Tree with OOB error rate\", round(oob_err,2))",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Random Forest Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#bagging-parameter-tuning",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#bagging-parameter-tuning",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Bagging parameter tuning",
    "text": "Bagging parameter tuning\nBagging tends to improve quickly as the number of resampled trees increases, and then it reaches a platform.\nThe figure below has been produced iterated the computation above over nbagg values of 1–200 and applied the bagging() function.\n\n\n\n\n\nError curve for bagging 1-200 deep, unpruned decision trees. The benefit of bagging is optimized at 187 trees although the majority of error reduction occurred within the first 100 trees",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Random Forest Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#variable-importance",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#variable-importance",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Variable importance",
    "text": "Variable importance\nDue to the bagging process, models that are normally perceived as interpretable are no longer so.\nHowever, we can still make inferences about how features are influencing our model using feature importance measures based on the sum of the reduction in the loss function (e.g., SSE) attributed to each variable at each split in a given tree.\n\n\nCode\nrequire(dplyr)\nVIP &lt;- importance(bag.Ames) \nVIP &lt;- VIP[order(VIP[,1], decreasing = TRUE),]\nhead(VIP, n=30)\n\n\n                 %IncMSE IncNodePurity\nGr_Liv_Area    26.419428    1923769.45\nNeighborhood   19.795920    5243520.37\nTotal_Bsmt_SF  14.370182    1012412.95\nFirst_Flr_SF   13.467640     511525.12\nMS_SubClass    10.468429     146349.46\nBsmtFin_Type_1  9.491046      57870.21\nYear_Remod_Add  9.273779     214595.97\nGarage_Area     7.485364     334439.51\nYear_Built      7.112852     266905.91\nFireplaces      6.999228      57043.18\nGarage_Cars     6.915052    1777310.87\nBsmt_Unf_SF     6.879415      78882.32\nSecond_Flr_SF   6.867619     103373.55\nExterior_1st    6.699528      95407.73\nOverall_Cond    6.676218      96814.53\nGarage_Cond     6.622676      49944.12\nExterior_2nd    5.907220      61992.47\nLatitude        5.872897      72448.76\nLot_Area        5.696540     134015.35\nLongitude       5.055073      71328.45\nGarage_Type     4.686528      37116.56\nBsmtFin_SF_1    4.543646      10169.18\nBsmt_Exposure   4.500273      48517.30\nFull_Bath       4.416821      84402.03\nGarage_Finish   4.405324      24862.60\nSale_Condition  4.173237      25932.41\nTotRms_AbvGrd   4.162582      27350.80\nCentral_Air     4.080131      21049.07\nHeating_QC      3.922138      19705.37\nMas_Vnr_Area    3.862446      72973.11\n\n\nImportance values can be plotted directly:\n\n\nCode\ninvVIP &lt;-VIP[order(VIP[,1], decreasing = FALSE),1] \ntVIP&lt;- tail(invVIP, n=15)\nbarplot(tVIP, horiz = TRUE, cex.names=0.5)\n\n\n\n\n\n\n\n\n\nAlternatively one can use the vipfunction from the vip package\n\n\nCode\nlibrary(vip)\nvip(bag.Ames, num_features = 40, bar = FALSE)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Random Forest Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#parameter-optimization-for-rf",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#parameter-optimization-for-rf",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Parameter optimization for RF",
    "text": "Parameter optimization for RF\nSeveral parameters can be changed to optimize a random forest predictor, but, usually, the most important one is the number of variables to be randomly selected at each split \\(m_{try}\\), followed by the number of tree, which tends to stabilize after a certain value.\nA common strategy to find the optimum combination of parameters is to perform a grid search through a combination of parameter values. Obviously it can be time consuming so a small grid is run in the example below to illustrate how to do it.\n\n\nCode\nnum_trees_range &lt;- seq(100, 400, 100)\n\nnum_vars_range &lt;- floor(ncol(ames_train)/(seq(2,4,1)))\n\nRFerrTable &lt;- data.frame(Model=character(), \n                       NumTree=integer(), NumVar=integer(), \n                       RMSE=double())\nerrValue &lt;- 1\nsystem.time(\nfor (i in seq_along(num_trees_range)){\n  for (j in seq_along(num_vars_range)) {  \n    numTrees &lt;- num_trees_range[i]\n    numVars &lt;- num_vars_range [j] # floor(ncol(ames_train)/3) # default\n    RF.Ames.n &lt;- randomForest(Sale_Price ~ ., \n                         data = ames_train, \n                         mtry = numVars,\n                         ntree= numTrees,\n                         importance = TRUE)\n    yhat.rf &lt;- predict(RF.Ames.n, newdata = ames_test)\n    oob.rf &lt;- RF.Ames.n$predicted\n    \n    test_rmse_rf  &lt;- sqrt(mean((yhat.rf - ames_test$Sale_Price)^2))\n\n  \n    RFerrTable[errValue, ] &lt;-  c(\"Random Forest\", \n                            NumTree = numTrees, NumVar = numVars, \n                            RMSE = round(test_rmse_rf,2)) \n    errValue &lt;- errValue+1\n  }\n}\n)\n\n\n   user  system elapsed \n 180.40    1.05  186.91 \n\n\n\n\nCode\nRFerrTable %&gt;% knitr::kable()\n\n\n\n\n\nModel\nNumTree\nNumVar\nRMSE\n\n\n\n\nRandom Forest\n100\n37\n24.51\n\n\nRandom Forest\n100\n24\n24.85\n\n\nRandom Forest\n100\n18\n24.79\n\n\nRandom Forest\n200\n37\n24.66\n\n\nRandom Forest\n200\n24\n24.86\n\n\nRandom Forest\n200\n18\n24.74\n\n\nRandom Forest\n300\n37\n24.51\n\n\nRandom Forest\n300\n24\n24.53\n\n\nRandom Forest\n300\n18\n25.12\n\n\nRandom Forest\n400\n37\n24.58\n\n\nRandom Forest\n400\n24\n24.62\n\n\nRandom Forest\n400\n18\n24.84\n\n\n\n\n\nThe minimum RMSE is attained at.\n\n\nCode\nbestRF &lt;- which(RFerrTable$RMSE==min(RFerrTable$RMSE))\nRFerrTable[bestRF,]\n\n\n          Model NumTree NumVar  RMSE\n1 Random Forest     100     37 24.51\n7 Random Forest     300     37 24.51\n\n\nCode\nminRFErr &lt;- as.numeric(RFerrTable[bestRF,4])\nerrTable[8, ] &lt;-  c(\"Random Forest (Optimized)\", round(minRFErr,2))",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Random Forest Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#error-comparison-for-all-approaches",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#error-comparison-for-all-approaches",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Error comparison for all approaches",
    "text": "Error comparison for all approaches\n\n\nCode\n# kableExtra::kable(errTable) %&gt;% kableExtra::kable_styling()\nknitr::kable(errTable) \n\n\n\n\n\nModel\nRMSE\n\n\n\n\nDefault Regression Tree\n39.69\n\n\nBig Regression Tree\n37.59\n\n\nOptimally pruned Regression Tree\n39.69\n\n\nBagged Tree with Train Data\n10.95\n\n\nBagged Tree with Test Data\n24.6\n\n\nBagged Tree with OOB error rate\n27.15\n\n\nRandom Forest (defaults)\n24.57\n\n\nRandom Forest (Optimized)\n24.51\n\n\n\n\n\nIn summary, it has been shown that a Random Forest with 400 trees and 37 variables provides the smallest error rate, though the improvement on the default values and even the bagging approach is very small. This may be seen as a confirmation from the fact that Random Forests are well known to be good “out-of-the-box predictors”, that is that they perform well, even without tuning.",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Random Forest Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#variable-importance-1",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#variable-importance-1",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Variable importance",
    "text": "Variable importance\nAs could be expected, a variable importance plot shows that there is hardly any difference between the variables by bagging or random forests.\n\n\nCode\nlibrary(vip)\nvip(RF.Ames, num_features = 40, bar = FALSE)",
    "crumbs": [
      "Home",
      "Ensemble Methods",
      "Random Forest Lab"
    ]
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html",
    "href": "labs/Lab-C1.1-knn4regression.html",
    "title": "k-NN regression",
    "section": "",
    "text": "The \\(k\\) nearest-neighbor estimator of \\(m(t)=E(Y|X=t)\\) is defined as \\[\n\\hat{m}(t) = \\frac{1}{k} \\sum_{i\\in N_k(t)} y_i,\n\\] where \\(N_k(t)\\) is the neighborhood of \\(t\\) defined by the \\(k\\) closest points \\(x_i\\) in the training sample.\n\n\nThe Boston housing data, included in the MASS package, contains information on housing in the Boston suburbs area.\n\nlibrary(MASS)\n# help(Boston)\ndata(Boston)\n\n\n\n\nOur goal is to predict the housing mean value, stored in the mdev variable, using as predictor variable the percentage of population with lower socio-economic status. lstat.\n\n\n\nWe start by plotting the values to show the relation among them.\n\nx &lt;- Boston$lstat\ny &lt;- Boston$medv\nplot(x,y, xlab=\"x: lstat\", ylab=\"y: medv\")\n\n\n\n\n\n\n\n\nWe migt consider fitting a polynomial curve but we start with KNN"
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html#data-for-prediction",
    "href": "labs/Lab-C1.1-knn4regression.html#data-for-prediction",
    "title": "k-NN regression",
    "section": "",
    "text": "The Boston housing data, included in the MASS package, contains information on housing in the Boston suburbs area.\n\nlibrary(MASS)\n# help(Boston)\ndata(Boston)"
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html#main-predicion-goal",
    "href": "labs/Lab-C1.1-knn4regression.html#main-predicion-goal",
    "title": "k-NN regression",
    "section": "",
    "text": "Our goal is to predict the housing mean value, stored in the mdev variable, using as predictor variable the percentage of population with lower socio-economic status. lstat."
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html#data-description",
    "href": "labs/Lab-C1.1-knn4regression.html#data-description",
    "title": "k-NN regression",
    "section": "",
    "text": "We start by plotting the values to show the relation among them.\n\nx &lt;- Boston$lstat\ny &lt;- Boston$medv\nplot(x,y, xlab=\"x: lstat\", ylab=\"y: medv\")\n\n\n\n\n\n\n\n\nWe migt consider fitting a polynomial curve but we start with KNN"
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html#tuning-the-model",
    "href": "labs/Lab-C1.1-knn4regression.html#tuning-the-model",
    "title": "k-NN regression",
    "section": "Tuning the model",
    "text": "Tuning the model\nRepeat the same process using different values of \\(k\\).\n\nk_values &lt;- c(100, 50, 15, 5)\n\nfor (k in k_values) {\n    mt &lt;- knn_regr(x, y, t=t, k=k, dist.method = \"euclidean\")\n    \n    plot(x, y, col=8, xlab=\"x: lstat\", ylab=\"y: medv\", \n         main=paste0(\"k-NN con k=\", k))\n    lines(t, mt, col=2, lwd=4)\n}"
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html#what-is-training-error",
    "href": "labs/Lab-C1.1-knn4regression.html#what-is-training-error",
    "title": "k-NN regression",
    "section": "What is Training Error?",
    "text": "What is Training Error?\nTraining error is calculated by evaluating the model’s predictions on the same dataset that was used to fit the model. Mathematically, it is computed as:\n\\[\n\\text{Training Error} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{m}(x_i))^2\n\\]\nwhere: - ( y_i ) are the observed values, - ( (x_i) ) are the predicted values from the model, - ( n ) is the total number of observations.\n\nWhy is Training Error Misleading?\nSince the model is evaluated on the same data it was trained on, the training error is overly optimistic and does not provide a reliable measure of how well the model generalizes to new data. This issue is particularly problematic when dealing with flexible models such as k-nearest neighbors (k-NN).\n\n\nExample: k-NN and Overfitting\nIn k-NN regression, the choice of ( k ) significantly affects the training error: - When ( k ) is very small, the model closely follows the training data, leading to low training error but high test error (overfitting). - When ( k ) is too large, the model smooths out the predictions too much, leading to higher training and test error (underfitting).\n\nk_values &lt;- c(200, 100, 50, 15, 5, 3)\n\nerrors &lt;- data.frame(k = integer(), RMSE = numeric())\n\n\nfor (k in k_values) {\n    mt &lt;- knn_regr(x, y, t=t, k=k, dist.method = \"euclidean\")\n    \n\n    RMSE &lt;- sqrt(mean((y - mt)^2))  \n    \n    errors &lt;- rbind(errors, data.frame(k = k, RMSE = RMSE))\n    \n    plot(x, y, col=8, xlab=\"x: lstat\", ylab=\"y: medv\", \n         main=paste0(\"k-NN con k=\", k, \" (RMSE: \", round(RMSE, 2), \")\"))\n    lines(t, mt, col=2, lwd=4)\n}\n\nWarning in y - mt: longitud de objeto mayor no es múltiplo de la longitud de\nuno menor\n\n\n\n\n\n\n\n\n\nWarning in y - mt: longitud de objeto mayor no es múltiplo de la longitud de\nuno menor\n\n\n\n\n\n\n\n\n\nWarning in y - mt: longitud de objeto mayor no es múltiplo de la longitud de\nuno menor\n\n\n\n\n\n\n\n\n\nWarning in y - mt: longitud de objeto mayor no es múltiplo de la longitud de\nuno menor\n\n\n\n\n\n\n\n\n\nWarning in y - mt: longitud de objeto mayor no es múltiplo de la longitud de\nuno menor\n\n\n\n\n\n\n\n\n\nWarning in y - mt: longitud de objeto mayor no es múltiplo de la longitud de\nuno menor\n\n\n\n\n\n\n\n\nprint(errors)\n\n    k     RMSE\n1 200 10.57989\n2 100 11.57428\n3  50 12.49364\n4  15 13.15771\n5   5 12.71861\n6   3 13.11624"
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html#a-better-approach-using-test-error",
    "href": "labs/Lab-C1.1-knn4regression.html#a-better-approach-using-test-error",
    "title": "k-NN regression",
    "section": "A Better Approach: Using Test Error",
    "text": "A Better Approach: Using Test Error\nTo properly evaluate model performance, we must estimate the test error, which is computed on a separate dataset that was not used for training. This can be done by: 1. Splitting the data into a training set and a test set. 2. Training the model on the training set. 3. Evaluating the model on the test set and computing the test error:\n\\[\n\\text{Test Error} = \\frac{1}{m} \\sum_{j=1}^{m} (y_j^{\\text{test}} - \\hat{m}(x_j^{\\text{test}}))^2\n\\]\nwhere: - ( y_j^{} ) are the actual test values, - ( (x_j^{}) ) are the model’s predictions on the test set, - ( m ) is the number of test observations.\nUsing training error alone can give a false sense of accuracy. Instead, we should always assess model performance on unseen data (test set) to ensure that our model generalizes well. In the next section, we will implement k-NN regression with a train-test split to correctly measure prediction error.\n\nAn (improved) example\n\nk_values &lt;- c(200, 100, 50, 15, 5, 3)\n\nerrors &lt;- data.frame(k = integer(), Training_RMSE = numeric(), Test_RMSE = numeric())\n\nset.seed(123)\nn &lt;- length(y)\ntrain_index &lt;- sample(1:n, size = floor(0.8 * n), replace = FALSE)\n\nx_train &lt;- x[train_index]\ny_train &lt;- y[train_index]\n\nx_test &lt;- x[-train_index]\ny_test &lt;- y[-train_index]\n\n\nfor (k in k_values) {\n    mt_train &lt;- knn_regr(x_train, y_train, t=x_train, k=k, dist.method = \"euclidean\")\n    \n    mt_test &lt;- knn_regr(x_train, y_train, t=x_test, k=k, dist.method = \"euclidean\")\n    \n    RMSE_train &lt;- sqrt(mean((y_train - mt_train)^2))  \n    RMSE_test &lt;- sqrt(mean((y_test - mt_test)^2))  \n    \n    errors &lt;- rbind(errors, data.frame(k = k, Training_RMSE = RMSE_train, Test_RMSE = RMSE_test))\n    \n    plot(x_train, y_train, col=8, xlab=\"x: lstat\", ylab=\"y: medv\", \n         main=paste0(\"k-NN con k=\", k, \" (Train RMSE: \", round(RMSE_train, 2), \n                      \", Test RMSE: \", round(RMSE_test, 2), \")\"))\n    \n    t_seq &lt;- seq(min(x), max(x), length.out=100)\n    mt_seq &lt;- knn_regr(x_train, y_train, t=t_seq, k=k, dist.method = \"euclidean\")\n    lines(t_seq, mt_seq, col=2, lwd=4)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(errors)\n\n    k Training_RMSE Test_RMSE\n1 200      6.540422  6.589752\n2 100      5.624611  5.867107\n3  50      5.146213  5.624467\n4  15      4.929732  5.709414\n5   5      4.530371  6.175764\n6   3      4.229030  6.422137"
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html#function-code",
    "href": "labs/Lab-C1.1-knn4regression.html#function-code",
    "title": "k-NN regression",
    "section": "Function Code",
    "text": "Function Code\n\nknn_regr &lt;- function(x, y, t=NULL, k=3, dist.method = \"euclidean\") {\n    nx &lt;- length(y)  # Number of observations in the training data\n    \n    # If t is not provided, use x as the set of query points\n    if (is.null(t)) { \n        t &lt;- as.matrix(x) \n    } else {\n        t &lt;- as.matrix(t)\n    }\n    \n    nt &lt;- dim(t)[1]  # Number of query points\n    \n    # Compute the distance matrix between each t[j] and all x[i]\n    Dtx &lt;- as.matrix(dist(rbind(t, as.matrix(x)), method = dist.method))\n    \n    # Extract only the distances between test points and training points\n    Dtx &lt;- Dtx[1:nt, nt+(1:nx)]\n    \n    mt &lt;- numeric(nt)  # Initialize the vector of predictions\n    \n    # Compute the k-NN estimate for each query point t[j]\n    for (j in 1:nt) {\n        d_t_x &lt;- Dtx[j,]  # Distances from t[j] to all training points\n        d_t_x_k &lt;- sort(d_t_x, partial=k)[k]  # Distance to the k-th nearest neighbor\n        N_t_k &lt;- unname(which(d_t_x &lt;= d_t_x_k))  # Indices of the k nearest neighbors\n        \n        mt[j] &lt;- mean(y[N_t_k])  # Compute the mean response of k nearest neighbors\n    }\n    \n    return(mt)  # Return the vector of estimated values\n}"
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html#step-by-step-explanation",
    "href": "labs/Lab-C1.1-knn4regression.html#step-by-step-explanation",
    "title": "k-NN regression",
    "section": "Step-by-Step Explanation",
    "text": "Step-by-Step Explanation\n\n1. Function Inputs\nThe function takes the following inputs: - x: The vector of predictor values (training data). - y: The corresponding response values. - t: A vector of test points where we want to estimate ( E(Y | X = t) ). If t is not provided, it defaults to x (in-sample estimation). - k: The number of nearest neighbors to consider. - dist.method: The method used to compute distances (default: Euclidean).\n\n\n2. Checking the Input t\nIf t is NULL, the function assigns x to t, meaning that predictions will be computed for the training points.\n\nif (is.null(t)){ \n    t &lt;- as.matrix(x) \n} else {\n    t &lt;- as.matrix(t)\n}\n\n\n\n3. Compute the Distance Matrix\nThe function calculates pairwise distances between every point in t and every point in x using the dist function.\n\ndist.method &lt;- \"euclidean\"\nDtx &lt;- as.matrix(dist(rbind(t, as.matrix(x)), method = dist.method))\n\nAfter computing the distance matrix, we extract only the distances between test points and training points:\n\nDtx &lt;- Dtx[1:nt, nt+(1:nx)]\n\n\n\n4. Find the k Nearest Neighbors\nFor each test point ( t[j] ): - Extract the distances to all training points. - Identify the k-th smallest distance. - Select the indices of the k closest training points.\n\nd_t_x &lt;- Dtx[j,]  # Distances from t[j] to all training points\nd_t_x_k &lt;- sort(d_t_x, partial=k)[k]  # Distance to the k-th nearest neighbor\nN_t_k &lt;- unname(which(d_t_x &lt;= d_t_x_k))  # Indices of the k nearest neighbors\n\n\n\n5. Compute the k-NN Estimate\nOnce the nearest neighbors are identified, their corresponding y values are averaged to compute the estimate:\n\nmt[j] &lt;- mean(y[N_t_k])  # Compute the mean response of k nearest neighbors\n\n\n\n6. Return the Estimated Values\nFinally, the function returns the vector mt containing the estimated values for all test points.\n\nreturn(mt)"
  },
  {
    "objectID": "labs/Lab-C2.1-PIMAIndian.html",
    "href": "labs/Lab-C2.1-PIMAIndian.html",
    "title": "Decision Trees Lab 0",
    "section": "",
    "text": "The Pima Indian Diabetes data set (PimaIndiansDiabetes2) is available in the mlbench package.\n\n\nCode\ndata(\"PimaIndiansDiabetes2\", package = \"mlbench\")\n\n\nThe data contains 768 individuals (female) and 9 clinical variables for predicting the probability of individuals in being diabete-positive or negative:\n\npregnant: number of times pregnant\nglucose: plasma glucose concentration\npressure: diastolic blood pressure (mm Hg)\ntriceps: triceps skin fold thickness (mm)\ninsulin: 2-Hour serum insulin (mu U/ml)\nmass: body mass index (weight in kg/(height in m)^2)\npedigree: diabetes pedigree function\nage: age (years)\ndiabetes: class variable\n\n\n\nCode\ndplyr::glimpse(PimaIndiansDiabetes2)\n\n\nRows: 768\nColumns: 9\n$ pregnant &lt;dbl&gt; 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, 0, 7, 1, 1…\n$ glucose  &lt;dbl&gt; 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168, 139,…\n$ pressure &lt;dbl&gt; 72, 66, 64, 66, 40, 74, 50, NA, 70, 96, 92, 74, 80, 60, 72, N…\n$ triceps  &lt;dbl&gt; 35, 29, NA, 23, 35, NA, 32, NA, 45, NA, NA, NA, NA, 23, 19, N…\n$ insulin  &lt;dbl&gt; NA, NA, NA, 94, 168, NA, 88, NA, 543, NA, NA, NA, NA, 846, 17…\n$ mass     &lt;dbl&gt; 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, NA, 37.…\n$ pedigree &lt;dbl&gt; 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.134, 0.158…\n$ age      &lt;dbl&gt; 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 59, 51, 3…\n$ diabetes &lt;fct&gt; pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg, pos, n…\n\n\nA typical classification/prediction problem is to build a model that can distinguish and predict diabetes using some or all the variables in the dataset.\nA quick exploration can be done wirh the skimr package:\n\n\nCode\nlibrary(skimr)\nskim(PimaIndiansDiabetes2)\n\n\n\nData summary\n\n\nName\nPimaIndiansDiabetes2\n\n\nNumber of rows\n768\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndiabetes\n0\n1\nFALSE\n2\nneg: 500, pos: 268\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npregnant\n0\n1.00\n3.85\n3.37\n0.00\n1.00\n3.00\n6.00\n17.00\n▇▃▂▁▁\n\n\nglucose\n5\n0.99\n121.69\n30.54\n44.00\n99.00\n117.00\n141.00\n199.00\n▁▇▇▃▂\n\n\npressure\n35\n0.95\n72.41\n12.38\n24.00\n64.00\n72.00\n80.00\n122.00\n▁▃▇▂▁\n\n\ntriceps\n227\n0.70\n29.15\n10.48\n7.00\n22.00\n29.00\n36.00\n99.00\n▆▇▁▁▁\n\n\ninsulin\n374\n0.51\n155.55\n118.78\n14.00\n76.25\n125.00\n190.00\n846.00\n▇▂▁▁▁\n\n\nmass\n11\n0.99\n32.46\n6.92\n18.20\n27.50\n32.30\n36.60\n67.10\n▅▇▃▁▁\n\n\npedigree\n0\n1.00\n0.47\n0.33\n0.08\n0.24\n0.37\n0.63\n2.42\n▇▃▁▁▁\n\n\nage\n0\n1.00\n33.24\n11.76\n21.00\n24.00\n29.00\n41.00\n81.00\n▇▃▁▁▁"
  },
  {
    "objectID": "labs/Lab-C2.1-PIMAIndian.html#the-pima-indians-dataset",
    "href": "labs/Lab-C2.1-PIMAIndian.html#the-pima-indians-dataset",
    "title": "Decision Trees Lab 0",
    "section": "",
    "text": "The Pima Indian Diabetes data set (PimaIndiansDiabetes2) is available in the mlbench package.\n\n\nCode\ndata(\"PimaIndiansDiabetes2\", package = \"mlbench\")\n\n\nThe data contains 768 individuals (female) and 9 clinical variables for predicting the probability of individuals in being diabete-positive or negative:\n\npregnant: number of times pregnant\nglucose: plasma glucose concentration\npressure: diastolic blood pressure (mm Hg)\ntriceps: triceps skin fold thickness (mm)\ninsulin: 2-Hour serum insulin (mu U/ml)\nmass: body mass index (weight in kg/(height in m)^2)\npedigree: diabetes pedigree function\nage: age (years)\ndiabetes: class variable\n\n\n\nCode\ndplyr::glimpse(PimaIndiansDiabetes2)\n\n\nRows: 768\nColumns: 9\n$ pregnant &lt;dbl&gt; 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, 0, 7, 1, 1…\n$ glucose  &lt;dbl&gt; 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168, 139,…\n$ pressure &lt;dbl&gt; 72, 66, 64, 66, 40, 74, 50, NA, 70, 96, 92, 74, 80, 60, 72, N…\n$ triceps  &lt;dbl&gt; 35, 29, NA, 23, 35, NA, 32, NA, 45, NA, NA, NA, NA, 23, 19, N…\n$ insulin  &lt;dbl&gt; NA, NA, NA, 94, 168, NA, 88, NA, 543, NA, NA, NA, NA, 846, 17…\n$ mass     &lt;dbl&gt; 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, NA, 37.…\n$ pedigree &lt;dbl&gt; 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.134, 0.158…\n$ age      &lt;dbl&gt; 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 59, 51, 3…\n$ diabetes &lt;fct&gt; pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg, pos, n…\n\n\nA typical classification/prediction problem is to build a model that can distinguish and predict diabetes using some or all the variables in the dataset.\nA quick exploration can be done wirh the skimr package:\n\n\nCode\nlibrary(skimr)\nskim(PimaIndiansDiabetes2)\n\n\n\nData summary\n\n\nName\nPimaIndiansDiabetes2\n\n\nNumber of rows\n768\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndiabetes\n0\n1\nFALSE\n2\nneg: 500, pos: 268\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npregnant\n0\n1.00\n3.85\n3.37\n0.00\n1.00\n3.00\n6.00\n17.00\n▇▃▂▁▁\n\n\nglucose\n5\n0.99\n121.69\n30.54\n44.00\n99.00\n117.00\n141.00\n199.00\n▁▇▇▃▂\n\n\npressure\n35\n0.95\n72.41\n12.38\n24.00\n64.00\n72.00\n80.00\n122.00\n▁▃▇▂▁\n\n\ntriceps\n227\n0.70\n29.15\n10.48\n7.00\n22.00\n29.00\n36.00\n99.00\n▆▇▁▁▁\n\n\ninsulin\n374\n0.51\n155.55\n118.78\n14.00\n76.25\n125.00\n190.00\n846.00\n▇▂▁▁▁\n\n\nmass\n11\n0.99\n32.46\n6.92\n18.20\n27.50\n32.30\n36.60\n67.10\n▅▇▃▁▁\n\n\npedigree\n0\n1.00\n0.47\n0.33\n0.08\n0.24\n0.37\n0.63\n2.42\n▇▃▁▁▁\n\n\nage\n0\n1.00\n33.24\n11.76\n21.00\n24.00\n29.00\n41.00\n81.00\n▇▃▁▁▁"
  },
  {
    "objectID": "labs/Lab-C2.1-PIMAIndian.html#assessing-model-performance",
    "href": "labs/Lab-C2.1-PIMAIndian.html#assessing-model-performance",
    "title": "Decision Trees Lab 0",
    "section": "Assessing model performance",
    "text": "Assessing model performance\nImagine we kow nothing about overfitting.\nWe may want to check the accuracy of the model on the dataset we have used to build it.\n\n\nCode\npredicted.classes&lt;- predict(model1, PimaIndiansDiabetes2, \"class\")\nmean(predicted.classes == PimaIndiansDiabetes2$diabetes)\n\n\n[1] 0.8294271\n\n\nA better strategy is to use train dataset to build the model and a test dataset to check how it works.\n\n\nCode\nset.seed(123)\nssize &lt;- nrow(PimaIndiansDiabetes2)\npropTrain &lt;- 0.8\ntraining.indices &lt;-sample(1:ssize, floor(ssize*propTrain))\ntrain.data  &lt;- PimaIndiansDiabetes2[training.indices, ]\ntest.data &lt;- PimaIndiansDiabetes2[-training.indices, ]\n\n\nNow we build the model on the train data and check its accuracy on the test data.\n\n\nCode\nmodel2 &lt;- rpart(diabetes ~., data = train.data)\npredicted.classes.test&lt;- predict(model2, test.data, \"class\")\nmean(predicted.classes.test == test.data$diabetes)\n\n\n[1] 0.7272727\n\n\nThe accuracy is good, but smaller, as expected."
  },
  {
    "objectID": "labs/Lab-C2.1-PIMAIndian.html#making-predictions-with-the-model",
    "href": "labs/Lab-C2.1-PIMAIndian.html#making-predictions-with-the-model",
    "title": "Decision Trees Lab 0",
    "section": "Making predictions with the model",
    "text": "Making predictions with the model\nAs an example on how to use the model we want to predict the class of individuals 521 and 562\n\n\nCode\n(aSample&lt;- PimaIndiansDiabetes2[c(521,562),])\n\n\n    pregnant glucose pressure triceps insulin mass pedigree age diabetes\n521        2      68       70      32      66 25.0    0.187  25      neg\n562        0     198       66      32     274 41.3    0.502  28      pos\n\n\n\n\nCode\npredict(model1, aSample, \"class\")\n\n\n521 562 \nneg pos \nLevels: neg pos\n\n\n\nIf we follow individuals 521 and 562 along the tree, we reach the same prediction.\nThe tree provides not only a classification but also an explanation."
  },
  {
    "objectID": "labs/Lab-C2.3-Py-ISLch08-baggboost.html",
    "href": "labs/Lab-C2.3-Py-ISLch08-baggboost.html",
    "title": "Lab: Tree based methods",
    "section": "",
    "text": "This lab presents some examples of applying shallow and ensemble-based trees.\nIt is adapted directly from lab 8 in “Introduction to Statistical Learning with Python examples”, 2nd edition, and it can be run in parallel in Python or in R using the respective notebooks for each language downloadable from the book’s website.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees Lab (Python)"
    ]
  },
  {
    "objectID": "labs/Lab-C2.3-Py-ISLch08-baggboost.html#introduction",
    "href": "labs/Lab-C2.3-Py-ISLch08-baggboost.html#introduction",
    "title": "Lab: Tree based methods",
    "section": "",
    "text": "This lab presents some examples of applying shallow and ensemble-based trees.\nIt is adapted directly from lab 8 in “Introduction to Statistical Learning with Python examples”, 2nd edition, and it can be run in parallel in Python or in R using the respective notebooks for each language downloadable from the book’s website.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees Lab (Python)"
    ]
  },
  {
    "objectID": "labs/Lab-C2.3-Py-ISLch08-baggboost.html#install-load-required-libraries",
    "href": "labs/Lab-C2.3-Py-ISLch08-baggboost.html#install-load-required-libraries",
    "title": "Lab: Tree based methods",
    "section": "Install / load required libraries",
    "text": "Install / load required libraries\nWe import some of our usual libraries at this top level.\nIt is assumed that all required libraries have been installed. If this is not the case any library (e.g.ISLP) can be installed with the following code:\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.pyplot import subplots\nimport sklearn.model_selection as skm\nfrom ISLP import load_data, confusion_table \nfrom ISLP.models import ModelSpec as MS\n\nWe also collect the new imports needed for this lab.\n\nfrom sklearn.tree import (DecisionTreeClassifier as DTC,\n                          DecisionTreeRegressor as DTR,\n                          plot_tree,\n                          export_text)\nfrom sklearn.metrics import (accuracy_score,\n                             log_loss)\nfrom sklearn.ensemble import \\\n     (RandomForestRegressor as RF,\n      GradientBoostingRegressor as GBR)\nfrom ISLP.bart import BART",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees Lab (Python)"
    ]
  },
  {
    "objectID": "labs/Lab-C2.3-Py-ISLch08-baggboost.html#fitting-classification-trees",
    "href": "labs/Lab-C2.3-Py-ISLch08-baggboost.html#fitting-classification-trees",
    "title": "Lab: Tree based methods",
    "section": "Fitting Classification Trees",
    "text": "Fitting Classification Trees\nThe first example illustrates how to build classification and regression trees.\nWe first use classification trees to analyze the Carseats data set. In these data, Sales is a continuous variable, and so we begin by recoding it as a binary variable. We use the where() function to create a variable, called High, which takes on a value of Yes if the Sales variable exceeds 8, and takes on a value of No otherwise.\n\nCarseats = load_data('Carseats')\nHigh = np.where(Carseats.Sales &gt; 8,\n                \"Yes\",\n                \"No\")\n\nWe now use DecisionTreeClassifier() to fit a classification tree in order to predict High using all variables but Sales. To do so, we must form a model matrix as we did when fitting regression models.\n\nmodel = MS(Carseats.columns.drop('Sales'), intercept=False)\nD = model.fit_transform(Carseats)\nfeature_names = list(D.columns)\nX = np.asarray(D)\n\nWe have converted D from a data frame to an array X, which is needed in some of the analysis below. We also need the feature_names for annotating our plots later.\nThere are several options needed to specify the classifier, such as max_depth (how deep to grow the tree), min_samples_split (minimum number of observations in a node to be eligible for splitting) and criterion (whether to use Gini or cross-entropy as the split criterion). We also set random_state for reproducibility; ties in the split criterion are broken at random.\n\nclf = DTC(criterion='entropy',\n          max_depth=3,\n          random_state=0)        \nclf.fit(X, High)\n\nDecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\n\n\nIn our discussion of qualitative features in Section 3.3, we noted that for a linear regression model such a feature could be represented by including a matrix of dummy variables (one-hot-encoding) in the model matrix, using the formula notation of statsmodels. As mentioned in Section 8.1, there is a more natural way to handle qualitative features when building a decision tree, that does not require such dummy variables; each split amounts to partitioning the levels into two groups. However, the sklearn implementation of decision trees does not take advantage of this approach; instead it simply treats the one-hot-encoded levels as separate variables.\n\nacs = accuracy_score(High, clf.predict(X))\nacs\n\n0.79\n\n\nWith only the default arguments, the training error rate is 21%. For classification trees, we can access the value of the deviance using log_loss(), \\[\\begin{equation*}\n\\begin{split}\n-2 \\sum_m \\sum_k n_{mk} \\log \\hat{p}_{mk},\n\\end{split}\n\\end{equation*}\\] where \\(n_{mk}\\) is the number of observations in the \\(m\\)th terminal node that belong to the \\(k\\)th class.\n\nresid_dev = np.sum(log_loss(High, clf.predict_proba(X)))\nresid_dev\n\n0.4710647062649358\n\n\nThis is closely related to the entropy, defined in (8.7). A small deviance indicates a tree that provides a good fit to the (training) data.\nOne of the most attractive properties of trees is that they can be graphically displayed. Here we use the plot() function to display the tree structure.\n\nax = subplots(figsize=(12,12))[1]\nplot_tree(clf,\n          feature_names=feature_names,\n          ax=ax);\n\n\n\n\n\n\n\n\nThe most important indicator of Sales appears to be ShelveLoc.\nWe can see a text representation of the tree using export_text(), which displays the split criterion (e.g. Price &lt;= 92.5) for each branch. For leaf nodes it shows the overall prediction\n(Yes or No). We can also see the number of observations in that leaf that take on values of Yes and No by specifying show_weights=True.\n\nprint(export_text(clf,\n                  feature_names=feature_names,\n                  show_weights=True))\n\n|--- ShelveLoc[Good] &lt;= 0.50\n|   |--- Price &lt;= 92.50\n|   |   |--- Income &lt;= 57.00\n|   |   |   |--- weights: [7.00, 3.00] class: No\n|   |   |--- Income &gt;  57.00\n|   |   |   |--- weights: [7.00, 29.00] class: Yes\n|   |--- Price &gt;  92.50\n|   |   |--- Advertising &lt;= 13.50\n|   |   |   |--- weights: [183.00, 41.00] class: No\n|   |   |--- Advertising &gt;  13.50\n|   |   |   |--- weights: [20.00, 25.00] class: Yes\n|--- ShelveLoc[Good] &gt;  0.50\n|   |--- Price &lt;= 135.00\n|   |   |--- US[Yes] &lt;= 0.50\n|   |   |   |--- weights: [6.00, 11.00] class: Yes\n|   |   |--- US[Yes] &gt;  0.50\n|   |   |   |--- weights: [2.00, 49.00] class: Yes\n|   |--- Price &gt;  135.00\n|   |   |--- Income &lt;= 46.00\n|   |   |   |--- weights: [6.00, 0.00] class: No\n|   |   |--- Income &gt;  46.00\n|   |   |   |--- weights: [5.00, 6.00] class: Yes\n\n\n\nIn order to properly evaluate the performance of a classification tree on these data, we must estimate the test error rather than simply computing the training error. We split the observations into a training set and a test set, build the tree using the training set, and evaluate its performance on the test data. This pattern is similar to that in Chapter 6, with the linear models replaced here by decision trees — the code for validation is almost identical. This approach leads to correct predictions for 68.5% of the locations in the test data set.\n\nvalidation = skm.ShuffleSplit(n_splits=1,\n                              test_size=200,\n                              random_state=0)\nresults = skm.cross_validate(clf,\n                             D,\n                             High,\n                             cv=validation)\nresults['test_score']\n\narray([0.685])\n\n\nNext, we consider whether pruning the tree might lead to improved classification performance. We first split the data into a training and test set. We will use cross-validation to prune the tree on the training set, and then evaluate the performance of the pruned tree on the test set.\n\n(X_train,\n X_test,\n High_train,\n High_test) = skm.train_test_split(X,\n                                   High,\n                                   test_size=0.5,\n                                   random_state=0)\n                                   \n\nWe first refit the full tree on the training set; here we do not set a max_depth parameter, since we will learn that through cross-validation.\n\nclf = DTC(criterion='entropy', random_state=0)\nclf.fit(X_train, High_train)\naccuracy_score(High_test, clf.predict(X_test))\n\n0.735\n\n\nNext we use the cost_complexity_pruning_path() method of clf to extract cost-complexity values.\n\nccp_path = clf.cost_complexity_pruning_path(X_train, High_train)\nkfold = skm.KFold(10,\n                  random_state=1,\n                  shuffle=True)\n\nThis yields a set of impurities and \\(\\alpha\\) values from which we can extract an optimal one by cross-validation.\n\ngrid = skm.GridSearchCV(clf,\n                        {'ccp_alpha': ccp_path.ccp_alphas},\n                        refit=True,\n                        cv=kfold,\n                        scoring='accuracy')\ngrid.fit(X_train, High_train)\ngrid.best_score_\n\n0.685\n\n\nLet’s take a look at the pruned true.\n\nax = subplots(figsize=(12, 12))[1]\nbest_ = grid.best_estimator_\nplot_tree(best_,\n          feature_names=feature_names,\n          ax=ax);\n\n\n\n\n\n\n\n\nThis is quite a bushy tree. We could count the leaves, or query best_ instead.\n\nbest_.tree_.n_leaves\n\n30\n\n\nThe tree with 30 terminal nodes results in the lowest cross-validation error rate, with an accuracy of 68.5%. How well does this pruned tree perform on the test data set? Once again, we apply the predict() function.\n\nprint(accuracy_score(High_test,\n                     best_.predict(X_test)))\nconfusion = confusion_table(best_.predict(X_test),\n                            High_test)\nconfusion\n\n0.72\n\n\n\n\n\n\n\n\nTruth\nNo\nYes\n\n\nPredicted\n\n\n\n\n\n\nNo\n94\n32\n\n\nYes\n24\n50\n\n\n\n\n\n\n\nNow 72.0% of the test observations are correctly classified, which is slightly worse than the error for the full tree (with 35 leaves). So cross-validation has not helped us much here; it only pruned off 5 leaves, at a cost of a slightly worse error. These results would change if we were to change the random number seeds above; even though cross-validation gives an unbiased approach to model selection, it does have variance.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees Lab (Python)"
    ]
  },
  {
    "objectID": "labs/Lab-C2.3-Py-ISLch08-baggboost.html#fitting-regression-trees",
    "href": "labs/Lab-C2.3-Py-ISLch08-baggboost.html#fitting-regression-trees",
    "title": "Lab: Tree based methods",
    "section": "Fitting Regression Trees",
    "text": "Fitting Regression Trees\nHere we fit a regression tree to the Boston data set. The steps are similar to those for classification trees.\n\nBoston = load_data(\"Boston\")\nmodel = MS(Boston.columns.drop('medv'), intercept=False)\nD = model.fit_transform(Boston)\nfeature_names = list(D.columns)\nX = np.asarray(D)\n\nFirst, we split the data into training and test sets, and fit the tree to the training data. Here we use 30% of the data for the test set.\n\n(X_train,\n X_test,\n y_train,\n y_test) = skm.train_test_split(X,\n                                Boston['medv'],\n                                test_size=0.3,\n                                random_state=0)\n\nHaving formed our training and test data sets, we fit the regression tree.\n\nreg = DTR(max_depth=3)\nreg.fit(X_train, y_train)\nax = subplots(figsize=(12,12))[1]\nplot_tree(reg,\n          feature_names=feature_names,\n          ax=ax);\n\n\n\n\n\n\n\n\nThe variable lstat measures the percentage of individuals with lower socioeconomic status. The tree indicates that lower values of lstat correspond to more expensive houses. The tree predicts a median house price of $12,042 for small-sized homes (rm &lt; 6.8), in suburbs in which residents have low socioeconomic status (lstat  &gt; 14.4) and the crime-rate is moderate (crim &gt; 5.8).\nNow we use the cross-validation function to see whether pruning the tree will improve performance.\n\nccp_path = reg.cost_complexity_pruning_path(X_train, y_train)\nkfold = skm.KFold(5,\n                  shuffle=True,\n                  random_state=10)\ngrid = skm.GridSearchCV(reg,\n                        {'ccp_alpha': ccp_path.ccp_alphas},\n                        refit=True,\n                        cv=kfold,\n                        scoring='neg_mean_squared_error')\nG = grid.fit(X_train, y_train)\n\nIn keeping with the cross-validation results, we use the pruned tree to make predictions on the test set.\n\nbest_ = grid.best_estimator_\nnp.mean((y_test - best_.predict(X_test))**2)\n\n28.06985754975404\n\n\nIn other words, the test set MSE associated with the regression tree is 28.07. The square root of the MSE is therefore around 5.30, indicating that this model leads to test predictions that are within around $5300 of the true median home value for the suburb.\nLet’s plot the best tree to see how interpretable it is.\n\nax = subplots(figsize=(12,12))[1]\nplot_tree(G.best_estimator_,\n          feature_names=feature_names,\n          ax=ax);",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees Lab (Python)"
    ]
  },
  {
    "objectID": "labs/Lab-C2.3-Py-ISLch08-baggboost.html#bagging-and-random-forests",
    "href": "labs/Lab-C2.3-Py-ISLch08-baggboost.html#bagging-and-random-forests",
    "title": "Lab: Tree based methods",
    "section": "Bagging and Random Forests",
    "text": "Bagging and Random Forests\nHere we apply bagging and random forests to the Boston data, using the RandomForestRegressor() from the sklearn.ensemble package. Recall that bagging is simply a special case of a random forest with \\(m=p\\). Therefore, the RandomForestRegressor() function can be used to perform both bagging and random forests. We start with bagging.\n\nbag_boston = RF(max_features=X_train.shape[1], random_state=0)\nbag_boston.fit(X_train, y_train)\n\nRandomForestRegressor(max_features=12, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_features=12, random_state=0)\n\n\nThe argument max_features indicates that all 12 predictors should be considered for each split of the tree — in other words, that bagging should be done. How well does this bagged model perform on the test set?\n\nax = subplots(figsize=(8,8))[1]\ny_hat_bag = bag_boston.predict(X_test)\nax.scatter(y_hat_bag, y_test)\nnp.mean((y_test - y_hat_bag)**2)\n\n14.634700151315787\n\n\n\n\n\n\n\n\n\nThe test set MSE associated with the bagged regression tree is 14.63, about half that obtained using an optimally-pruned single tree. We could change the number of trees grown from the default of 100 by using the n_estimators argument:\n\nbag_boston = RF(max_features=X_train.shape[1],\n                n_estimators=500,\n                random_state=0).fit(X_train, y_train)\ny_hat_bag = bag_boston.predict(X_test)\nnp.mean((y_test - y_hat_bag)**2)\n\n14.605662565263161\n\n\nThere is not much change. Bagging and random forests cannot overfit by increasing the number of trees, but can underfit if the number is too small.\nGrowing a random forest proceeds in exactly the same way, except that we use a smaller value of the max_features argument. By default, RandomForestRegressor() uses \\(p\\) variables when building a random forest of regression trees (i.e. it defaults to bagging), and RandomForestClassifier() uses \\(\\sqrt{p}\\) variables when building a random forest of classification trees. Here we use max_features=6.\n\nRF_boston = RF(max_features=6,\n               random_state=0).fit(X_train, y_train)\ny_hat_RF = RF_boston.predict(X_test)\nnp.mean((y_test - y_hat_RF)**2)\n\n20.04276446710527\n\n\nThe test set MSE is 20.04; this indicates that random forests did somewhat worse than bagging in this case. Extracting the feature_importances_ values from the fitted model, we can view the importance of each variable.\n\nfeature_imp = pd.DataFrame(\n    {'importance':RF_boston.feature_importances_},\n    index=feature_names)\nfeature_imp.sort_values(by='importance', ascending=False)\n\n\n\n\n\n\n\n\nimportance\n\n\n\n\nlstat\n0.356203\n\n\nrm\n0.332163\n\n\nptratio\n0.067270\n\n\ncrim\n0.055404\n\n\nindus\n0.053851\n\n\ndis\n0.041582\n\n\nnox\n0.035225\n\n\ntax\n0.025355\n\n\nage\n0.021506\n\n\nrad\n0.004784\n\n\nchas\n0.004203\n\n\nzn\n0.002454\n\n\n\n\n\n\n\nThis is a relative measure of the total decrease in node impurity that results from splits over that variable, averaged over all trees (this was plotted in Figure 8.9 for a model fit to the Heart data).\nThe results indicate that across all of the trees considered in the random forest, the wealth level of the community (lstat) and the house size (rm) are by far the two most important variables.",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees Lab (Python)"
    ]
  },
  {
    "objectID": "labs/Lab-C2.3-Py-ISLch08-baggboost.html#boosting",
    "href": "labs/Lab-C2.3-Py-ISLch08-baggboost.html#boosting",
    "title": "Lab: Tree based methods",
    "section": "Boosting",
    "text": "Boosting\nHere we use GradientBoostingRegressor() from sklearn.ensemble to fit boosted regression trees to the Boston data set. For classification we would use GradientBoostingClassifier(). The argument n_estimators=5000 indicates that we want 5000 trees, and the option max_depth=3 limits the depth of each tree. The argument learning_rate is the \\(\\lambda\\) mentioned earlier in the description of boosting.\n\nboost_boston = GBR(n_estimators=5000,\n                   learning_rate=0.001,\n                   max_depth=3,\n                   random_state=0)\nboost_boston.fit(X_train, y_train)\n\nGradientBoostingRegressor(learning_rate=0.001, n_estimators=5000,\n                          random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingRegressorGradientBoostingRegressor(learning_rate=0.001, n_estimators=5000,\n                          random_state=0)\n\n\nWe can see how the training error decreases with the train_score_ attribute. To get an idea of how the test error decreases we can use the staged_predict() method to get the predicted values along the path.\n\ntest_error = np.zeros_like(boost_boston.train_score_)\nfor idx, y_ in enumerate(boost_boston.staged_predict(X_test)):\n   test_error[idx] = np.mean((y_test - y_)**2)\n\nplot_idx = np.arange(boost_boston.train_score_.shape[0])\nax = subplots(figsize=(8,8))[1]\nax.plot(plot_idx,\n        boost_boston.train_score_,\n        'b',\n        label='Training')\nax.plot(plot_idx,\n        test_error,\n        'r',\n        label='Test')\nax.legend();\n\n\n\n\n\n\n\n\nWe now use the boosted model to predict medv on the test set:\n\ny_hat_boost = boost_boston.predict(X_test);\nnp.mean((y_test - y_hat_boost)**2)\n\n14.481405918831591\n\n\nThe test MSE obtained is 14.48, similar to the test MSE for bagging. If we want to, we can perform boosting with a different value of the shrinkage parameter \\(\\lambda\\) in (8.10). The default value is 0.001, but this is easily modified. Here we take \\(\\lambda=0.2\\).\n\nboost_boston = GBR(n_estimators=5000,\n                   learning_rate=0.2,\n                   max_depth=3,\n                   random_state=0)\nboost_boston.fit(X_train,\n                 y_train)\ny_hat_boost = boost_boston.predict(X_test);\nnp.mean((y_test - y_hat_boost)**2)\n\n14.501514553719565\n\n\nIn this case, using \\(\\lambda=0.2\\) leads to a almost the same test MSE as when using \\(\\lambda=0.001\\).",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees Lab (Python)"
    ]
  },
  {
    "objectID": "labs/Lab-C2.3-Py-ISLch08-baggboost.html#bayesian-additive-regression-trees",
    "href": "labs/Lab-C2.3-Py-ISLch08-baggboost.html#bayesian-additive-regression-trees",
    "title": "Lab: Tree based methods",
    "section": "Bayesian Additive Regression Trees",
    "text": "Bayesian Additive Regression Trees\nIn this section we demonstrate a Python implementation of BART found in the ISLP.bart package. We fit a model to the Boston housing data set. This BART() estimator is designed for quantitative outcome variables, though other implementations are available for fitting logistic and probit models to categorical outcomes.\n\nbart_boston = BART(random_state=0, burnin=5, ndraw=15)\nbart_boston.fit(X_train, y_train)\n\nBART(burnin=5, ndraw=15, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BARTBART(burnin=5, ndraw=15, random_state=0)\n\n\nOn this data set, with this split into test and training, we see that the test error of BART is similar to that of random forest.\n\nyhat_test = bart_boston.predict(X_test.astype(np.float32))\nnp.mean((y_test - yhat_test)**2)\n\n22.145009458109232\n\n\nWe can check how many times each variable appeared in the collection of trees. This gives a summary similar to the variable importance plot for boosting and random forests.\n\nvar_inclusion = pd.Series(bart_boston.variable_inclusion_.mean(0),\n                               index=D.columns)\nvar_inclusion\n\ncrim       26.933333\nzn         27.866667\nindus      26.466667\nchas       22.466667\nnox        26.600000\nrm         29.800000\nage        22.733333\ndis        26.466667\nrad        23.666667\ntax        24.133333\nptratio    24.266667\nlstat      31.000000\ndtype: float64",
    "crumbs": [
      "Home",
      "Tree-based Methods",
      "Decision Trees Lab (Python)"
    ]
  }
]