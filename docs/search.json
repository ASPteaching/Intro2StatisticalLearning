[
  {
    "objectID": "labs/The-caret_package.html",
    "href": "labs/The-caret_package.html",
    "title": "The caret package",
    "section": "",
    "text": "options(width=100) \nif(!require(\"knitr\")) install.packages(\"knitr\")\nlibrary(\"knitr\")\n#getOption(\"width\")\nknitr::opts_chunk$set(comment=NA,echo = TRUE, cache=TRUE)"
  },
  {
    "objectID": "labs/The-caret_package.html#learning-to-use-caret",
    "href": "labs/The-caret_package.html#learning-to-use-caret",
    "title": "The caret package",
    "section": "Learning to use caret",
    "text": "Learning to use caret\nThere are multiple resources to learn caretthat go from simple tutorials like this one or similars to courses, papers and a book by Max Kuhn, the creator or the package."
  },
  {
    "objectID": "labs/The-caret_package.html#data-loading",
    "href": "labs/The-caret_package.html#data-loading",
    "title": "The caret package",
    "section": "Data loading",
    "text": "Data loading\n\nlibrary(\"mlbench\")\ndata(Sonar)\nnames(Sonar)\n\n [1] \"V1\"    \"V2\"    \"V3\"    \"V4\"    \"V5\"    \"V6\"    \"V7\"    \"V8\"    \"V9\"    \"V10\"   \"V11\"   \"V12\"  \n[13] \"V13\"   \"V14\"   \"V15\"   \"V16\"   \"V17\"   \"V18\"   \"V19\"   \"V20\"   \"V21\"   \"V22\"   \"V23\"   \"V24\"  \n[25] \"V25\"   \"V26\"   \"V27\"   \"V28\"   \"V29\"   \"V30\"   \"V31\"   \"V32\"   \"V33\"   \"V34\"   \"V35\"   \"V36\"  \n[37] \"V37\"   \"V38\"   \"V39\"   \"V40\"   \"V41\"   \"V42\"   \"V43\"   \"V44\"   \"V45\"   \"V46\"   \"V47\"   \"V48\"  \n[49] \"V49\"   \"V50\"   \"V51\"   \"V52\"   \"V53\"   \"V54\"   \"V55\"   \"V56\"   \"V57\"   \"V58\"   \"V59\"   \"V60\"  \n[61] \"Class\"\n\n\nThe sonarpackage has 208 data points collected on 60 predictors (energy within a particular frequency band)."
  },
  {
    "objectID": "labs/The-caret_package.html#traintest-splitting",
    "href": "labs/The-caret_package.html#traintest-splitting",
    "title": "The caret package",
    "section": "Train/test splitting",
    "text": "Train/test splitting\nWe will most of the time want to split the data into two groups: a training set and a test set.\nThis may be done with the createDataPartition function:\n\nset.seed(1234) # Control of data generation\ninTrain &lt;- createDataPartition(y=Sonar$Class, p=.75, list=FALSE)\nstr(inTrain)\n\n int [1:157, 1] 2 3 4 6 7 8 9 11 14 15 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr \"Resample1\"\n\ntraining &lt;- Sonar[inTrain,]\ntesting &lt;- Sonar[-inTrain,]\nnrow(training)\n\n[1] 157\n\n\nOthers similar functions are: createFolds and createResample,"
  },
  {
    "objectID": "labs/The-caret_package.html#preprocessing-and-training",
    "href": "labs/The-caret_package.html#preprocessing-and-training",
    "title": "The caret package",
    "section": "Preprocessing and training",
    "text": "Preprocessing and training\nUsually, before prediction, data may have to be cleaned and pre-processed.\nCaret allows to integrate it with the training step using the train function.\nThis function has multiple parameter such as:\n\nmethod: Can choose from more than 200 models\npreprocess: all type of filtering and transformations\n\n\nCART1Model &lt;- train (Class ~ ., \n                   data=training, \n                   method=\"rpart1SE\",\n                   preProc=c(\"center\",\"scale\"))\nCART1Model\n\nCART \n\n157 samples\n 60 predictor\n  2 classes: 'M', 'R' \n\nPre-processing: centered (60), scaled (60) \nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 157, 157, 157, 157, 157, 157, ... \nResampling results:\n\n  Accuracy   Kappa   \n  0.6752493  0.350363\n\n\n\nRefining specifications\nMany specifications can be passed using the trainControl instruction.\n\nctrl &lt;- trainControl(method = \"repeatedcv\", repeats=3)\nCART1Model3x10cv &lt;- train (Class ~ ., \n                         data=training, \n                         method=\"rpart1SE\",\n                         trControl=ctrl,\n                         preProc=c(\"center\",\"scale\"))\n\nCART1Model3x10cv\n\nCART \n\n157 samples\n 60 predictor\n  2 classes: 'M', 'R' \n\nPre-processing: centered (60), scaled (60) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 141, 142, 142, 141, 141, 142, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.7087173  0.4168066\n\n\nWe can change the method used by changing the trainControl parameter.\nIn the example below we fit a classification tree with different options:\n\nctrl &lt;- trainControl(method = \"repeatedcv\", repeats=3,\n                     classProbs=TRUE,\n                     summaryFunction=twoClassSummary)\n\nCART1Model3x10cv &lt;- train (Class ~ ., \n                         data=training, \n                         method=\"rpart1SE\", \n                         trControl=ctrl, \n                         metric=\"ROC\", \n                         preProc=c(\"center\",\"scale\"))\n\nCART1Model3x10cv\n\nCART \n\n157 samples\n 60 predictor\n  2 classes: 'M', 'R' \n\nPre-processing: centered (60), scaled (60) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 141, 141, 142, 141, 141, 142, ... \nResampling results:\n\n  ROC        Sens   Spec     \n  0.7757068  0.775  0.6869048\n\n\n\nCART2Fit3x10cv &lt;- train (Class ~ ., \n                       data=training, \n                       method=\"rpart\", \n                       trControl=ctrl, \n                       metric=\"ROC\", \n                       preProc=c(\"center\",\"scale\"))\nCART2Fit3x10cv\n\nCART \n\n157 samples\n 60 predictor\n  2 classes: 'M', 'R' \n\nPre-processing: centered (60), scaled (60) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 142, 142, 142, 142, 142, 140, ... \nResampling results across tuning parameters:\n\n  cp          ROC        Sens       Spec     \n  0.06849315  0.7033441  0.6851852  0.6779762\n  0.10958904  0.6829282  0.7523148  0.5922619\n  0.47945205  0.5517196  0.8629630  0.2404762\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.06849315.\n\nplot(CART2Fit3x10cv)\n\n\n\n\n\n\n\n\n\nCART2Fit3x10cv &lt;- train (Class ~ ., \n                       data=training, \n                       method=\"rpart\", \n                       trControl=ctrl, \n                       metric=\"ROC\",  \n                       tuneLength=10,\n                       preProc=c(\"center\",\"scale\"))\nCART2Fit3x10cv\n\nCART \n\n157 samples\n 60 predictor\n  2 classes: 'M', 'R' \n\nPre-processing: centered (60), scaled (60) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 141, 142, 140, 141, 141, 142, ... \nResampling results across tuning parameters:\n\n  cp          ROC        Sens       Spec     \n  0.00000000  0.7375744  0.7305556  0.6220238\n  0.05327245  0.7382523  0.7453704  0.6130952\n  0.10654490  0.6816468  0.7773148  0.5696429\n  0.15981735  0.6787368  0.8092593  0.5482143\n  0.21308980  0.6787368  0.8092593  0.5482143\n  0.26636225  0.6787368  0.8092593  0.5482143\n  0.31963470  0.6787368  0.8092593  0.5482143\n  0.37290715  0.6787368  0.8092593  0.5482143\n  0.42617960  0.6787368  0.8092593  0.5482143\n  0.47945205  0.5748016  0.8680556  0.2815476\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.05327245.\n\nplot(CART2Fit3x10cv)"
  },
  {
    "objectID": "labs/The-caret_package.html#predict-confusionmatrix-functions",
    "href": "labs/The-caret_package.html#predict-confusionmatrix-functions",
    "title": "The caret package",
    "section": "Predict & confusionMatrix functions",
    "text": "Predict & confusionMatrix functions\nTo predict new samples can be used predict function.\n\ntype = prob : to compute class probabilities\ntype = raw : to predict the class\n\nThe confusionMatrix function will compute the confusion matrix and associated statistics for the model fit.\n\nCART2Probs &lt;- predict(CART2Fit3x10cv, newdata = testing, type = \"prob\")\nCART2Classes &lt;- predict(CART2Fit3x10cv, newdata = testing, type = \"raw\")\nconfusionMatrix(data=CART2Classes,testing$Class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  M  R\n         M 21  5\n         R  6 19\n                                          \n               Accuracy : 0.7843          \n                 95% CI : (0.6468, 0.8871)\n    No Information Rate : 0.5294          \n    P-Value [Acc &gt; NIR] : 0.0001502       \n                                          \n                  Kappa : 0.5681          \n                                          \n Mcnemar's Test P-Value : 1.0000000       \n                                          \n            Sensitivity : 0.7778          \n            Specificity : 0.7917          \n         Pos Pred Value : 0.8077          \n         Neg Pred Value : 0.7600          \n             Prevalence : 0.5294          \n         Detection Rate : 0.4118          \n   Detection Prevalence : 0.5098          \n      Balanced Accuracy : 0.7847          \n                                          \n       'Positive' Class : M"
  },
  {
    "objectID": "labs/The-caret_package.html#model-comparison",
    "href": "labs/The-caret_package.html#model-comparison",
    "title": "The caret package",
    "section": "Model comparison",
    "text": "Model comparison\nThe resamplesfunction enable smodel comparison\n\nresamps=resamples(list(CART2=CART2Fit3x10cv,\n                       CART1=CART1Model3x10cv))\nsummary(resamps)\n\n\nCall:\nsummary.resamples(object = resamps)\n\nModels: CART2, CART1 \nNumber of resamples: 30 \n\nROC \n           Min.   1st Qu.    Median      Mean   3rd Qu.     Max. NA's\nCART2 0.5000000 0.6294643 0.7455357 0.7382523 0.8058036 0.952381    0\nCART1 0.5535714 0.7249504 0.7926587 0.7757068 0.8315972 0.937500    0\n\nSens \n           Min.   1st Qu.    Median      Mean 3rd Qu. Max. NA's\nCART2 0.4444444 0.6250000 0.7500000 0.7453704   0.875    1    0\nCART1 0.4444444 0.6666667 0.7777778 0.7750000   0.875    1    0\n\nSpec \n       Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nCART2 0.250 0.5714286 0.6250000 0.6130952 0.7142857 0.8750000    0\nCART1 0.375 0.5714286 0.7142857 0.6869048 0.8571429 0.8571429    0\n\nxyplot(resamps,what=\"BlandAltman\")\n\n\n\n\n\n\n\ndiffs&lt;-diff(resamps)\nsummary(diffs)\n\n\nCall:\nsummary.diff.resamples(object = diffs)\n\np-value adjustment: bonferroni \nUpper diagonal: estimates of the difference\nLower diagonal: p-value for H0: difference = 0\n\nROC \n      CART2  CART1   \nCART2        -0.03745\nCART1 0.1598         \n\nSens \n      CART2  CART1   \nCART2        -0.02963\nCART1 0.4514         \n\nSpec \n      CART2   CART1   \nCART2         -0.07381\nCART1 0.02404"
  },
  {
    "objectID": "labs/The-caret_package.html#adaboost",
    "href": "labs/The-caret_package.html#adaboost",
    "title": "The caret package",
    "section": "Adaboost",
    "text": "Adaboost\nIn this example, we are using the rpart algorithm as the base learner for AdaBoost. We can then use the predict function to make predictions on new data:\n\nlibrary(caret)\nlibrary(mlbench)\n\ndata(BreastCancer)\n\n# Split the data into training and testing sets\nset.seed(123)\ntrainIndex &lt;- createDataPartition(BreastCancer$Class, p = 0.7, list = FALSE)\ntraining &lt;- BreastCancer[trainIndex, ]\ntesting &lt;- BreastCancer[-trainIndex, ]\n\n# Next, set up \n# - the training control and \n# - tuning parameters for the AdaBoost algorithm:\n\nctrl &lt;- trainControl(method = \"repeatedcv\", \n                     number = 10, repeats = 3,\n                     classProbs = TRUE, \n                     summaryFunction = twoClassSummary)\n\nparams &lt;- data.frame(method = \"AdaBoost\", \n                     nIter = 100, \n                     interaction.depth = 1, \n                     shrinkage = 0.1)\n\n#  we are using 10-fold cross-validation with 3 repeats and the twoClassSummary function for evaluation. \n# We are also setting the number of iterations for the AdaBoost algorithm to 100, the maximum interaction depth to 1, and the shrinkage factor to 0.1.\n\n# Use the train function to train the AdaBoost algorithm on the training data and evaluate its performance on the testing data:\n\nadaboost &lt;- train(Class ~ ., data = training, \n                  method = \"rpart\", \n                  trControl = ctrl, \n                  tuneGrid = params)\n\npredictions &lt;- predict(adaboost, newdata = testing)\n\n# Evaluate the performance of the model\nconfusionMatrix(predictions, testData$diagnosis)"
  },
  {
    "objectID": "labs/The-caret_package.html#gradient-boosting",
    "href": "labs/The-caret_package.html#gradient-boosting",
    "title": "The caret package",
    "section": "Gradient boosting",
    "text": "Gradient boosting\nWe use the gbm method in train() function from the caret package to build a Gradient Boosting model on the Breast Cancer dataset.\n\nlibrary(caret)\nlibrary(gbm)\ndata(BreastCancer)\n\n# Convert the diagnosis column to a binary factor\nBreastCancer$diagnosis &lt;- ifelse(BreastCancer$diagnosis == \"M\", 1, 0)\n\n# Split the dataset into training and testing sets\ntrainIndex &lt;- createDataPartition(BreastCancer$diagnosis, p = 0.7, list = FALSE)\ntrainData &lt;- BreastCancer[trainIndex, ]\ntestData &lt;- BreastCancer[-trainIndex, ]\n\n# Define the training control\nctrl &lt;- trainControl(method = \"cv\", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)\n\n# Define the Gradient Boosting model\nmodel &lt;- train(diagnosis ~ ., data = trainData, method = \"gbm\", trControl = ctrl,\n               verbose = FALSE, metric = \"ROC\", n.trees = 1000, interaction.depth = 3, shrinkage = 0.01)\n\n# Make predictions on the testing set\npredictions &lt;- predict(model, testData)\n\n# Evaluate the performance of the model\nconfusionMatrix(predictions, testData$diagnosis)"
  },
  {
    "objectID": "labs/The-caret_package.html#xgboost",
    "href": "labs/The-caret_package.html#xgboost",
    "title": "The caret package",
    "section": "XGBoost",
    "text": "XGBoost\n\nIn this example, we use the xgbTree method in train() function from the caret package to build an XGBoost model on the BreastCancer dataset.\nThe hyperparameters are set to default values, except for parameters:\n\nnrounds,\nmax_depth,\neta, lambda, and\nalpha\n\nThe final performance is evaluated using a confusion matrix.\n\n\nlibrary(caret)\nlibrary(xgboost)\ndata(BreastCancer)\n\n# Convert the diagnosis column to a binary factor\nBreastCancer$diagnosis &lt;- ifelse(BreastCancer$diagnosis == \"M\", 1, 0)\n\n# Split the dataset into training and testing sets\ntrainIndex &lt;- createDataPartition(BreastCancer$diagnosis, p = 0.7, list = FALSE)\ntrainData &lt;- BreastCancer[trainIndex, ]\ntestData &lt;- BreastCancer[-trainIndex, ]\n\n# Define the training control\nctrl &lt;- trainControl(method = \"cv\", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)\n\n# Define the XGBoost model\nmodel &lt;- train(diagnosis ~ ., \n               data = trainData, \n               method = \"xgbTree\", trControl = ctrl,\n               verbose = FALSE, metric = \"ROC\", \n               nrounds = 1000, max_depth = 3, \n               eta = 0.01, lambda = 1, alpha = 0)\n\n# Make predictions on the testing set\npredictions &lt;- predict(model, testData)\n\n# Evaluate the performance of the model\nconfusionMatrix(predictions, testData$diagnosis)"
  },
  {
    "objectID": "labs/The-caret_package.html#references",
    "href": "labs/The-caret_package.html#references",
    "title": "The caret package",
    "section": "References",
    "text": "References\n\nOfficial references and resources\n\nCaret tutorial at UseR! 2014\nThe caret package\nJSS Paper\nApplied Predictive Modeling Blog\nCaret cheatsheet in Rstudio cheatsheet page\n\n\n\nOther resources\n\nCaret Package – A Practical Guide to Machine Learning in R -Create predictive models in R with Caret\nCaret R Package for Applied Predictive Modeling"
  },
  {
    "objectID": "labs/Lab-C2.3-Py-ISLch08-baggboost-lab.html",
    "href": "labs/Lab-C2.3-Py-ISLch08-baggboost-lab.html",
    "title": "Lab: Tree based methods",
    "section": "",
    "text": "This lab presents some examples of applying shallow and ensemble-based trees.\nIt is adapted directly from lab 8 in “Introduction to Statistical Learning with Python examples”, 2nd edition, and it can be run in parallel in Python or in R using the respective notebooks for each language downloadable from the book’s website."
  },
  {
    "objectID": "labs/Lab-C2.3-Py-ISLch08-baggboost-lab.html#introduction",
    "href": "labs/Lab-C2.3-Py-ISLch08-baggboost-lab.html#introduction",
    "title": "Lab: Tree based methods",
    "section": "",
    "text": "This lab presents some examples of applying shallow and ensemble-based trees.\nIt is adapted directly from lab 8 in “Introduction to Statistical Learning with Python examples”, 2nd edition, and it can be run in parallel in Python or in R using the respective notebooks for each language downloadable from the book’s website."
  },
  {
    "objectID": "labs/Lab-C2.3-Py-ISLch08-baggboost-lab.html#install-load-required-libraries",
    "href": "labs/Lab-C2.3-Py-ISLch08-baggboost-lab.html#install-load-required-libraries",
    "title": "Lab: Tree based methods",
    "section": "Install / load required libraries",
    "text": "Install / load required libraries\nWe import some of our usual libraries at this top level.\nIt is assumed that all required libraries have been installed. If this is not the case any library (e.g.ISLP) can be installed with the following code:\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.pyplot import subplots\nimport sklearn.model_selection as skm\nfrom ISLP import load_data, confusion_table \nfrom ISLP.models import ModelSpec as MS\n\nWe also collect the new imports needed for this lab.\n\nfrom sklearn.tree import (DecisionTreeClassifier as DTC,\n                          DecisionTreeRegressor as DTR,\n                          plot_tree,\n                          export_text)\nfrom sklearn.metrics import (accuracy_score,\n                             log_loss)\nfrom sklearn.ensemble import \\\n     (RandomForestRegressor as RF,\n      GradientBoostingRegressor as GBR)\nfrom ISLP.bart import BART"
  },
  {
    "objectID": "labs/Lab-C2.3-Py-ISLch08-baggboost-lab.html#fitting-classification-trees",
    "href": "labs/Lab-C2.3-Py-ISLch08-baggboost-lab.html#fitting-classification-trees",
    "title": "Lab: Tree based methods",
    "section": "Fitting Classification Trees",
    "text": "Fitting Classification Trees\nThe first example illustrates how to build classification and regression trees.\nWe first use classification trees to analyze the Carseats data set. In these data, Sales is a continuous variable, and so we begin by recoding it as a binary variable. We use the where() function to create a variable, called High, which takes on a value of Yes if the Sales variable exceeds 8, and takes on a value of No otherwise.\n\nCarseats = load_data('Carseats')\nHigh = np.where(Carseats.Sales &gt; 8,\n                \"Yes\",\n                \"No\")\n\nWe now use DecisionTreeClassifier() to fit a classification tree in order to predict High using all variables but Sales. To do so, we must form a model matrix as we did when fitting regression models.\n\nmodel = MS(Carseats.columns.drop('Sales'), intercept=False)\nD = model.fit_transform(Carseats)\nfeature_names = list(D.columns)\nX = np.asarray(D)\n\nWe have converted D from a data frame to an array X, which is needed in some of the analysis below. We also need the feature_names for annotating our plots later.\nThere are several options needed to specify the classifier, such as max_depth (how deep to grow the tree), min_samples_split (minimum number of observations in a node to be eligible for splitting) and criterion (whether to use Gini or cross-entropy as the split criterion). We also set random_state for reproducibility; ties in the split criterion are broken at random.\n\nclf = DTC(criterion='entropy',\n          max_depth=3,\n          random_state=0)        \nclf.fit(X, High)\n\nDecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\n\n\nIn our discussion of qualitative features in Section 3.3, we noted that for a linear regression model such a feature could be represented by including a matrix of dummy variables (one-hot-encoding) in the model matrix, using the formula notation of statsmodels. As mentioned in Section 8.1, there is a more natural way to handle qualitative features when building a decision tree, that does not require such dummy variables; each split amounts to partitioning the levels into two groups. However, the sklearn implementation of decision trees does not take advantage of this approach; instead it simply treats the one-hot-encoded levels as separate variables.\n\nacs = accuracy_score(High, clf.predict(X))\nacs\n\n0.79\n\n\nWith only the default arguments, the training error rate is 21%. For classification trees, we can access the value of the deviance using log_loss(), \\[\\begin{equation*}\n\\begin{split}\n-2 \\sum_m \\sum_k n_{mk} \\log \\hat{p}_{mk},\n\\end{split}\n\\end{equation*}\\] where \\(n_{mk}\\) is the number of observations in the \\(m\\)th terminal node that belong to the \\(k\\)th class.\n\nresid_dev = np.sum(log_loss(High, clf.predict_proba(X)))\nresid_dev\n\n0.4710647062649358\n\n\nThis is closely related to the entropy, defined in (8.7). A small deviance indicates a tree that provides a good fit to the (training) data.\nOne of the most attractive properties of trees is that they can be graphically displayed. Here we use the plot() function to display the tree structure.\n\nax = subplots(figsize=(12,12))[1]\nplot_tree(clf,\n          feature_names=feature_names,\n          ax=ax);\n\n\n\n\n\n\n\n\nThe most important indicator of Sales appears to be ShelveLoc.\nWe can see a text representation of the tree using export_text(), which displays the split criterion (e.g. Price &lt;= 92.5) for each branch. For leaf nodes it shows the overall prediction\n(Yes or No). We can also see the number of observations in that leaf that take on values of Yes and No by specifying show_weights=True.\n\nprint(export_text(clf,\n                  feature_names=feature_names,\n                  show_weights=True))\n\n|--- ShelveLoc[Good] &lt;= 0.50\n|   |--- Price &lt;= 92.50\n|   |   |--- Income &lt;= 57.00\n|   |   |   |--- weights: [7.00, 3.00] class: No\n|   |   |--- Income &gt;  57.00\n|   |   |   |--- weights: [7.00, 29.00] class: Yes\n|   |--- Price &gt;  92.50\n|   |   |--- Advertising &lt;= 13.50\n|   |   |   |--- weights: [183.00, 41.00] class: No\n|   |   |--- Advertising &gt;  13.50\n|   |   |   |--- weights: [20.00, 25.00] class: Yes\n|--- ShelveLoc[Good] &gt;  0.50\n|   |--- Price &lt;= 135.00\n|   |   |--- US[Yes] &lt;= 0.50\n|   |   |   |--- weights: [6.00, 11.00] class: Yes\n|   |   |--- US[Yes] &gt;  0.50\n|   |   |   |--- weights: [2.00, 49.00] class: Yes\n|   |--- Price &gt;  135.00\n|   |   |--- Income &lt;= 46.00\n|   |   |   |--- weights: [6.00, 0.00] class: No\n|   |   |--- Income &gt;  46.00\n|   |   |   |--- weights: [5.00, 6.00] class: Yes\n\n\n\nIn order to properly evaluate the performance of a classification tree on these data, we must estimate the test error rather than simply computing the training error. We split the observations into a training set and a test set, build the tree using the training set, and evaluate its performance on the test data. This pattern is similar to that in Chapter 6, with the linear models replaced here by decision trees — the code for validation is almost identical. This approach leads to correct predictions for 68.5% of the locations in the test data set.\n\nvalidation = skm.ShuffleSplit(n_splits=1,\n                              test_size=200,\n                              random_state=0)\nresults = skm.cross_validate(clf,\n                             D,\n                             High,\n                             cv=validation)\nresults['test_score']\n\narray([0.685])\n\n\nNext, we consider whether pruning the tree might lead to improved classification performance. We first split the data into a training and test set. We will use cross-validation to prune the tree on the training set, and then evaluate the performance of the pruned tree on the test set.\n\n(X_train,\n X_test,\n High_train,\n High_test) = skm.train_test_split(X,\n                                   High,\n                                   test_size=0.5,\n                                   random_state=0)\n                                   \n\nWe first refit the full tree on the training set; here we do not set a max_depth parameter, since we will learn that through cross-validation.\n\nclf = DTC(criterion='entropy', random_state=0)\nclf.fit(X_train, High_train)\naccuracy_score(High_test, clf.predict(X_test))\n\n0.735\n\n\nNext we use the cost_complexity_pruning_path() method of clf to extract cost-complexity values.\n\nccp_path = clf.cost_complexity_pruning_path(X_train, High_train)\nkfold = skm.KFold(10,\n                  random_state=1,\n                  shuffle=True)\n\nThis yields a set of impurities and \\(\\alpha\\) values from which we can extract an optimal one by cross-validation.\n\ngrid = skm.GridSearchCV(clf,\n                        {'ccp_alpha': ccp_path.ccp_alphas},\n                        refit=True,\n                        cv=kfold,\n                        scoring='accuracy')\ngrid.fit(X_train, High_train)\ngrid.best_score_\n\n0.685\n\n\nLet’s take a look at the pruned true.\n\nax = subplots(figsize=(12, 12))[1]\nbest_ = grid.best_estimator_\nplot_tree(best_,\n          feature_names=feature_names,\n          ax=ax);\n\n\n\n\n\n\n\n\nThis is quite a bushy tree. We could count the leaves, or query best_ instead.\n\nbest_.tree_.n_leaves\n\n30\n\n\nThe tree with 30 terminal nodes results in the lowest cross-validation error rate, with an accuracy of 68.5%. How well does this pruned tree perform on the test data set? Once again, we apply the predict() function.\n\nprint(accuracy_score(High_test,\n                     best_.predict(X_test)))\nconfusion = confusion_table(best_.predict(X_test),\n                            High_test)\nconfusion\n\n0.72\n\n\n\n\n\n\n\n\nTruth\nNo\nYes\n\n\nPredicted\n\n\n\n\n\n\nNo\n94\n32\n\n\nYes\n24\n50\n\n\n\n\n\n\n\nNow 72.0% of the test observations are correctly classified, which is slightly worse than the error for the full tree (with 35 leaves). So cross-validation has not helped us much here; it only pruned off 5 leaves, at a cost of a slightly worse error. These results would change if we were to change the random number seeds above; even though cross-validation gives an unbiased approach to model selection, it does have variance."
  },
  {
    "objectID": "labs/Lab-C2.3-Py-ISLch08-baggboost-lab.html#fitting-regression-trees",
    "href": "labs/Lab-C2.3-Py-ISLch08-baggboost-lab.html#fitting-regression-trees",
    "title": "Lab: Tree based methods",
    "section": "Fitting Regression Trees",
    "text": "Fitting Regression Trees\nHere we fit a regression tree to the Boston data set. The steps are similar to those for classification trees.\n\nBoston = load_data(\"Boston\")\nmodel = MS(Boston.columns.drop('medv'), intercept=False)\nD = model.fit_transform(Boston)\nfeature_names = list(D.columns)\nX = np.asarray(D)\n\nFirst, we split the data into training and test sets, and fit the tree to the training data. Here we use 30% of the data for the test set.\n\n(X_train,\n X_test,\n y_train,\n y_test) = skm.train_test_split(X,\n                                Boston['medv'],\n                                test_size=0.3,\n                                random_state=0)\n\nHaving formed our training and test data sets, we fit the regression tree.\n\nreg = DTR(max_depth=3)\nreg.fit(X_train, y_train)\nax = subplots(figsize=(12,12))[1]\nplot_tree(reg,\n          feature_names=feature_names,\n          ax=ax);\n\n\n\n\n\n\n\n\nThe variable lstat measures the percentage of individuals with lower socioeconomic status. The tree indicates that lower values of lstat correspond to more expensive houses. The tree predicts a median house price of $12,042 for small-sized homes (rm &lt; 6.8), in suburbs in which residents have low socioeconomic status (lstat  &gt; 14.4) and the crime-rate is moderate (crim &gt; 5.8).\nNow we use the cross-validation function to see whether pruning the tree will improve performance.\n\nccp_path = reg.cost_complexity_pruning_path(X_train, y_train)\nkfold = skm.KFold(5,\n                  shuffle=True,\n                  random_state=10)\ngrid = skm.GridSearchCV(reg,\n                        {'ccp_alpha': ccp_path.ccp_alphas},\n                        refit=True,\n                        cv=kfold,\n                        scoring='neg_mean_squared_error')\nG = grid.fit(X_train, y_train)\n\nIn keeping with the cross-validation results, we use the pruned tree to make predictions on the test set.\n\nbest_ = grid.best_estimator_\nnp.mean((y_test - best_.predict(X_test))**2)\n\n28.06985754975404\n\n\nIn other words, the test set MSE associated with the regression tree is 28.07. The square root of the MSE is therefore around 5.30, indicating that this model leads to test predictions that are within around $5300 of the true median home value for the suburb.\nLet’s plot the best tree to see how interpretable it is.\n\nax = subplots(figsize=(12,12))[1]\nplot_tree(G.best_estimator_,\n          feature_names=feature_names,\n          ax=ax);"
  },
  {
    "objectID": "labs/Lab-C2.3-Py-ISLch08-baggboost-lab.html#bagging-and-random-forests",
    "href": "labs/Lab-C2.3-Py-ISLch08-baggboost-lab.html#bagging-and-random-forests",
    "title": "Lab: Tree based methods",
    "section": "Bagging and Random Forests",
    "text": "Bagging and Random Forests\nHere we apply bagging and random forests to the Boston data, using the RandomForestRegressor() from the sklearn.ensemble package. Recall that bagging is simply a special case of a random forest with \\(m=p\\). Therefore, the RandomForestRegressor() function can be used to perform both bagging and random forests. We start with bagging.\n\nbag_boston = RF(max_features=X_train.shape[1], random_state=0)\nbag_boston.fit(X_train, y_train)\n\nRandomForestRegressor(max_features=12, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_features=12, random_state=0)\n\n\nThe argument max_features indicates that all 12 predictors should be considered for each split of the tree — in other words, that bagging should be done. How well does this bagged model perform on the test set?\n\nax = subplots(figsize=(8,8))[1]\ny_hat_bag = bag_boston.predict(X_test)\nax.scatter(y_hat_bag, y_test)\nnp.mean((y_test - y_hat_bag)**2)\n\n14.634700151315787\n\n\n\n\n\n\n\n\n\nThe test set MSE associated with the bagged regression tree is 14.63, about half that obtained using an optimally-pruned single tree. We could change the number of trees grown from the default of 100 by using the n_estimators argument:\n\nbag_boston = RF(max_features=X_train.shape[1],\n                n_estimators=500,\n                random_state=0).fit(X_train, y_train)\ny_hat_bag = bag_boston.predict(X_test)\nnp.mean((y_test - y_hat_bag)**2)\n\n14.605662565263161\n\n\nThere is not much change. Bagging and random forests cannot overfit by increasing the number of trees, but can underfit if the number is too small.\nGrowing a random forest proceeds in exactly the same way, except that we use a smaller value of the max_features argument. By default, RandomForestRegressor() uses \\(p\\) variables when building a random forest of regression trees (i.e. it defaults to bagging), and RandomForestClassifier() uses \\(\\sqrt{p}\\) variables when building a random forest of classification trees. Here we use max_features=6.\n\nRF_boston = RF(max_features=6,\n               random_state=0).fit(X_train, y_train)\ny_hat_RF = RF_boston.predict(X_test)\nnp.mean((y_test - y_hat_RF)**2)\n\n20.04276446710527\n\n\nThe test set MSE is 20.04; this indicates that random forests did somewhat worse than bagging in this case. Extracting the feature_importances_ values from the fitted model, we can view the importance of each variable.\n\nfeature_imp = pd.DataFrame(\n    {'importance':RF_boston.feature_importances_},\n    index=feature_names)\nfeature_imp.sort_values(by='importance', ascending=False)\n\n\n\n\n\n\n\n\nimportance\n\n\n\n\nlstat\n0.356203\n\n\nrm\n0.332163\n\n\nptratio\n0.067270\n\n\ncrim\n0.055404\n\n\nindus\n0.053851\n\n\ndis\n0.041582\n\n\nnox\n0.035225\n\n\ntax\n0.025355\n\n\nage\n0.021506\n\n\nrad\n0.004784\n\n\nchas\n0.004203\n\n\nzn\n0.002454\n\n\n\n\n\n\n\nThis is a relative measure of the total decrease in node impurity that results from splits over that variable, averaged over all trees (this was plotted in Figure 8.9 for a model fit to the Heart data).\nThe results indicate that across all of the trees considered in the random forest, the wealth level of the community (lstat) and the house size (rm) are by far the two most important variables."
  },
  {
    "objectID": "labs/Lab-C2.3-Py-ISLch08-baggboost-lab.html#boosting",
    "href": "labs/Lab-C2.3-Py-ISLch08-baggboost-lab.html#boosting",
    "title": "Lab: Tree based methods",
    "section": "Boosting",
    "text": "Boosting\nHere we use GradientBoostingRegressor() from sklearn.ensemble to fit boosted regression trees to the Boston data set. For classification we would use GradientBoostingClassifier(). The argument n_estimators=5000 indicates that we want 5000 trees, and the option max_depth=3 limits the depth of each tree. The argument learning_rate is the \\(\\lambda\\) mentioned earlier in the description of boosting.\n\nboost_boston = GBR(n_estimators=5000,\n                   learning_rate=0.001,\n                   max_depth=3,\n                   random_state=0)\nboost_boston.fit(X_train, y_train)\n\nGradientBoostingRegressor(learning_rate=0.001, n_estimators=5000,\n                          random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingRegressorGradientBoostingRegressor(learning_rate=0.001, n_estimators=5000,\n                          random_state=0)\n\n\nWe can see how the training error decreases with the train_score_ attribute. To get an idea of how the test error decreases we can use the staged_predict() method to get the predicted values along the path.\n\ntest_error = np.zeros_like(boost_boston.train_score_)\nfor idx, y_ in enumerate(boost_boston.staged_predict(X_test)):\n   test_error[idx] = np.mean((y_test - y_)**2)\n\nplot_idx = np.arange(boost_boston.train_score_.shape[0])\nax = subplots(figsize=(8,8))[1]\nax.plot(plot_idx,\n        boost_boston.train_score_,\n        'b',\n        label='Training')\nax.plot(plot_idx,\n        test_error,\n        'r',\n        label='Test')\nax.legend();\n\n\n\n\n\n\n\n\nWe now use the boosted model to predict medv on the test set:\n\ny_hat_boost = boost_boston.predict(X_test);\nnp.mean((y_test - y_hat_boost)**2)\n\n14.481405918831591\n\n\nThe test MSE obtained is 14.48, similar to the test MSE for bagging. If we want to, we can perform boosting with a different value of the shrinkage parameter \\(\\lambda\\) in (8.10). The default value is 0.001, but this is easily modified. Here we take \\(\\lambda=0.2\\).\n\nboost_boston = GBR(n_estimators=5000,\n                   learning_rate=0.2,\n                   max_depth=3,\n                   random_state=0)\nboost_boston.fit(X_train,\n                 y_train)\ny_hat_boost = boost_boston.predict(X_test);\nnp.mean((y_test - y_hat_boost)**2)\n\n14.501514553719565\n\n\nIn this case, using \\(\\lambda=0.2\\) leads to a almost the same test MSE as when using \\(\\lambda=0.001\\)."
  },
  {
    "objectID": "labs/Lab-C2.3-Py-ISLch08-baggboost-lab.html#bayesian-additive-regression-trees",
    "href": "labs/Lab-C2.3-Py-ISLch08-baggboost-lab.html#bayesian-additive-regression-trees",
    "title": "Lab: Tree based methods",
    "section": "Bayesian Additive Regression Trees",
    "text": "Bayesian Additive Regression Trees\nIn this section we demonstrate a Python implementation of BART found in the ISLP.bart package. We fit a model to the Boston housing data set. This BART() estimator is designed for quantitative outcome variables, though other implementations are available for fitting logistic and probit models to categorical outcomes.\n\nbart_boston = BART(random_state=0, burnin=5, ndraw=15)\nbart_boston.fit(X_train, y_train)\n\nBART(burnin=5, ndraw=15, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BARTBART(burnin=5, ndraw=15, random_state=0)\n\n\nOn this data set, with this split into test and training, we see that the test error of BART is similar to that of random forest.\n\nyhat_test = bart_boston.predict(X_test.astype(np.float32))\nnp.mean((y_test - yhat_test)**2)\n\n22.145009458109232\n\n\nWe can check how many times each variable appeared in the collection of trees. This gives a summary similar to the variable importance plot for boosting and random forests.\n\nvar_inclusion = pd.Series(bart_boston.variable_inclusion_.mean(0),\n                               index=D.columns)\nvar_inclusion\n\ncrim       26.933333\nzn         27.866667\nindus      26.466667\nchas       22.466667\nnox        26.600000\nrm         29.800000\nage        22.733333\ndis        26.466667\nrad        23.666667\ntax        24.133333\nptratio    24.266667\nlstat      31.000000\ndtype: float64"
  },
  {
    "objectID": "labs/Lab-C2.1-PIMAIndian.html",
    "href": "labs/Lab-C2.1-PIMAIndian.html",
    "title": "Decision Trees Lab 0",
    "section": "",
    "text": "The Pima Indian Diabetes data set (PimaIndiansDiabetes2) is available in the mlbench package.\n\n\nCode\ndata(\"PimaIndiansDiabetes2\", package = \"mlbench\")\n\n\nThe data contains 768 individuals (female) and 9 clinical variables for predicting the probability of individuals in being diabete-positive or negative:\n\npregnant: number of times pregnant\nglucose: plasma glucose concentration\npressure: diastolic blood pressure (mm Hg)\ntriceps: triceps skin fold thickness (mm)\ninsulin: 2-Hour serum insulin (mu U/ml)\nmass: body mass index (weight in kg/(height in m)^2)\npedigree: diabetes pedigree function\nage: age (years)\ndiabetes: class variable\n\n\n\nCode\ndplyr::glimpse(PimaIndiansDiabetes2)\n\n\nRows: 768\nColumns: 9\n$ pregnant &lt;dbl&gt; 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, 0, 7, 1, 1…\n$ glucose  &lt;dbl&gt; 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168, 139,…\n$ pressure &lt;dbl&gt; 72, 66, 64, 66, 40, 74, 50, NA, 70, 96, 92, 74, 80, 60, 72, N…\n$ triceps  &lt;dbl&gt; 35, 29, NA, 23, 35, NA, 32, NA, 45, NA, NA, NA, NA, 23, 19, N…\n$ insulin  &lt;dbl&gt; NA, NA, NA, 94, 168, NA, 88, NA, 543, NA, NA, NA, NA, 846, 17…\n$ mass     &lt;dbl&gt; 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, NA, 37.…\n$ pedigree &lt;dbl&gt; 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.134, 0.158…\n$ age      &lt;dbl&gt; 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 59, 51, 3…\n$ diabetes &lt;fct&gt; pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg, pos, n…\n\n\nA typical classification/prediction problem is to build a model that can distinguish and predict diabetes using some or all the variables in the dataset.\nA quick exploration can be done wirh the skimr package:\n\n\nCode\nlibrary(skimr)\nskim(PimaIndiansDiabetes2)\n\n\n\nData summary\n\n\nName\nPimaIndiansDiabetes2\n\n\nNumber of rows\n768\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndiabetes\n0\n1\nFALSE\n2\nneg: 500, pos: 268\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npregnant\n0\n1.00\n3.85\n3.37\n0.00\n1.00\n3.00\n6.00\n17.00\n▇▃▂▁▁\n\n\nglucose\n5\n0.99\n121.69\n30.54\n44.00\n99.00\n117.00\n141.00\n199.00\n▁▇▇▃▂\n\n\npressure\n35\n0.95\n72.41\n12.38\n24.00\n64.00\n72.00\n80.00\n122.00\n▁▃▇▂▁\n\n\ntriceps\n227\n0.70\n29.15\n10.48\n7.00\n22.00\n29.00\n36.00\n99.00\n▆▇▁▁▁\n\n\ninsulin\n374\n0.51\n155.55\n118.78\n14.00\n76.25\n125.00\n190.00\n846.00\n▇▂▁▁▁\n\n\nmass\n11\n0.99\n32.46\n6.92\n18.20\n27.50\n32.30\n36.60\n67.10\n▅▇▃▁▁\n\n\npedigree\n0\n1.00\n0.47\n0.33\n0.08\n0.24\n0.37\n0.63\n2.42\n▇▃▁▁▁\n\n\nage\n0\n1.00\n33.24\n11.76\n21.00\n24.00\n29.00\n41.00\n81.00\n▇▃▁▁▁"
  },
  {
    "objectID": "labs/Lab-C2.1-PIMAIndian.html#the-pima-indians-dataset",
    "href": "labs/Lab-C2.1-PIMAIndian.html#the-pima-indians-dataset",
    "title": "Decision Trees Lab 0",
    "section": "",
    "text": "The Pima Indian Diabetes data set (PimaIndiansDiabetes2) is available in the mlbench package.\n\n\nCode\ndata(\"PimaIndiansDiabetes2\", package = \"mlbench\")\n\n\nThe data contains 768 individuals (female) and 9 clinical variables for predicting the probability of individuals in being diabete-positive or negative:\n\npregnant: number of times pregnant\nglucose: plasma glucose concentration\npressure: diastolic blood pressure (mm Hg)\ntriceps: triceps skin fold thickness (mm)\ninsulin: 2-Hour serum insulin (mu U/ml)\nmass: body mass index (weight in kg/(height in m)^2)\npedigree: diabetes pedigree function\nage: age (years)\ndiabetes: class variable\n\n\n\nCode\ndplyr::glimpse(PimaIndiansDiabetes2)\n\n\nRows: 768\nColumns: 9\n$ pregnant &lt;dbl&gt; 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, 0, 7, 1, 1…\n$ glucose  &lt;dbl&gt; 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168, 139,…\n$ pressure &lt;dbl&gt; 72, 66, 64, 66, 40, 74, 50, NA, 70, 96, 92, 74, 80, 60, 72, N…\n$ triceps  &lt;dbl&gt; 35, 29, NA, 23, 35, NA, 32, NA, 45, NA, NA, NA, NA, 23, 19, N…\n$ insulin  &lt;dbl&gt; NA, NA, NA, 94, 168, NA, 88, NA, 543, NA, NA, NA, NA, 846, 17…\n$ mass     &lt;dbl&gt; 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, NA, 37.…\n$ pedigree &lt;dbl&gt; 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.134, 0.158…\n$ age      &lt;dbl&gt; 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 59, 51, 3…\n$ diabetes &lt;fct&gt; pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg, pos, n…\n\n\nA typical classification/prediction problem is to build a model that can distinguish and predict diabetes using some or all the variables in the dataset.\nA quick exploration can be done wirh the skimr package:\n\n\nCode\nlibrary(skimr)\nskim(PimaIndiansDiabetes2)\n\n\n\nData summary\n\n\nName\nPimaIndiansDiabetes2\n\n\nNumber of rows\n768\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndiabetes\n0\n1\nFALSE\n2\nneg: 500, pos: 268\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npregnant\n0\n1.00\n3.85\n3.37\n0.00\n1.00\n3.00\n6.00\n17.00\n▇▃▂▁▁\n\n\nglucose\n5\n0.99\n121.69\n30.54\n44.00\n99.00\n117.00\n141.00\n199.00\n▁▇▇▃▂\n\n\npressure\n35\n0.95\n72.41\n12.38\n24.00\n64.00\n72.00\n80.00\n122.00\n▁▃▇▂▁\n\n\ntriceps\n227\n0.70\n29.15\n10.48\n7.00\n22.00\n29.00\n36.00\n99.00\n▆▇▁▁▁\n\n\ninsulin\n374\n0.51\n155.55\n118.78\n14.00\n76.25\n125.00\n190.00\n846.00\n▇▂▁▁▁\n\n\nmass\n11\n0.99\n32.46\n6.92\n18.20\n27.50\n32.30\n36.60\n67.10\n▅▇▃▁▁\n\n\npedigree\n0\n1.00\n0.47\n0.33\n0.08\n0.24\n0.37\n0.63\n2.42\n▇▃▁▁▁\n\n\nage\n0\n1.00\n33.24\n11.76\n21.00\n24.00\n29.00\n41.00\n81.00\n▇▃▁▁▁"
  },
  {
    "objectID": "labs/Lab-C2.1-PIMAIndian.html#assessing-model-performance",
    "href": "labs/Lab-C2.1-PIMAIndian.html#assessing-model-performance",
    "title": "Decision Trees Lab 0",
    "section": "Assessing model performance",
    "text": "Assessing model performance\nImagine we kow nothing about overfitting.\nWe may want to check the accuracy of the model on the dataset we have used to build it.\n\n\nCode\npredicted.classes&lt;- predict(model1, PimaIndiansDiabetes2, \"class\")\nmean(predicted.classes == PimaIndiansDiabetes2$diabetes)\n\n\n[1] 0.8294271\n\n\nA better strategy is to use train dataset to build the model and a test dataset to check how it works.\n\n\nCode\nset.seed(123)\nssize &lt;- nrow(PimaIndiansDiabetes2)\npropTrain &lt;- 0.8\ntraining.indices &lt;-sample(1:ssize, floor(ssize*propTrain))\ntrain.data  &lt;- PimaIndiansDiabetes2[training.indices, ]\ntest.data &lt;- PimaIndiansDiabetes2[-training.indices, ]\n\n\nNow we build the model on the train data and check its accuracy on the test data.\n\n\nCode\nmodel2 &lt;- rpart(diabetes ~., data = train.data)\npredicted.classes.test&lt;- predict(model2, test.data, \"class\")\nmean(predicted.classes.test == test.data$diabetes)\n\n\n[1] 0.7272727\n\n\nThe accuracy is good, but smaller, as expected."
  },
  {
    "objectID": "labs/Lab-C2.1-PIMAIndian.html#making-predictions-with-the-model",
    "href": "labs/Lab-C2.1-PIMAIndian.html#making-predictions-with-the-model",
    "title": "Decision Trees Lab 0",
    "section": "Making predictions with the model",
    "text": "Making predictions with the model\nAs an example on how to use the model we want to predict the class of individuals 521 and 562\n\n\nCode\n(aSample&lt;- PimaIndiansDiabetes2[c(521,562),])\n\n\n    pregnant glucose pressure triceps insulin mass pedigree age diabetes\n521        2      68       70      32      66 25.0    0.187  25      neg\n562        0     198       66      32     274 41.3    0.502  28      pos\n\n\n\n\nCode\npredict(model1, aSample, \"class\")\n\n\n521 562 \nneg pos \nLevels: neg pos\n\n\n\nIf we follow individuals 521 and 562 along the tree, we reach the same prediction.\nThe tree provides not only a classification but also an explanation."
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html",
    "href": "labs/Lab-C1.1-knn4regression.html",
    "title": "k-NN regression",
    "section": "",
    "text": "The \\(k\\) nearest-neighbor estimator of \\(m(t)=E(Y|X=t)\\) is defined as \\[\n\\hat{m}(t) = \\frac{1}{k} \\sum_{i\\in N_k(t)} y_i,\n\\] where \\(N_k(t)\\) is the neighborhood of \\(t\\) defined by the \\(k\\) closest points \\(x_i\\) in the training sample.\n\n\nThe Boston housing data, included in the MASS package, contains information on housing in the Boston suburbs area.\n\nlibrary(MASS)\n# help(Boston)\ndata(Boston)\n\n\n\n\nOur goal is to predict the housing mean value, stored in the mdev variable, using as predictor variable the percentage of population with lower socio-economic status. lstat.\n\n\n\nWe start by plotting the values to show the relation among them.\n\nx &lt;- Boston$lstat\ny &lt;- Boston$medv\nplot(x,y, xlab=\"x: lstat\", ylab=\"y: medv\")\n\n\n\n\n\n\n\n\nWe migt consider fitting a polynomial curve but we start with KNN"
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html#data-for-prediction",
    "href": "labs/Lab-C1.1-knn4regression.html#data-for-prediction",
    "title": "k-NN regression",
    "section": "",
    "text": "The Boston housing data, included in the MASS package, contains information on housing in the Boston suburbs area.\n\nlibrary(MASS)\n# help(Boston)\ndata(Boston)"
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html#main-predicion-goal",
    "href": "labs/Lab-C1.1-knn4regression.html#main-predicion-goal",
    "title": "k-NN regression",
    "section": "",
    "text": "Our goal is to predict the housing mean value, stored in the mdev variable, using as predictor variable the percentage of population with lower socio-economic status. lstat."
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html#data-description",
    "href": "labs/Lab-C1.1-knn4regression.html#data-description",
    "title": "k-NN regression",
    "section": "",
    "text": "We start by plotting the values to show the relation among them.\n\nx &lt;- Boston$lstat\ny &lt;- Boston$medv\nplot(x,y, xlab=\"x: lstat\", ylab=\"y: medv\")\n\n\n\n\n\n\n\n\nWe migt consider fitting a polynomial curve but we start with KNN"
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html#tuning-the-model",
    "href": "labs/Lab-C1.1-knn4regression.html#tuning-the-model",
    "title": "k-NN regression",
    "section": "Tuning the model",
    "text": "Tuning the model\nRepeat the same process using different values of \\(k\\).\n\nk_values &lt;- c(100, 50, 15, 5)\n\nfor (k in k_values) {\n    mt &lt;- knn_regr(x, y, t=t, k=k, dist.method = \"euclidean\")\n    \n    plot(x, y, col=8, xlab=\"x: lstat\", ylab=\"y: medv\", \n         main=paste0(\"k-NN con k=\", k))\n    lines(t, mt, col=2, lwd=4)\n}"
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html#what-is-training-error",
    "href": "labs/Lab-C1.1-knn4regression.html#what-is-training-error",
    "title": "k-NN regression",
    "section": "What is Training Error?",
    "text": "What is Training Error?\nTraining error is calculated by evaluating the model’s predictions on the same dataset that was used to fit the model. Mathematically, it is computed as:\n\\[\n\\text{Training Error} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{m}(x_i))^2\n\\]\nwhere: - ( y_i ) are the observed values, - ( (x_i) ) are the predicted values from the model, - ( n ) is the total number of observations.\n\nWhy is Training Error Misleading?\nSince the model is evaluated on the same data it was trained on, the training error is overly optimistic and does not provide a reliable measure of how well the model generalizes to new data. This issue is particularly problematic when dealing with flexible models such as k-nearest neighbors (k-NN).\n\n\nExample: k-NN and Overfitting\nIn k-NN regression, the choice of ( k ) significantly affects the training error: - When ( k ) is very small, the model closely follows the training data, leading to low training error but high test error (overfitting). - When ( k ) is too large, the model smooths out the predictions too much, leading to higher training and test error (underfitting).\n\nk_values &lt;- c(200, 100, 50, 15, 5, 3)\n\nerrors &lt;- data.frame(k = integer(), RMSE = numeric())\n\n\nfor (k in k_values) {\n    mt &lt;- knn_regr(x, y, t=t, k=k, dist.method = \"euclidean\")\n    \n\n    RMSE &lt;- sqrt(mean((y - mt)^2))  \n    \n    errors &lt;- rbind(errors, data.frame(k = k, RMSE = RMSE))\n    \n    plot(x, y, col=8, xlab=\"x: lstat\", ylab=\"y: medv\", \n         main=paste0(\"k-NN con k=\", k, \" (RMSE: \", round(RMSE, 2), \")\"))\n    lines(t, mt, col=2, lwd=4)\n}\n\nWarning in y - mt: longitud de objeto mayor no es múltiplo de la longitud de\nuno menor\n\n\n\n\n\n\n\n\n\nWarning in y - mt: longitud de objeto mayor no es múltiplo de la longitud de\nuno menor\n\n\n\n\n\n\n\n\n\nWarning in y - mt: longitud de objeto mayor no es múltiplo de la longitud de\nuno menor\n\n\n\n\n\n\n\n\n\nWarning in y - mt: longitud de objeto mayor no es múltiplo de la longitud de\nuno menor\n\n\n\n\n\n\n\n\n\nWarning in y - mt: longitud de objeto mayor no es múltiplo de la longitud de\nuno menor\n\n\n\n\n\n\n\n\n\nWarning in y - mt: longitud de objeto mayor no es múltiplo de la longitud de\nuno menor\n\n\n\n\n\n\n\n\nprint(errors)\n\n    k     RMSE\n1 200 10.57989\n2 100 11.57428\n3  50 12.49364\n4  15 13.15771\n5   5 12.71861\n6   3 13.11624"
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html#a-better-approach-using-test-error",
    "href": "labs/Lab-C1.1-knn4regression.html#a-better-approach-using-test-error",
    "title": "k-NN regression",
    "section": "A Better Approach: Using Test Error",
    "text": "A Better Approach: Using Test Error\nTo properly evaluate model performance, we must estimate the test error, which is computed on a separate dataset that was not used for training. This can be done by: 1. Splitting the data into a training set and a test set. 2. Training the model on the training set. 3. Evaluating the model on the test set and computing the test error:\n\\[\n\\text{Test Error} = \\frac{1}{m} \\sum_{j=1}^{m} (y_j^{\\text{test}} - \\hat{m}(x_j^{\\text{test}}))^2\n\\]\nwhere: - ( y_j^{} ) are the actual test values, - ( (x_j^{}) ) are the model’s predictions on the test set, - ( m ) is the number of test observations.\nUsing training error alone can give a false sense of accuracy. Instead, we should always assess model performance on unseen data (test set) to ensure that our model generalizes well. In the next section, we will implement k-NN regression with a train-test split to correctly measure prediction error.\n\nAn (improved) example\n\nk_values &lt;- c(200, 100, 50, 15, 5, 3)\n\nerrors &lt;- data.frame(k = integer(), Training_RMSE = numeric(), Test_RMSE = numeric())\n\nset.seed(123)\nn &lt;- length(y)\ntrain_index &lt;- sample(1:n, size = floor(0.8 * n), replace = FALSE)\n\nx_train &lt;- x[train_index]\ny_train &lt;- y[train_index]\n\nx_test &lt;- x[-train_index]\ny_test &lt;- y[-train_index]\n\n\nfor (k in k_values) {\n    mt_train &lt;- knn_regr(x_train, y_train, t=x_train, k=k, dist.method = \"euclidean\")\n    \n    mt_test &lt;- knn_regr(x_train, y_train, t=x_test, k=k, dist.method = \"euclidean\")\n    \n    RMSE_train &lt;- sqrt(mean((y_train - mt_train)^2))  \n    RMSE_test &lt;- sqrt(mean((y_test - mt_test)^2))  \n    \n    errors &lt;- rbind(errors, data.frame(k = k, Training_RMSE = RMSE_train, Test_RMSE = RMSE_test))\n    \n    plot(x_train, y_train, col=8, xlab=\"x: lstat\", ylab=\"y: medv\", \n         main=paste0(\"k-NN con k=\", k, \" (Train RMSE: \", round(RMSE_train, 2), \n                      \", Test RMSE: \", round(RMSE_test, 2), \")\"))\n    \n    t_seq &lt;- seq(min(x), max(x), length.out=100)\n    mt_seq &lt;- knn_regr(x_train, y_train, t=t_seq, k=k, dist.method = \"euclidean\")\n    lines(t_seq, mt_seq, col=2, lwd=4)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(errors)\n\n    k Training_RMSE Test_RMSE\n1 200      6.540422  6.589752\n2 100      5.624611  5.867107\n3  50      5.146213  5.624467\n4  15      4.929732  5.709414\n5   5      4.530371  6.175764\n6   3      4.229030  6.422137"
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html#function-code",
    "href": "labs/Lab-C1.1-knn4regression.html#function-code",
    "title": "k-NN regression",
    "section": "Function Code",
    "text": "Function Code\n\nknn_regr &lt;- function(x, y, t=NULL, k=3, dist.method = \"euclidean\") {\n    nx &lt;- length(y)  # Number of observations in the training data\n    \n    # If t is not provided, use x as the set of query points\n    if (is.null(t)) { \n        t &lt;- as.matrix(x) \n    } else {\n        t &lt;- as.matrix(t)\n    }\n    \n    nt &lt;- dim(t)[1]  # Number of query points\n    \n    # Compute the distance matrix between each t[j] and all x[i]\n    Dtx &lt;- as.matrix(dist(rbind(t, as.matrix(x)), method = dist.method))\n    \n    # Extract only the distances between test points and training points\n    Dtx &lt;- Dtx[1:nt, nt+(1:nx)]\n    \n    mt &lt;- numeric(nt)  # Initialize the vector of predictions\n    \n    # Compute the k-NN estimate for each query point t[j]\n    for (j in 1:nt) {\n        d_t_x &lt;- Dtx[j,]  # Distances from t[j] to all training points\n        d_t_x_k &lt;- sort(d_t_x, partial=k)[k]  # Distance to the k-th nearest neighbor\n        N_t_k &lt;- unname(which(d_t_x &lt;= d_t_x_k))  # Indices of the k nearest neighbors\n        \n        mt[j] &lt;- mean(y[N_t_k])  # Compute the mean response of k nearest neighbors\n    }\n    \n    return(mt)  # Return the vector of estimated values\n}"
  },
  {
    "objectID": "labs/Lab-C1.1-knn4regression.html#step-by-step-explanation",
    "href": "labs/Lab-C1.1-knn4regression.html#step-by-step-explanation",
    "title": "k-NN regression",
    "section": "Step-by-Step Explanation",
    "text": "Step-by-Step Explanation\n\n1. Function Inputs\nThe function takes the following inputs: - x: The vector of predictor values (training data). - y: The corresponding response values. - t: A vector of test points where we want to estimate ( E(Y | X = t) ). If t is not provided, it defaults to x (in-sample estimation). - k: The number of nearest neighbors to consider. - dist.method: The method used to compute distances (default: Euclidean).\n\n\n2. Checking the Input t\nIf t is NULL, the function assigns x to t, meaning that predictions will be computed for the training points.\n\nif (is.null(t)){ \n    t &lt;- as.matrix(x) \n} else {\n    t &lt;- as.matrix(t)\n}\n\n\n\n3. Compute the Distance Matrix\nThe function calculates pairwise distances between every point in t and every point in x using the dist function.\n\ndist.method &lt;- \"euclidean\"\nDtx &lt;- as.matrix(dist(rbind(t, as.matrix(x)), method = dist.method))\n\nAfter computing the distance matrix, we extract only the distances between test points and training points:\n\nDtx &lt;- Dtx[1:nt, nt+(1:nx)]\n\n\n\n4. Find the k Nearest Neighbors\nFor each test point ( t[j] ): - Extract the distances to all training points. - Identify the k-th smallest distance. - Select the indices of the k closest training points.\n\nd_t_x &lt;- Dtx[j,]  # Distances from t[j] to all training points\nd_t_x_k &lt;- sort(d_t_x, partial=k)[k]  # Distance to the k-th nearest neighbor\nN_t_k &lt;- unname(which(d_t_x &lt;= d_t_x_k))  # Indices of the k nearest neighbors\n\n\n\n5. Compute the k-NN Estimate\nOnce the nearest neighbors are identified, their corresponding y values are averaged to compute the estimate:\n\nmt[j] &lt;- mean(y[N_t_k])  # Compute the mean response of k nearest neighbors\n\n\n\n6. Return the Estimated Values\nFinally, the function returns the vector mt containing the estimated values for all test points.\n\nreturn(mt)"
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html",
    "title": "Lab 2: Ensembles of Trees",
    "section": "",
    "text": "Code\n# Helper packages\nlibrary(dplyr)       # for data wrangling\nlibrary(ggplot2)     # for awesome plotting\nlibrary(modeldata)  # for parallel backend to foreach\nlibrary(foreach)     # for parallel processing with for loops\n\n# Modeling packages\n#library(caret)       # for general model fitting\nlibrary(rpart)       # for fitting decision trees\nlibrary(ipred)       # for fitting bagged decision trees"
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#the-dataset",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#the-dataset",
    "title": "Lab 2: Ensembles of Trees",
    "section": "The dataset",
    "text": "The dataset\nPackge AmesHousing contains the data jointly with some instructions to create the required dataset.\nWe will use, however data from the modeldata package where some preprocessing of the data has already been performed (see: https://www.tmwr.org/ames)\n\n\nCode\ndata(ames, package = \"modeldata\")\ndim(ames)\n\n\n[1] 2930   74"
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#exploratory-data-analysis-an-preprocessing",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#exploratory-data-analysis-an-preprocessing",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Exploratory Data Analysis an preprocessing",
    "text": "Exploratory Data Analysis an preprocessing\nThe dataset has 74 variables, and has already been prepared by the package modeldatamaintainers.\nIn any case it is always recommended to do some Exploratory Analysis.\n\n\nCode\nlibrary(skimr)\nskim(ames)\n\n\n\nData summary\n\n\nName\names\n\n\nNumber of rows\n2930\n\n\nNumber of columns\n74\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n40\n\n\nnumeric\n34\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nMS_SubClass\n0\n1\nFALSE\n16\nOne: 1079, Two: 575, One: 287, One: 192\n\n\nMS_Zoning\n0\n1\nFALSE\n7\nRes: 2273, Res: 462, Flo: 139, Res: 27\n\n\nStreet\n0\n1\nFALSE\n2\nPav: 2918, Grv: 12\n\n\nAlley\n0\n1\nFALSE\n3\nNo_: 2732, Gra: 120, Pav: 78\n\n\nLot_Shape\n0\n1\nFALSE\n4\nReg: 1859, Sli: 979, Mod: 76, Irr: 16\n\n\nLand_Contour\n0\n1\nFALSE\n4\nLvl: 2633, HLS: 120, Bnk: 117, Low: 60\n\n\nUtilities\n0\n1\nFALSE\n3\nAll: 2927, NoS: 2, NoS: 1\n\n\nLot_Config\n0\n1\nFALSE\n5\nIns: 2140, Cor: 511, Cul: 180, FR2: 85\n\n\nLand_Slope\n0\n1\nFALSE\n3\nGtl: 2789, Mod: 125, Sev: 16\n\n\nNeighborhood\n0\n1\nFALSE\n28\nNor: 443, Col: 267, Old: 239, Edw: 194\n\n\nCondition_1\n0\n1\nFALSE\n9\nNor: 2522, Fee: 164, Art: 92, RRA: 50\n\n\nCondition_2\n0\n1\nFALSE\n8\nNor: 2900, Fee: 13, Art: 5, Pos: 4\n\n\nBldg_Type\n0\n1\nFALSE\n5\nOne: 2425, Twn: 233, Dup: 109, Twn: 101\n\n\nHouse_Style\n0\n1\nFALSE\n8\nOne: 1481, Two: 873, One: 314, SLv: 128\n\n\nOverall_Cond\n0\n1\nFALSE\n9\nAve: 1654, Abo: 533, Goo: 390, Ver: 144\n\n\nRoof_Style\n0\n1\nFALSE\n6\nGab: 2321, Hip: 551, Gam: 22, Fla: 20\n\n\nRoof_Matl\n0\n1\nFALSE\n8\nCom: 2887, Tar: 23, WdS: 9, WdS: 7\n\n\nExterior_1st\n0\n1\nFALSE\n16\nVin: 1026, Met: 450, HdB: 442, Wd : 420\n\n\nExterior_2nd\n0\n1\nFALSE\n17\nVin: 1015, Met: 447, HdB: 406, Wd : 397\n\n\nMas_Vnr_Type\n0\n1\nFALSE\n5\nNon: 1775, Brk: 880, Sto: 249, Brk: 25\n\n\nExter_Cond\n0\n1\nFALSE\n5\nTyp: 2549, Goo: 299, Fai: 67, Exc: 12\n\n\nFoundation\n0\n1\nFALSE\n6\nPCo: 1310, CBl: 1244, Brk: 311, Sla: 49\n\n\nBsmt_Cond\n0\n1\nFALSE\n6\nTyp: 2616, Goo: 122, Fai: 104, No_: 80\n\n\nBsmt_Exposure\n0\n1\nFALSE\n5\nNo: 1906, Av: 418, Gd: 284, Mn: 239\n\n\nBsmtFin_Type_1\n0\n1\nFALSE\n7\nGLQ: 859, Unf: 851, ALQ: 429, Rec: 288\n\n\nBsmtFin_Type_2\n0\n1\nFALSE\n7\nUnf: 2499, Rec: 106, LwQ: 89, No_: 81\n\n\nHeating\n0\n1\nFALSE\n6\nGas: 2885, Gas: 27, Gra: 9, Wal: 6\n\n\nHeating_QC\n0\n1\nFALSE\n5\nExc: 1495, Typ: 864, Goo: 476, Fai: 92\n\n\nCentral_Air\n0\n1\nFALSE\n2\nY: 2734, N: 196\n\n\nElectrical\n0\n1\nFALSE\n6\nSBr: 2682, Fus: 188, Fus: 50, Fus: 8\n\n\nFunctional\n0\n1\nFALSE\n8\nTyp: 2728, Min: 70, Min: 65, Mod: 35\n\n\nGarage_Type\n0\n1\nFALSE\n7\nAtt: 1731, Det: 782, Bui: 186, No_: 157\n\n\nGarage_Finish\n0\n1\nFALSE\n4\nUnf: 1231, RFn: 812, Fin: 728, No_: 159\n\n\nGarage_Cond\n0\n1\nFALSE\n6\nTyp: 2665, No_: 159, Fai: 74, Goo: 15\n\n\nPaved_Drive\n0\n1\nFALSE\n3\nPav: 2652, Dir: 216, Par: 62\n\n\nPool_QC\n0\n1\nFALSE\n5\nNo_: 2917, Exc: 4, Goo: 4, Typ: 3\n\n\nFence\n0\n1\nFALSE\n5\nNo_: 2358, Min: 330, Goo: 118, Goo: 112\n\n\nMisc_Feature\n0\n1\nFALSE\n6\nNon: 2824, She: 95, Gar: 5, Oth: 4\n\n\nSale_Type\n0\n1\nFALSE\n10\nWD : 2536, New: 239, COD: 87, Con: 26\n\n\nSale_Condition\n0\n1\nFALSE\n6\nNor: 2413, Par: 245, Abn: 190, Fam: 46\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nLot_Frontage\n0\n1\n57.65\n33.50\n0.00\n43.00\n63.00\n78.00\n313.00\n▇▇▁▁▁\n\n\nLot_Area\n0\n1\n10147.92\n7880.02\n1300.00\n7440.25\n9436.50\n11555.25\n215245.00\n▇▁▁▁▁\n\n\nYear_Built\n0\n1\n1971.36\n30.25\n1872.00\n1954.00\n1973.00\n2001.00\n2010.00\n▁▂▃▆▇\n\n\nYear_Remod_Add\n0\n1\n1984.27\n20.86\n1950.00\n1965.00\n1993.00\n2004.00\n2010.00\n▅▂▂▃▇\n\n\nMas_Vnr_Area\n0\n1\n101.10\n178.63\n0.00\n0.00\n0.00\n162.75\n1600.00\n▇▁▁▁▁\n\n\nBsmtFin_SF_1\n0\n1\n4.18\n2.23\n0.00\n3.00\n3.00\n7.00\n7.00\n▃▂▇▁▇\n\n\nBsmtFin_SF_2\n0\n1\n49.71\n169.14\n0.00\n0.00\n0.00\n0.00\n1526.00\n▇▁▁▁▁\n\n\nBsmt_Unf_SF\n0\n1\n559.07\n439.54\n0.00\n219.00\n465.50\n801.75\n2336.00\n▇▅▂▁▁\n\n\nTotal_Bsmt_SF\n0\n1\n1051.26\n440.97\n0.00\n793.00\n990.00\n1301.50\n6110.00\n▇▃▁▁▁\n\n\nFirst_Flr_SF\n0\n1\n1159.56\n391.89\n334.00\n876.25\n1084.00\n1384.00\n5095.00\n▇▃▁▁▁\n\n\nSecond_Flr_SF\n0\n1\n335.46\n428.40\n0.00\n0.00\n0.00\n703.75\n2065.00\n▇▃▂▁▁\n\n\nGr_Liv_Area\n0\n1\n1499.69\n505.51\n334.00\n1126.00\n1442.00\n1742.75\n5642.00\n▇▇▁▁▁\n\n\nBsmt_Full_Bath\n0\n1\n0.43\n0.52\n0.00\n0.00\n0.00\n1.00\n3.00\n▇▆▁▁▁\n\n\nBsmt_Half_Bath\n0\n1\n0.06\n0.25\n0.00\n0.00\n0.00\n0.00\n2.00\n▇▁▁▁▁\n\n\nFull_Bath\n0\n1\n1.57\n0.55\n0.00\n1.00\n2.00\n2.00\n4.00\n▁▇▇▁▁\n\n\nHalf_Bath\n0\n1\n0.38\n0.50\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▁\n\n\nBedroom_AbvGr\n0\n1\n2.85\n0.83\n0.00\n2.00\n3.00\n3.00\n8.00\n▁▇▂▁▁\n\n\nKitchen_AbvGr\n0\n1\n1.04\n0.21\n0.00\n1.00\n1.00\n1.00\n3.00\n▁▇▁▁▁\n\n\nTotRms_AbvGrd\n0\n1\n6.44\n1.57\n2.00\n5.00\n6.00\n7.00\n15.00\n▁▇▂▁▁\n\n\nFireplaces\n0\n1\n0.60\n0.65\n0.00\n0.00\n1.00\n1.00\n4.00\n▇▇▁▁▁\n\n\nGarage_Cars\n0\n1\n1.77\n0.76\n0.00\n1.00\n2.00\n2.00\n5.00\n▅▇▂▁▁\n\n\nGarage_Area\n0\n1\n472.66\n215.19\n0.00\n320.00\n480.00\n576.00\n1488.00\n▃▇▃▁▁\n\n\nWood_Deck_SF\n0\n1\n93.75\n126.36\n0.00\n0.00\n0.00\n168.00\n1424.00\n▇▁▁▁▁\n\n\nOpen_Porch_SF\n0\n1\n47.53\n67.48\n0.00\n0.00\n27.00\n70.00\n742.00\n▇▁▁▁▁\n\n\nEnclosed_Porch\n0\n1\n23.01\n64.14\n0.00\n0.00\n0.00\n0.00\n1012.00\n▇▁▁▁▁\n\n\nThree_season_porch\n0\n1\n2.59\n25.14\n0.00\n0.00\n0.00\n0.00\n508.00\n▇▁▁▁▁\n\n\nScreen_Porch\n0\n1\n16.00\n56.09\n0.00\n0.00\n0.00\n0.00\n576.00\n▇▁▁▁▁\n\n\nPool_Area\n0\n1\n2.24\n35.60\n0.00\n0.00\n0.00\n0.00\n800.00\n▇▁▁▁▁\n\n\nMisc_Val\n0\n1\n50.64\n566.34\n0.00\n0.00\n0.00\n0.00\n17000.00\n▇▁▁▁▁\n\n\nMo_Sold\n0\n1\n6.22\n2.71\n1.00\n4.00\n6.00\n8.00\n12.00\n▅▆▇▃▃\n\n\nYear_Sold\n0\n1\n2007.79\n1.32\n2006.00\n2007.00\n2008.00\n2009.00\n2010.00\n▇▇▇▇▃\n\n\nSale_Price\n0\n1\n180796.06\n79886.69\n12789.00\n129500.00\n160000.00\n213500.00\n755000.00\n▇▇▁▁▁\n\n\nLongitude\n0\n1\n-93.64\n0.03\n-93.69\n-93.66\n-93.64\n-93.62\n-93.58\n▅▅▇▆▁\n\n\nLatitude\n0\n1\n42.03\n0.02\n41.99\n42.02\n42.03\n42.05\n42.06\n▂▂▇▇▇\n\n\n\n\n\nThe exploration shows that the data set is well formed with factor data types for categorical variables and no missings.\nIt can also be seen that tha response variabl, Sales_Price varies on a high range, as confirmed below.\n\n\nCode\nsummary(ames$Sale_Price)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12789  129500  160000  180796  213500  755000 \n\n\n\n\nCode\nboxplot(ames[,sapply(ames, is.numeric)], las=2, cex.axis=0.5)\n\n\n\n\n\n\n\n\n\nAlthough not strictly necessary, given that the variable that has the widest range of variation is the variable to predict we can consider transforming it. In this case the simplest transform seems to express the price in thousands instead of dollars. The distribution of the variable is asymetrical so we may also consider taking logarithm, but given that it would complicate the interpretation of the results, and that something like normality is not required by the methods we use, only division by 1000 is performed.\n\n\nCode\nrequire(dplyr)\names &lt;- ames %&gt;% mutate(Sale_Price = Sale_Price/1000)\nboxplot(ames)"
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#spliting-the-data-into-testtrain",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#spliting-the-data-into-testtrain",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Spliting the data into test/train",
    "text": "Spliting the data into test/train\nWe split the data in separate test / training sets and do it in such a way that samplig is balanced for the response variable, Sale_Price.\n\n\nCode\nif(!require(rsample))\n  install.packages(\"rsample\", dep=TRUE)\n# Stratified sampling with the rsample package\nset.seed(123)\nsplit &lt;- rsample::initial_split(ames, prop = 0.7, \n                       strata = \"Sale_Price\")\names_train  &lt;- training(split)\names_test   &lt;- testing(split)"
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#optimizing-the-tree",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#optimizing-the-tree",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Optimizing the tree",
    "text": "Optimizing the tree\nIn order to optimize the tree we ccan ompute the best cost complexity value\n\n\nCode\nset.seed(123)\ncv_ames_rt1 &lt;- tree::cv.tree(ames_rt1, K = 5)\n\noptSize &lt;- rev(cv_ames_rt1$size)[which.min(rev(cv_ames_rt1$dev))]\npaste(\"Optimal size obtained is:\", optSize)\n\n\n[1] \"Optimal size obtained is: 12\"\n\n\nThe best value of alpha is obtained with the same tree, which suggests that there will be no advantage in pruning.\nThis is confirmed by plotting tree size vs deviance which shows that the tree with the samllest error is the biggest one that can be obtained.\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggpubr)\n\nresultados_cv &lt;- data.frame(\n                   n_nodes  = cv_ames_rt1$size,\n                   deviance = cv_ames_rt1$dev,\n                   alpha    = cv_ames_rt1$k\n                 )\n\np1 &lt;- ggplot(data = resultados_cv, aes(x = n_nodes, y = deviance)) +\n      geom_line() + \n      geom_point() +\n      geom_vline(xintercept = optSize, color = \"red\") +\n      labs(title = \"Error vs tree size\") +\n      theme_bw() \n  \np2 &lt;- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +\n      geom_line() + \n      geom_point() +\n      labs(title = \"Error vs penalization (alpha)\") +\n      theme_bw() \n\nggarrange(p1, p2)\n\n\n\n\n\n\n\n\n\nWe could have tried to obtain a bigger tree with the hope that pruning might find a better tree. This can be done setting the tree parameters to minimal values.\n\n\nCode\nctlPars2 &lt;-tree.control(nobs=nrow(ames_train), mincut = 1, minsize = 2, mindev = 0)\names_rt2 &lt;-  tree::tree(\n                    formula = Sale_Price ~ .,\n                    data    = ames_train,\n                    split   = \"deviance\",\n                    control = ctlPars2)\nsummary(ames_rt2)\n\n\n\nRegression tree:\ntree::tree(formula = Sale_Price ~ ., data = ames_train, control = ctlPars2, \n    split = \"deviance\")\nVariables actually used in tree construction:\n [1] \"Neighborhood\"   \"First_Flr_SF\"   \"Garage_Cond\"    \"Gr_Liv_Area\"   \n [5] \"Central_Air\"    \"Garage_Area\"    \"Enclosed_Porch\" \"Lot_Area\"      \n [9] \"Longitude\"      \"Latitude\"       \"MS_SubClass\"    \"Lot_Frontage\"  \n[13] \"Overall_Cond\"   \"Year_Built\"     \"MS_Zoning\"      \"Alley\"         \n[17] \"Fireplaces\"     \"House_Style\"    \"BsmtFin_Type_1\" \"Bsmt_Exposure\" \n[21] \"Bsmt_Unf_SF\"    \"Electrical\"     \"Year_Remod_Add\" \"Exterior_1st\"  \n[25] \"Open_Porch_SF\"  \"Exterior_2nd\"   \"Functional\"     \"Mo_Sold\"       \n[29] \"Total_Bsmt_SF\"  \"Bsmt_Full_Bath\" \"Sale_Condition\" \"Heating_QC\"    \n[33] \"Mas_Vnr_Area\"   \"Garage_Cars\"    \"Land_Contour\"   \"Condition_1\"   \n[37] \"Mas_Vnr_Type\"   \"Second_Flr_SF\"  \"Lot_Config\"     \"Exter_Cond\"    \n[41] \"Fence\"          \"Lot_Shape\"      \"Bsmt_Cond\"      \"Bedroom_AbvGr\" \n[45] \"Wood_Deck_SF\"   \"Roof_Style\"     \"Sale_Type\"      \"BsmtFin_Type_2\"\n[49] \"Bldg_Type\"      \"Garage_Type\"    \"Year_Sold\"      \"Garage_Finish\" \n[53] \"Screen_Porch\"   \"Half_Bath\"      \"Foundation\"     \"BsmtFin_SF_1\"  \n[57] \"Full_Bath\"      \"Land_Slope\"     \"TotRms_AbvGrd\" \nNumber of terminal nodes:  1222 \nResidual mean deviance:  2.579 = 2133 / 827 \nDistribution of residuals:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -2.750  -0.500   0.000   0.000   0.500   2.943 \n\n\nThis bigger tree has indeed a smaller deviance but pruning provides no benefit:\n\n\nCode\nset.seed(123)\ncv_ames_rt2 &lt;- tree::cv.tree(ames_rt2, K = 5)\n\noptSize2 &lt;- rev(cv_ames_rt2$size)[which.min(rev(cv_ames_rt2$dev))]\npaste(\"Optimal size obtained is:\", optSize2)\n\n\n[1] \"Optimal size obtained is: 12\"\n\n\n\n\nCode\nprunedTree2 &lt;- tree::prune.tree(\n                  tree = ames_rt2,\n                  best = optSize2\n               )\nsummary(prunedTree2)\n\n\n\nRegression tree:\nsnip.tree(tree = ames_rt2, nodes = c(61L, 31L, 12L, 13L, 60L, \n18L, 19L, 14L, 23L, 10L, 22L, 8L))\nVariables actually used in tree construction:\n[1] \"Neighborhood\"  \"First_Flr_SF\"  \"Gr_Liv_Area\"   \"Total_Bsmt_SF\"\n[5] \"Second_Flr_SF\"\nNumber of terminal nodes:  12 \nResidual mean deviance:  1428 = 2909000 / 2037 \nDistribution of residuals:\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-227.1000  -21.2400   -0.2372    0.0000   19.0800  212.0000 \n\n\n\n\nCode\nres_cv2 &lt;- data.frame(\n                   n_nodes  = cv_ames_rt2$size,\n                   deviance = cv_ames_rt2$dev,\n                   alpha    = cv_ames_rt2$k\n                 )\n\np1 &lt;- ggplot(data = res_cv2, aes(x = n_nodes, y = deviance)) +\n      geom_line() + \n      geom_point() +\n      geom_vline(xintercept = optSize2, color = \"red\") +\n      labs(title = \"Error vs tree size\") +\n      theme_bw() \n  \np2 &lt;- ggplot(data = res_cv2, aes(x = alpha, y = deviance)) +\n      geom_line() + \n      geom_point() +\n      labs(title = \"Error vs penalization (alpha)\") +\n      theme_bw() \n\nggarrange(p1, p2)\n\n\n\n\n\n\n\n\n\nThe performance of the trees is hardly different between small or big tree in pruned or non-pruned version.\n\n\nCode\names_rt_pred1 &lt;- predict(ames_rt1, newdata = ames_test)\ntest_rmse1    &lt;- sqrt(mean((ames_rt_pred1 - ames_test$Sale_Price)^2))\npaste(\"Error test (rmse) for initial tree:\", round(test_rmse1,2))\n\n\n[1] \"Error test (rmse) for initial tree: 39.69\"\n\n\n\n\nCode\names_rt_pred2 &lt;- predict(ames_rt2, newdata = ames_test)\ntest_rmse2    &lt;- sqrt(mean((ames_rt_pred2 - ames_test$Sale_Price)^2))\npaste(\"Error test (rmse) for big tree:\", round(test_rmse2,2))\n\n\n[1] \"Error test (rmse) for big tree: 37.59\"\n\n\n\n\nCode\names_pruned_pred &lt;- predict(prunedTree2, newdata = ames_test)\ntest_rmse3    &lt;- sqrt(mean((ames_pruned_pred - ames_test$Sale_Price)^2))\npaste(\"Error test (rmse) for pruned tree:\", round(test_rmse3,2))\n\n\n[1] \"Error test (rmse) for pruned tree: 39.69\"\n\n\nCode\nimprovement &lt;- (test_rmse3-test_rmse2)/test_rmse2*100\n\n\nThe MSE for each model will be saved to facilitate comparison with other models\n\n\nCode\nerrTable &lt;- data.frame(Model=character(),  RMSE=double())\nerrTable[1, ] &lt;-  c(\"Default Regression Tree\", round(test_rmse1,2))\nerrTable[2, ] &lt;-  c(\"Big Regression Tree\", round(test_rmse2,2))\nerrTable[3, ] &lt;-  c(\"Optimally pruned Regression Tree\", round(test_rmse3,2))\n\n\n\n\nCode\n# kableExtra::kable(errTable) %&gt;% kableExtra::kable_styling()\nknitr::kable(errTable) \n\n\n\n\n\nModel\nRMSE\n\n\n\n\nDefault Regression Tree\n39.69\n\n\nBig Regression Tree\n37.59\n\n\nOptimally pruned Regression Tree\n39.69\n\n\n\n\n\nIn summary, what is illustrated by this example is that, for some datasets, it is very hard to obtain an optimal tree because there seems to be a minimum complexity which is very hard to decrease.\nBuilding a saturated tree only provides a slight improvement of less than 5% in RMSE at the cost of having to use 5 times more variables in a tree withh more than 1000 nodes.\nThis is a good point to consider using an ensemble instead of single trees."
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#feature-interpretation",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#feature-interpretation",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Feature interpretation",
    "text": "Feature interpretation\nTo measure feature importance, the reduction in the loss function (e.g., SSE) attributed to each variable at each split is tabulated.\nIn some instances, a single variable could be used multiple times in a tree; consequently, the total reduction in the loss function across all splits by a variable are summed up and used as the total feature importance.\nNot all packages store the informaticon required to compute variable importance. For instance, the treepackges does not, but rpartor caret do save it.\n\n\nCode\nvip(ames_rt1bis, num_features = 40, bar = FALSE)"
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#distinct-error-rates",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#distinct-error-rates",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Distinct error rates",
    "text": "Distinct error rates\nThe following chunks of code show the fit between data and predictions for the train set, the test set and the out-of bag samples\n\nError estimated from train samples\n\n\nCode\nyhattrain.bag &lt;- predict(bag.Ames, newdata = ames_train)\n# train_mse_bag  &lt;- sqrt(mean(yhattrain.bag - ames_train$Sale_Price)^2)\ntrain_rmse_bag &lt;- sqrt(mean((yhattrain.bag - ames_train$Sale_Price)^2))\nshowError&lt;- paste(\"Error train (rmse) for bagged tree:\", round(train_rmse_bag,6))\nplot(yhattrain.bag, ames_train$Sale_Price, main=showError)\nabline(0, 1)\n\n\n\n\n\n\n\n\n\nTHis error is much smaller even than those from trees because of overfitting\n\n\nError estimated from test samples\n\n\nCode\nyhat.bag &lt;- predict(bag.Ames, newdata = ames_test)\n# test_mse_bag  &lt;- sqrt(mean(yhat.bag - ames_test$Sale_Price)^2)\ntest_rmse_bag  &lt;- sqrt(mean((yhat.bag - ames_test$Sale_Price)^2))\nshowError&lt;- paste(\"Error test (rmse) for bagged tree:\", round(test_rmse_bag,4))\nplot(yhat.bag, ames_test$Sale_Price, main=showError)\nabline(0, 1)\n\n\n\n\n\n\n\n\n\n\n\nError estimated from out-of-bag samples\nBagging allows computing an out-of bag error estimate.\nOut of bag error rate is reported in the output and can be computed from the predicted (“the predicted values of the input data based on out-of-bag samples”)\n\n\nCode\noob_err&lt;- sqrt(mean((bag.Ames$predicted-ames_train$Sale_Price)^2))\nshowError &lt;- paste(\"Out of bag error for bagged tree:\", round(oob_err,4))\nplot(bag.Ames$predicted, ames_train$Sale_Price, main=showError)\nabline(0, 1)\n\n\n\n\n\n\n\n\n\nInterestingly this may be not only bigger than the error estimated on the train set but also bigger than the error estimated on the test set.\nWe can collect error rates and compare to each other and also to those obtained from regression trees:\n\n\nCode\nerrTable &lt;- data.frame(Model=character(),  RMSE=double())\nerrTable[1, ] &lt;-  c(\"Default Regression Tree\", round(test_rmse1,2))\nerrTable[2, ] &lt;-  c(\"Big Regression Tree\", round(test_rmse2,2))\nerrTable[3, ] &lt;-  c(\"Optimally pruned Regression Tree\", round(test_rmse3,2))\nerrTable[4, ] &lt;-  c(\"Bagged Tree with Train Data\", round(train_rmse_bag,2))\nerrTable[5, ] &lt;-  c(\"Bagged Tree with Test Data\", round(test_rmse_bag,2))\nerrTable[6, ] &lt;-  c(\"Bagged Tree with OOB error rate\", round(oob_err,2))"
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#bagging-parameter-tuning",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#bagging-parameter-tuning",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Bagging parameter tuning",
    "text": "Bagging parameter tuning\nBagging tends to improve quickly as the number of resampled trees increases, and then it reaches a platform.\nThe figure below has been produced iterated the computation above over nbagg values of 1–200 and applied the bagging() function.\n\n\n\n\n\nError curve for bagging 1-200 deep, unpruned decision trees. The benefit of bagging is optimized at 187 trees although the majority of error reduction occurred within the first 100 trees"
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#variable-importance",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#variable-importance",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Variable importance",
    "text": "Variable importance\nDue to the bagging process, models that are normally perceived as interpretable are no longer so.\nHowever, we can still make inferences about how features are influencing our model using feature importance measures based on the sum of the reduction in the loss function (e.g., SSE) attributed to each variable at each split in a given tree.\n\n\nCode\nrequire(dplyr)\nVIP &lt;- importance(bag.Ames) \nVIP &lt;- VIP[order(VIP[,1], decreasing = TRUE),]\nhead(VIP, n=30)\n\n\n                 %IncMSE IncNodePurity\nGr_Liv_Area    26.419428    1923769.45\nNeighborhood   19.795920    5243520.37\nTotal_Bsmt_SF  14.370182    1012412.95\nFirst_Flr_SF   13.467640     511525.12\nMS_SubClass    10.468429     146349.46\nBsmtFin_Type_1  9.491046      57870.21\nYear_Remod_Add  9.273779     214595.97\nGarage_Area     7.485364     334439.51\nYear_Built      7.112852     266905.91\nFireplaces      6.999228      57043.18\nGarage_Cars     6.915052    1777310.87\nBsmt_Unf_SF     6.879415      78882.32\nSecond_Flr_SF   6.867619     103373.55\nExterior_1st    6.699528      95407.73\nOverall_Cond    6.676218      96814.53\nGarage_Cond     6.622676      49944.12\nExterior_2nd    5.907220      61992.47\nLatitude        5.872897      72448.76\nLot_Area        5.696540     134015.35\nLongitude       5.055073      71328.45\nGarage_Type     4.686528      37116.56\nBsmtFin_SF_1    4.543646      10169.18\nBsmt_Exposure   4.500273      48517.30\nFull_Bath       4.416821      84402.03\nGarage_Finish   4.405324      24862.60\nSale_Condition  4.173237      25932.41\nTotRms_AbvGrd   4.162582      27350.80\nCentral_Air     4.080131      21049.07\nHeating_QC      3.922138      19705.37\nMas_Vnr_Area    3.862446      72973.11\n\n\nImportance values can be plotted directly:\n\n\nCode\ninvVIP &lt;-VIP[order(VIP[,1], decreasing = FALSE),1] \ntVIP&lt;- tail(invVIP, n=15)\nbarplot(tVIP, horiz = TRUE, cex.names=0.5)\n\n\n\n\n\n\n\n\n\nAlternatively one can use the vipfunction from the vip package\n\n\nCode\nlibrary(vip)\nvip(bag.Ames, num_features = 40, bar = FALSE)"
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#parameter-optimization-for-rf",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#parameter-optimization-for-rf",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Parameter optimization for RF",
    "text": "Parameter optimization for RF\nSeveral parameters can be changed to optimize a random forest predictor, but, usually, the most important one is the number of variables to be randomly selected at each split \\(m_{try}\\), followed by the number of tree, which tends to stabilize after a certain value.\nA common strategy to find the optimum combination of parameters is to perform a grid search through a combination of parameter values. Obviously it can be time consuming so a small grid is run in the example below to illustrate how to do it.\n\n\nCode\nnum_trees_range &lt;- seq(100, 400, 100)\n\nnum_vars_range &lt;- floor(ncol(ames_train)/(seq(2,4,1)))\n\nRFerrTable &lt;- data.frame(Model=character(), \n                       NumTree=integer(), NumVar=integer(), \n                       RMSE=double())\nerrValue &lt;- 1\nsystem.time(\nfor (i in seq_along(num_trees_range)){\n  for (j in seq_along(num_vars_range)) {  \n    numTrees &lt;- num_trees_range[i]\n    numVars &lt;- num_vars_range [j] # floor(ncol(ames_train)/3) # default\n    RF.Ames.n &lt;- randomForest(Sale_Price ~ ., \n                         data = ames_train, \n                         mtry = numVars,\n                         ntree= numTrees,\n                         importance = TRUE)\n    yhat.rf &lt;- predict(RF.Ames.n, newdata = ames_test)\n    oob.rf &lt;- RF.Ames.n$predicted\n    \n    test_rmse_rf  &lt;- sqrt(mean((yhat.rf - ames_test$Sale_Price)^2))\n\n  \n    RFerrTable[errValue, ] &lt;-  c(\"Random Forest\", \n                            NumTree = numTrees, NumVar = numVars, \n                            RMSE = round(test_rmse_rf,2)) \n    errValue &lt;- errValue+1\n  }\n}\n)\n\n\n   user  system elapsed \n 180.40    1.05  186.91 \n\n\n\n\nCode\nRFerrTable %&gt;% knitr::kable()\n\n\n\n\n\nModel\nNumTree\nNumVar\nRMSE\n\n\n\n\nRandom Forest\n100\n37\n24.51\n\n\nRandom Forest\n100\n24\n24.85\n\n\nRandom Forest\n100\n18\n24.79\n\n\nRandom Forest\n200\n37\n24.66\n\n\nRandom Forest\n200\n24\n24.86\n\n\nRandom Forest\n200\n18\n24.74\n\n\nRandom Forest\n300\n37\n24.51\n\n\nRandom Forest\n300\n24\n24.53\n\n\nRandom Forest\n300\n18\n25.12\n\n\nRandom Forest\n400\n37\n24.58\n\n\nRandom Forest\n400\n24\n24.62\n\n\nRandom Forest\n400\n18\n24.84\n\n\n\n\n\nThe minimum RMSE is attained at.\n\n\nCode\nbestRF &lt;- which(RFerrTable$RMSE==min(RFerrTable$RMSE))\nRFerrTable[bestRF,]\n\n\n          Model NumTree NumVar  RMSE\n1 Random Forest     100     37 24.51\n7 Random Forest     300     37 24.51\n\n\nCode\nminRFErr &lt;- as.numeric(RFerrTable[bestRF,4])\nerrTable[8, ] &lt;-  c(\"Random Forest (Optimized)\", round(minRFErr,2))"
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#error-comparison-for-all-approaches",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#error-comparison-for-all-approaches",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Error comparison for all approaches",
    "text": "Error comparison for all approaches\n\n\nCode\n# kableExtra::kable(errTable) %&gt;% kableExtra::kable_styling()\nknitr::kable(errTable) \n\n\n\n\n\nModel\nRMSE\n\n\n\n\nDefault Regression Tree\n39.69\n\n\nBig Regression Tree\n37.59\n\n\nOptimally pruned Regression Tree\n39.69\n\n\nBagged Tree with Train Data\n10.95\n\n\nBagged Tree with Test Data\n24.6\n\n\nBagged Tree with OOB error rate\n27.15\n\n\nRandom Forest (defaults)\n24.57\n\n\nRandom Forest (Optimized)\n24.51\n\n\n\n\n\nIn summary, it has been shown that a Random Forest with 400 trees and 37 variables provides the smallest error rate, though the improvement on the default values and even the bagging approach is very small. This may be seen as a confirmation from the fact that Random Forests are well known to be good “out-of-the-box predictors”, that is that they perform well, even without tuning."
  },
  {
    "objectID": "labs/Lab-C.2.4-Ensembles-1RF.html#variable-importance-1",
    "href": "labs/Lab-C.2.4-Ensembles-1RF.html#variable-importance-1",
    "title": "Lab 2: Ensembles of Trees",
    "section": "Variable importance",
    "text": "Variable importance\nAs could be expected, a variable importance plot shows that there is hardly any difference between the variables by bagging or random forests.\n\n\nCode\nlibrary(vip)\nvip(RF.Ames, num_features = 40, bar = FALSE)"
  },
  {
    "objectID": "labs/Lab-C.2.4-Boosting.html",
    "href": "labs/Lab-C.2.4-Boosting.html",
    "title": "Ensembles Lab 2: Boosting",
    "section": "",
    "text": "# Helper packages\nlibrary(dplyr)       # for data wrangling\nlibrary(ggplot2)     # for awesome plotting\nlibrary(modeldata)  \nlibrary(foreach)     # for parallel processing with for loops\n\n# Modeling packages\n# library(tidymodels)\nlibrary(xgboost)\nlibrary(gbm)"
  },
  {
    "objectID": "labs/Lab-C.2.4-Boosting.html#ames-housing-dataset",
    "href": "labs/Lab-C.2.4-Boosting.html#ames-housing-dataset",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Ames Housing dataset",
    "text": "Ames Housing dataset\nPackge AmesHousing contains the data jointly with some instructions to create the required dataset.\nWe will use, however data from the modeldata package where some preprocessing of the data has already been performed (see: https://www.tmwr.org/ames)\nThe dataset has 74 variables so a descriptive analysis is not provided.\n\ndim(ames)\n\n[1] 2930   74\n\nboxplot(ames)\n\n\n\n\n\n\n\n\nWe proceed as in the previous lab and divide the reponse variable by 1000 facilitate reviewing the results .\n\nrequire(dplyr)\names &lt;- ames %&gt;% mutate(Sale_Price = Sale_Price/1000)\nboxplot(ames)"
  },
  {
    "objectID": "labs/Lab-C.2.4-Boosting.html#spliting-the-data-into-testtrain",
    "href": "labs/Lab-C.2.4-Boosting.html#spliting-the-data-into-testtrain",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Spliting the data into test/train",
    "text": "Spliting the data into test/train\nThe data are split in separate test / training sets and do it in such a way that samplig is balanced for the response variable, Sale_Price.\n\n# Stratified sampling with the rsample package\nset.seed(123)\nsplit &lt;- rsample::initial_split(ames, prop = 0.7, \n                       strata = \"Sale_Price\")\names_train  &lt;- training(split)\names_test   &lt;- testing(split)"
  },
  {
    "objectID": "labs/Lab-C.2.4-Boosting.html#xgboost-parameters-overview",
    "href": "labs/Lab-C.2.4-Boosting.html#xgboost-parameters-overview",
    "title": "Ensembles Lab 2: Boosting",
    "section": "XGBoost Parameters Overview",
    "text": "XGBoost Parameters Overview\nThe xgboost() function in the XGBoost package trains Gradient Boosting models for regression and classification tasks. Key parameters and hyperparameters include:\n\nParameters:\n\nparams: List of training parameters.\ndata: Training data.\nnrounds: Number of boosting rounds.\nwatchlist: Validation set for early stopping.\nobj: Custom objective function.\nfeval: Custom evaluation function.\nverbose: Verbosity level.\nprint_every_n: Print frequency.\nearly_stopping_rounds: Rounds for early stopping.\nmaximize: Maximize evaluation metric.\nsave_period: Model save frequency.\nsave_name: Name for saved model.\nxgb_model: Existing XGBoost model.\ncallbacks: List of callback functions.\n\n\n\nXGBoost Parameters Overview\nNumerous parameters govern XGBoost’s behavior. A detailed description of all parameters can be found in the XGBoost documentation. Key considerations include those controlling tree growth, model learning rate, and early stopping to prevent overfitting:\n\nParameters:\n\nbooster [default = gbtree]: Type of weak learner, trees (“gbtree”, “dart”) or linear models (“gblinear”).\neta [default=0.3, alias: learning_rate]: Reduces each tree’s contribution by multiplying its original influence by this value.\ngamma [default=0, alias: min_split_loss]: Minimum cost reduction required for a split to occur.\nmax_depth [default=6]: Maximum depth trees can reach.\nsubsample [default=1]: Proportion of observations used for each tree’s training. If less than 1, applies Stochastic Gradient Boosting.\ncolsample_bytree: Number of predictors considered at each split.\nnrounds: Number of boosting iterations, i.e., the number of models in the ensemble.\nearly_stopping_rounds: Number of consecutive iterations without improvement to trigger early stopping. If NULL, early stopping is disabled. Requires a separate validation set (watchlist) for early stopping.\nwatchlist: Validation set used for early stopping.\nseed: Seed for result reproducibility. Note: use set.seed() instead."
  },
  {
    "objectID": "labs/Lab-C.2.4-Boosting.html#test-training-in-xgboost",
    "href": "labs/Lab-C.2.4-Boosting.html#test-training-in-xgboost",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Test / Training in xGBoost",
    "text": "Test / Training in xGBoost\n\nXGBoost Data Formats\nXGBoost models can work with various data formats, including R matrices.\nHowever, it’s advisable to use xgb.DMatrix, a specialized and optimized data structure within this library.\n\names_train &lt;- xgb.DMatrix(\n                data  = ames_train %&gt;% select(-Sale_Price)\n                %&gt;% data.matrix(),\n                label = ames_train$Sale_Price,\n               )\n\names_test &lt;- xgb.DMatrix(\n                data  = ames_test %&gt;% select(-Sale_Price)\n                %&gt;% data.matrix(),\n                label = ames_test$Sale_Price\n               )"
  },
  {
    "objectID": "labs/Lab-C.2.4-Boosting.html#fit-the-model",
    "href": "labs/Lab-C.2.4-Boosting.html#fit-the-model",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Fit the model",
    "text": "Fit the model\n\nset.seed(123)\names.boost &lt;- xgb.train(\n            data    = ames_train,\n            params  = list(max_depth = 2),\n            nrounds = 1000,\n            eta= 0.05\n          )\names.boost\n\n##### xgb.Booster\nraw: 872.5 Kb \ncall:\n  xgb.train(params = list(max_depth = 2), data = ames_train, nrounds = 1000, \n    eta = 0.05)\nparams (as set within xgb.train):\n  max_depth = \"2\", eta = \"0.05\", validate_parameters = \"1\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.print.evaluation(period = print_every_n)\n# of features: 73 \nniter: 1000\nnfeatures : 73"
  },
  {
    "objectID": "labs/Lab-C.2.4-Boosting.html#prediction-and-model-assessment",
    "href": "labs/Lab-C.2.4-Boosting.html#prediction-and-model-assessment",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Prediction and model assessment",
    "text": "Prediction and model assessment\n\names.boost.trainpred &lt;- predict(ames.boost,\n                   newdata = ames_train\n                 )\n\names.boost.pred &lt;- predict(ames.boost,\n                   newdata = ames_test\n                 )\n\ntrain_rmseboost &lt;- sqrt(mean((ames.boost.trainpred - getinfo(ames_train, \"label\"))^2))\n\ntest_rmseboost &lt;- sqrt(mean((ames.boost.pred - getinfo(ames_test, \"label\"))^2))\n\npaste(\"Error train (rmse) in XGBoost:\", round(train_rmseboost,2))\n\n[1] \"Error train (rmse) in XGBoost: 14.31\"\n\npaste(\"Error test (rmse) in XGBoost:\", round(test_rmseboost,2))\n\n[1] \"Error test (rmse) in XGBoost: 22.69\""
  },
  {
    "objectID": "labs/Lab-C.2.4b-BoostingOptimization.html",
    "href": "labs/Lab-C.2.4b-BoostingOptimization.html",
    "title": "Ensembles Lab 2: Boosting",
    "section": "",
    "text": "# Helper packages\nlibrary(dplyr)       # for data wrangling\nlibrary(ggplot2)     # for awesome plotting\nlibrary(modeldata)  \nlibrary(foreach)     # for parallel processing with for loops\n\n# Modeling packages\n# library(tidymodels)\nlibrary(xgboost)\nlibrary(gbm)"
  },
  {
    "objectID": "labs/Lab-C.2.4b-BoostingOptimization.html#ames-housing-dataset",
    "href": "labs/Lab-C.2.4b-BoostingOptimization.html#ames-housing-dataset",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Ames Housing dataset",
    "text": "Ames Housing dataset\nPackge AmesHousing contains the data jointly with some instructions to create the required dataset.\nWe will use, however data from the modeldata package where some preprocessing of the data has already been performed (see: https://www.tmwr.org/ames)\nThe dataset has 74 variables so a descriptive analysis is not provided.\n\ndim(ames)\n\n[1] 2930   74\n\nboxplot(ames)\n\n\n\n\n\n\n\n\nWe proceed as in the previous lab and divide the reponse variable by 1000 facilitate reviewing the results .\n\nrequire(dplyr)\names &lt;- ames %&gt;% mutate(Sale_Price = Sale_Price/1000)\nboxplot(ames)"
  },
  {
    "objectID": "labs/Lab-C.2.4b-BoostingOptimization.html#spliting-the-data-into-testtrain",
    "href": "labs/Lab-C.2.4b-BoostingOptimization.html#spliting-the-data-into-testtrain",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Spliting the data into test/train",
    "text": "Spliting the data into test/train\nThe data are split in separate test / training sets and do it in such a way that samplig is balanced for the response variable, Sale_Price.\n\n# Stratified sampling with the rsample package\nset.seed(123)\nsplit &lt;- rsample::initial_split(ames, prop = 0.7, \n                       strata = \"Sale_Price\")\names_train  &lt;- training(split)\names_test   &lt;- testing(split)"
  },
  {
    "objectID": "labs/Lab-C.2.4b-BoostingOptimization.html#tree-number",
    "href": "labs/Lab-C.2.4b-BoostingOptimization.html#tree-number",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Tree number",
    "text": "Tree number\nThis is a critical parameter as far as adding new trees increases risk of overfitting.\nBefore optimization is run, data is shaped into an object of class xgb.DMatrix, which is required to run XGBoost through this package.\n\names_train_num &lt;- model.matrix(Sale_Price ~ . , data = ames_train)[,-1]\names_test_num &lt;- model.matrix(Sale_Price ~ . , data = ames_test)[,-1]\n\ntrain_labels &lt;- ames_train$Sale_Price\ntest_labels &lt;- ames_test$Sale_Price\n\names_train_matrix &lt;- xgb.DMatrix(\n  data = ames_train_num,\n  label = train_labels\n)\n\names_test_matrix &lt;- xgb.DMatrix(\n  data = ames_test_num,\n  label = test_labels\n)\n\n\nboostResult_cv &lt;- xgb.cv(\n  data = ames_train_matrix,\n  params = list(eta = 0.3, max_depth = 6, subsample = 1, objective = \"reg:squarederror\"),\n  nrounds = 500,\n  nfold = 5,\n  metrics = \"rmse\", \n  verbose = 0\n)\n\nboostResult_cv &lt;- boostResult_cv$evaluation_log\nprint(boostResult_cv)\n\n      iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n     &lt;num&gt;           &lt;num&gt;          &lt;num&gt;          &lt;num&gt;         &lt;num&gt;\n  1:     1    1.412915e+02   0.9570592195      141.81926      3.756892\n  2:     2    1.019455e+02   0.7941089688      103.61188      3.395008\n  3:     3    7.430242e+01   0.7320607454       77.11700      3.556748\n  4:     4    5.479365e+01   0.6093311366       59.41119      3.735165\n  5:     5    4.118576e+01   0.5362387078       48.04453      4.272442\n ---                                                                  \n496:   496    7.383272e-03   0.0006022381       29.16183      4.817437\n497:   497    7.293321e-03   0.0005722715       29.16184      4.817432\n498:   498    7.164931e-03   0.0005468412       29.16184      4.817431\n499:   499    7.053888e-03   0.0005474267       29.16184      4.817429\n500:   500    6.977090e-03   0.0005256163       29.16187      4.817472\n\n\nWe aim at at the lowest number of trees that has associated a small cross-validation error.\n\nggplot(data = boostResult_cv) +\n  geom_line(aes(x = iter, y = train_rmse_mean, color = \"train rmse\")) + \n  geom_line(aes(x = iter, y = test_rmse_mean, color = \"cv rmse\")) +\n  geom_point(\n    data = slice_min(boostResult_cv, order_by = test_rmse_mean, n = 1),\n    aes(x = iter, y = test_rmse_mean),\n    color = \"firebrick\"\n  ) +\n  labs(\n    title = \"Evolution of cv-error vs number of trees\",\n    x     = \"number of trees\",\n    y     = \"cv-error (rmse)\",\n    color = \"\"\n  ) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\npaste(\"Optimal number of rounds (nrounds):\", slice_min(boostResult_cv, order_by = test_rmse_mean, n = 1)$iter)\n\n[1] \"Optimal number of rounds (nrounds): 77\""
  },
  {
    "objectID": "labs/Lab-C.2.4b-BoostingOptimization.html#learning-rate",
    "href": "labs/Lab-C.2.4b-BoostingOptimization.html#learning-rate",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Learning rate",
    "text": "Learning rate\nAlongside the number of trees, the learning rate (eta) is the most crucial hyperparameter in Gradient Boosting. It controls how quickly the model learns and thus influences the risk of overfitting.\nThese two hyperparameters are interdependent: a lower learning rate requires more trees to achieve good results but reduces the risk of overfitting.\n\n# Rango de valores para la tasa de aprendizaje (eta)\neta_range &lt;- c(0.001, 0.01, 0.1, 0.3)\ndf_results_cv &lt;- data.frame()\n\nfor (i in seq_along(eta_range)) {\n  set.seed(123)\n  \n  # Validación cruzada con el eta actual\n  results_cv &lt;- xgb.cv(\n    data = ames_train_matrix,  # ✅ Usamos el xgb.DMatrix correcto\n    params = list(\n      eta = eta_range[i], \n      max_depth = 6, \n      subsample = 1, \n      objective = \"reg:squarederror\"\n    ),\n    nrounds = 1000,\n    nfold = 5,\n    metrics = \"rmse\", \n    verbose = 0\n  )\n  \n  # Extraer la evaluación de RMSE y registrar resultados\n  results_cv &lt;- results_cv$evaluation_log\n  results_cv &lt;- results_cv %&gt;%\n    select(iter, test_rmse_mean) %&gt;%\n    mutate(eta = as.character(eta_range[i]))  # Guardamos el eta usado\n  \n  df_results_cv &lt;- df_results_cv %&gt;% bind_rows(results_cv)\n}\n\n\nggplot(data = df_results_cv) +\n  geom_line(aes(x = iter, y = test_rmse_mean, color = eta)) + \n  labs(\n    title = \"Evolución del error en validación cruzada vs tasa de aprendizaje (eta)\",\n    x = \"Número de iteraciones\",\n    y = \"Error RMSE en validación cruzada\",\n    color = \"Eta\"\n  ) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "labs/Lab-C.2.4b-BoostingOptimization.html#optimized-predictor",
    "href": "labs/Lab-C.2.4b-BoostingOptimization.html#optimized-predictor",
    "title": "Ensembles Lab 2: Boosting",
    "section": "Optimized predictor",
    "text": "Optimized predictor\nIn order to obtain improved predictors one can perform a a grid search for the best parameter combination can be performed.\n\n# Convertir variables categóricas a dummy variables usando model.matrix()\names_train_num &lt;- model.matrix(Sale_Price ~ . , data = ames_train)[,-1]\names_test_num &lt;- model.matrix(Sale_Price ~ . , data = ames_test)[,-1]\n\n# Extraer etiquetas de Sale_Price\ntrain_labels &lt;- ames_train$Sale_Price\ntest_labels &lt;- ames_test$Sale_Price\n\n# Convertir a xgb.DMatrix\names_train_matrix &lt;- xgb.DMatrix(\n  data = ames_train_num,\n  label = train_labels\n)\n\names_test_matrix &lt;- xgb.DMatrix(\n  data = ames_test_num,\n  label = test_labels\n)\n\n# Range of parameter values to test\neta_values &lt;- c(0.01, 0.05, 0.1, 0.3)  \nnrounds_values &lt;- c(500, 1000, 2000)  \n\nbest_rmse &lt;- Inf\nbest_params &lt;- list()\n\n\ncv_results_df &lt;- data.frame()\n\nset.seed(123)\n\nfor (eta in eta_values) {\n  for (nrounds in nrounds_values) {\n    \n    cv_results &lt;- xgb.cv(\n      data = ames_train_matrix,  \n      params = list(\n        eta = eta,\n        max_depth = 6,\n        subsample = 0.8,\n        colsample_bytree = 0.8,\n        objective = \"reg:squarederror\"\n      ),\n      nrounds = nrounds,\n      nfold = 5,\n      metrics = \"rmse\",\n      verbose = 0,\n      early_stopping_rounds = 10\n    )\n\n    if (is.null(cv_results)) next\n    \n    results_row &lt;- data.frame(\n      eta = eta,\n      nrounds = nrounds,\n      min_rmse = min(cv_results$evaluation_log$test_rmse_mean),\n      best_nrounds = cv_results$evaluation_log$iter[which.min(cv_results$evaluation_log$test_rmse_mean)]\n    )\n    \n    cv_results_df &lt;- bind_rows(cv_results_df, results_row)\n\n    if (results_row$min_rmse &lt; best_rmse) {\n      best_rmse &lt;- results_row$min_rmse\n      best_params &lt;- list(\n        eta = results_row$eta,\n        nrounds = results_row$best_nrounds\n      )\n    }\n  }\n}\n\n\ncat(\"\\n, Best hyperparameters values found:\\n\")\n\n\n, Best hyperparameters values found:\n\ncat(\"Eta:\", best_params$eta, \"\\n\")\n\nEta: 0.01 \n\ncat(\"Nrounds:\", best_params$nrounds, \"\\n\")\n\nNrounds: 1000 \n\ncat(\"RMSE mínimo:\", round(best_rmse, 4), \"\\n\")\n\nRMSE mínimo: 24.4476"
  },
  {
    "objectID": "labs/Lab-C1.2-knn4classification.html",
    "href": "labs/Lab-C1.2-knn4classification.html",
    "title": "k-NN for classification",
    "section": "",
    "text": "Introduction\nThe k-nearest neighbors (k-NN) classifier is a non-parametric method used for classification tasks. Given a new observation ( s ), the method assigns it the most frequent class among its ( k ) nearest neighbors in the training set.\nMathematically, the probability estimate of belonging to class 1 at point ( s ) is:\n\\[\n\\hat{P}_1(s) = \\frac{1}{k} \\sum_{i \\in N_k(s)} I(y_i = 1)\n\\]\nwhere: - ( N_k(s) ) represents the set of ( k ) nearest neighbors of ( s ), - ( I(y_i = 1) ) is an indicator function that equals 1 if the observation belongs to class 1 and 0 otherwise.\nThe predicted class is given by:\n\\[\n\\hat{y}(s) = \\arg\\max_{c} \\hat{P}_c(s)\n\\]\nwhere ( _c(s) ) is the estimated probability of class ( c ).\nIn this document we illustrate how the boundaries of the classifer are affected by the tuning parameter \\(k\\).\n\n\nLoading Data\nWe use a simulated dataset consisting of two-dimensional observations (features x and y) along with their class labels (BLUE / ORANGE).\n\ndf.xy &lt;- read.table(file=\"2clas2dim.csv\", dec=\".\", sep=\";\", header = TRUE)\ndim(df.xy)\n\n[1] 200   3\n\ndf.xy[sample(1:nrow(df.xy),10),]\n\n             x          y  class\n138 -0.2489676  1.2222646 ORANGE\n120  0.2662677  1.2212793 ORANGE\n97   0.7183153 -1.3513801   BLUE\n135  0.3463350  2.6248487 ORANGE\n72   1.0718618 -2.8478080   BLUE\n9    2.2078186  0.6543405   BLUE\n70   0.5070200  0.8164981   BLUE\n164  0.1541083  1.6869867 ORANGE\n191  0.2815986  0.8269676 ORANGE\n24   1.1204994 -0.8028946   BLUE\n\n\nNow let’s see how Knn works for one point\n\nk &lt;- 40\ns &lt;- 0\nt &lt;- 0\nst &lt;- c(s,t)\n\n# Compute distances between (s,t) and all data points\nd_st_xy &lt;- as.matrix(dist(rbind(st, df.xy[,1:2])))[1, -1]\n\n# Identify the k-th smallest distance\nd_st_xy_k &lt;- sort(d_st_xy, partial=k)[k]\n\n# Identify the indices of the k nearest neighbors\nN_st_k &lt;- unname(which(d_st_xy &lt;= d_st_xy_k))\n\n# Compute probability estimate of class 'ORANGE' at (s,t)\n(pr_1_k_st &lt;- sum(df.xy[N_st_k,3] == 'ORANGE') / k)\n\n[1] 0.25\n\n\nThis code: 1. Defines a test point ( (s,t) = (0,0) ). 2. Computes distances to all training points. 3. Selects the ( k ) nearest neighbors. 4. Estimates the probability of class ‘ORANGE’ at ( (s,t) ).\n\n\nVisualization of k-NN Neighborhood\n\nplot(df.xy[,1:2], col=df.xy$class, pch=1 + 18 * ((1:200) %in% N_st_k), asp=1)\npoints(s, t, pch=\"*\", col=3, cex=3)\n\n\n\n\n\n\n\n\nThis plot:\n\nDisplays the dataset with colors representing different classes.\nHighlights the selected neighbors of the test point ( (s,t) ).\n\n\n\nGeneralizing the k-NN Classifier\nThe process can be encapsulated in a function.\n\nknn.class &lt;- function(st, xy, group, k=3) {\n  d_st_xy &lt;- as.matrix(dist(rbind(st, xy)))[1, -1]\n  d_st_xy_k &lt;- sort(d_st_xy, partial=k)[k]\n  N_st_k &lt;- unname(which(d_st_xy &lt;= d_st_xy_k))\n  return(sum(group[N_st_k] == 1) / k)  # Probability of class 1\n}\n\nThis function: - Computes distances between a test point st and all training points xy. - Identifies the ( k ) nearest neighbors. - Estimates the probability of the test point belonging to class 1.\n\n\nUsing the k-NN Classifier\n\nst &lt;- c(0,0)\ngroup &lt;- as.numeric(df.xy[,3] == 'ORANGE')\nknn.class(st=st, xy=df.xy[,1:2], group=group, k=40)\n\n[1] 0.25\n\n\nThis code predicts the class probability for ( (0,0) ).\n\n\nCreating a Probability Map\n\ns &lt;- t &lt;- seq(-3.5, 3.5, by=.1)\nns &lt;- length(s)\nnt &lt;- length(t)\nhat_p &lt;- matrix(0, nrow=ns, ncol=nt)\n\nk &lt;- 50\n\nfor (i in 1:ns) {\n  for (j in 1:nt) {\n    hat_p[i, j] &lt;- knn.class(st=c(s[i], t[j]), xy=df.xy[,1:2], group=group, k=k)\n  }\n}\n\nThis block: - Defines a grid of test points in the feature space. - Computes class probabilities for each grid point using k-NN.\n\n\nDecision Boundary Visualization\n\nplot(df.xy[,1], df.xy[,2], col=df.xy[,3], asp=1)\ncontour(s, t, hat_p, levels=.5, lwd=2, add=TRUE)\n\n\n\n\n\n\n\n\nThis plot: - Shows the decision boundary where the class probability is 0.5.\n\n\nComparison with Logistic Regression\n\n# Fit logistic regression model\nglm.class &lt;- glm(group ~ x + y, data=df.xy, family=binomial)\nb012 &lt;- coefficients(glm.class)\n\n# Create a new plot to ensure abline() has a base plot\nplot(df.xy[,1], df.xy[,2], col=df.xy[,3], asp=1, main=\"Logistic Regression Decision Boundary\")\n\n# Add decision boundary line\nabline(a = -b012[1]/b012[3], b = -b012[2]/b012[3], lwd=2, col=6)\n\n\n\n\n\n\n\n\nThis fits a logistic regression model and plots its decision boundary for comparison.\n\n\nK-NN classifier is also a flexible classifier\nWe observe that: - k-NN produces a flexible decision boundary, adapting to the local structure of the data. - Larger ( k ) values create smoother boundaries, reducing variance. - Logistic regression assumes a linear boundary, which may not be suitable for non-linear patterns."
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "This example has been adapted from the book “Introduction to Statistical Learning with R”, lab 8.3.\nThe authors have decided to use the R tree package, which is not the most powerful R package for trees, but offers a good compromise between power and flexibility.\nThe lab relies on the Carseats dataset, a simulated dataset, that is included with the book’s package, containing several variables about sales of child car seats at different stores.\n\nrequire(ISLR2)\ndata(\"Carseats\")\n# help(\"Carseats\")\n\nA data frame with 400 observations on the following 11 variables.\n\nSales: Unit sales (in thousands) at each location\nCompPrice: Price charged by competitor at each location\nIncome: Community income level (in thousands of dollars)\nAdvertising: Local advertising budget for company at each location (in thousands of dollars)\nPopulation: Population size in region (in thousands)\nPrice: Price company charges for car seats at each site\nShelveLoc: A factor with levels Bad, Good and Medium indicating the quality of the shelving location for the car seats at each site\nAge: Average age of the local population\nEducation: Education level at each location\nUrban: A factor with levels No and Yes to indicate whether the store is in an urban or rural location\nUS: A factor with levels No and Yes to indicate whether the store is in the US or not\n\nThe first part of the lab will aim at predicting the variable sales.\nIn order to apply classification trees first, we start by categorizing the salesvariable. This is not usually seen as a good strategy, so take it only for didactical purpose.\n\n\n\nWe use a generic name for the dataset, in order to facilitate code reuse.\n\nmyDescription &lt;- \"The data are a simulated data set containing sales of child car seats at different stores [@james2013introduction]\"\nmydataset &lt;- Carseats\n\n\nn &lt;- nrow(mydataset)\np &lt;- ncol(mydataset)\n\nThere are 400 rows and 11 columns.\nThe variable Sales is categorized creating a new variable, High, which takes on a value of Yes if the Sales variable exceeds 8, and a value of No otherwise.\n\n# as.factor() changes the type of variable to factor\nmydataset$High=as.factor(ifelse(mydataset$Sales&lt;=8,\"No\",\"Yes\"))\n\nThe number of observations for each class is:\n\nkable(table(mydataset$High), caption= \"Number of observations for each class\", col.names = c('High','Freq'))\n\n\nNumber of observations for each class\n\n\nHigh\nFreq\n\n\n\n\nNo\n236\n\n\nYes\n164\n\n\n\n\n\nThe aim is of this study is to predict the categorical values of sales (High) using all variables but Sales.\nIt is a classification problem and we will build a classification tree model.\n\n\nThis is a short data set summary\n\nsummary(mydataset)\n\n     Sales          CompPrice       Income        Advertising    \n Min.   : 0.000   Min.   : 77   Min.   : 21.00   Min.   : 0.000  \n 1st Qu.: 5.390   1st Qu.:115   1st Qu.: 42.75   1st Qu.: 0.000  \n Median : 7.490   Median :125   Median : 69.00   Median : 5.000  \n Mean   : 7.496   Mean   :125   Mean   : 68.66   Mean   : 6.635  \n 3rd Qu.: 9.320   3rd Qu.:135   3rd Qu.: 91.00   3rd Qu.:12.000  \n Max.   :16.270   Max.   :175   Max.   :120.00   Max.   :29.000  \n   Population        Price        ShelveLoc        Age          Education   \n Min.   : 10.0   Min.   : 24.0   Bad   : 96   Min.   :25.00   Min.   :10.0  \n 1st Qu.:139.0   1st Qu.:100.0   Good  : 85   1st Qu.:39.75   1st Qu.:12.0  \n Median :272.0   Median :117.0   Medium:219   Median :54.50   Median :14.0  \n Mean   :264.8   Mean   :115.8                Mean   :53.32   Mean   :13.9  \n 3rd Qu.:398.5   3rd Qu.:131.0                3rd Qu.:66.00   3rd Qu.:16.0  \n Max.   :509.0   Max.   :191.0                Max.   :80.00   Max.   :18.0  \n Urban       US       High    \n No :118   No :142   No :236  \n Yes:282   Yes:258   Yes:164  \n                              \n                              \n                              \n                              \n\n\nAn improved description:\n\nskimr::skim(mydataset)\n\n\nData summary\n\n\nName\nmydataset\n\n\nNumber of rows\n400\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nShelveLoc\n0\n1\nFALSE\n3\nMed: 219, Bad: 96, Goo: 85\n\n\nUrban\n0\n1\nFALSE\n2\nYes: 282, No: 118\n\n\nUS\n0\n1\nFALSE\n2\nYes: 258, No: 142\n\n\nHigh\n0\n1\nFALSE\n2\nNo: 236, Yes: 164\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSales\n0\n1\n7.50\n2.82\n0\n5.39\n7.49\n9.32\n16.27\n▁▆▇▃▁\n\n\nCompPrice\n0\n1\n124.97\n15.33\n77\n115.00\n125.00\n135.00\n175.00\n▁▅▇▃▁\n\n\nIncome\n0\n1\n68.66\n27.99\n21\n42.75\n69.00\n91.00\n120.00\n▇▆▇▆▅\n\n\nAdvertising\n0\n1\n6.64\n6.65\n0\n0.00\n5.00\n12.00\n29.00\n▇▃▃▁▁\n\n\nPopulation\n0\n1\n264.84\n147.38\n10\n139.00\n272.00\n398.50\n509.00\n▇▇▇▇▇\n\n\nPrice\n0\n1\n115.80\n23.68\n24\n100.00\n117.00\n131.00\n191.00\n▁▂▇▆▁\n\n\nAge\n0\n1\n53.32\n16.20\n25\n39.75\n54.50\n66.00\n80.00\n▇▆▇▇▇\n\n\nEducation\n0\n1\n13.90\n2.62\n10\n12.00\n14.00\n16.00\n18.00\n▇▇▃▇▇\n\n\n\n\n\n\n\n\n\nIt is very common that the data need to be preprocessed before training the model*\nIn this case, there seem to be no missing values, no outliers and most variables are decently symmetrical, so no cleaning or preprocessing are required.\n\n\n\nIn order to properly evaluate the performance of a model, we must estimate the error rather than simply computing the training error.\nWith this aim in mind we proceed as follows:\n\nsplit the observations into a training set and a test set,\nbuild the model using the training set, and\nevaluate its performance on the test data.\n\n\nset.seed(2)\npt &lt;- 1/2\ntrain &lt;- sample(1:nrow(mydataset),pt*nrow(mydataset))\nmydataset.test &lt;- mydataset[-train,]\nHigh.test &lt;-  mydataset[-train,\"High\"]\n\nThe train and tets set have 200 200 observations respectively.\nIn train data, the number of observations for each class is:\n\nkableExtra::kable(table(mydataset[train,\"High\"]), caption= \"Train data: number of observations for each class\", col.names = c('High','Freq'))\n\n\nTrain data: number of observations for each class\n\n\nHigh\nFreq\n\n\n\n\nNo\n119\n\n\nYes\n81\n\n\n\n\n\n\n\n\nWe now use the tree() function to fit a classification tree in order to predict High using all variables but Sales using only de train set.\n\nlibrary(tree)\ntree.mydataset=tree(High~.-Sales, mydataset,\n                    subset=train, \n                    split=\"deviance\")\n\n\n\n\n\nThe summary() function lists the variables that are used as internal nodes in the tree, the number of terminal nodes, and the training error rate\n\nsummary(tree.mydataset)\n\n\nClassification tree:\ntree(formula = High ~ . - Sales, data = mydataset, subset = train, \n    split = \"deviance\")\nVariables actually used in tree construction:\n[1] \"Price\"       \"Population\"  \"ShelveLoc\"   \"Age\"         \"Education\"  \n[6] \"CompPrice\"   \"Advertising\" \"Income\"      \"US\"         \nNumber of terminal nodes:  21 \nResidual mean deviance:  0.5543 = 99.22 / 179 \nMisclassification error rate: 0.115 = 23 / 200 \n\n# summary(tree.mydataset2)\n\nFor classification trees the deviance of a tree (roughly equivalent to the concept of impurity) is defined as the sum over all terminal leaves of: \\[\n-2 \\sum_m \\sum_k n_{mk} log(\\hat{p}_{mk}),\n\\]\nwhere \\(n_{mk}\\) is the number of observations in the mth terminal node that belong to the kth class.\nThe residual mean deviance reported is simply the deviance divided by \\(n - |T_0|\\) where \\(T_0\\) is the number of terminal nodes.\n\n\n\nThe next step is display the tree graphically. We use the plot() function to display the tree structure, and the text()function to display the node labels.\n\nplot(tree.mydataset)\ntext(tree.mydataset,pretty=0, cex=0.6)\n\n\n\n\nClassification tree\n\n\n\n\nIt is also possible to show a R print output corresponding to each branch of the tree.\n\ntree.mydataset\n\nnode), split, n, deviance, yval, (yprob)\n      * denotes terminal node\n\n  1) root 200 270.000 No ( 0.59500 0.40500 )  \n    2) Price &lt; 96.5 40  47.050 Yes ( 0.27500 0.72500 )  \n      4) Population &lt; 414 31  40.320 Yes ( 0.35484 0.64516 )  \n        8) ShelveLoc: Bad,Medium 25  34.300 Yes ( 0.44000 0.56000 )  \n         16) Age &lt; 64.5 17  20.600 Yes ( 0.29412 0.70588 )  \n           32) Education &lt; 13.5 7   0.000 Yes ( 0.00000 1.00000 ) *\n           33) Education &gt; 13.5 10  13.860 Yes ( 0.50000 0.50000 )  \n             66) Education &lt; 16.5 5   5.004 No ( 0.80000 0.20000 ) *\n             67) Education &gt; 16.5 5   5.004 Yes ( 0.20000 0.80000 ) *\n         17) Age &gt; 64.5 8   8.997 No ( 0.75000 0.25000 ) *\n        9) ShelveLoc: Good 6   0.000 Yes ( 0.00000 1.00000 ) *\n      5) Population &gt; 414 9   0.000 Yes ( 0.00000 1.00000 ) *\n    3) Price &gt; 96.5 160 201.800 No ( 0.67500 0.32500 )  \n      6) ShelveLoc: Bad,Medium 135 154.500 No ( 0.74074 0.25926 )  \n       12) Price &lt; 124.5 82 107.700 No ( 0.63415 0.36585 )  \n         24) Age &lt; 49.5 34  45.230 Yes ( 0.38235 0.61765 )  \n           48) CompPrice &lt; 130.5 21  28.680 No ( 0.57143 0.42857 )  \n             96) Population &lt; 134.5 6   0.000 No ( 1.00000 0.00000 ) *\n             97) Population &gt; 134.5 15  20.190 Yes ( 0.40000 0.60000 )  \n              194) Population &lt; 343 7   5.742 Yes ( 0.14286 0.85714 ) *\n              195) Population &gt; 343 8  10.590 No ( 0.62500 0.37500 ) *\n           49) CompPrice &gt; 130.5 13   7.051 Yes ( 0.07692 0.92308 ) *\n         25) Age &gt; 49.5 48  46.330 No ( 0.81250 0.18750 )  \n           50) CompPrice &lt; 124.5 28  14.410 No ( 0.92857 0.07143 )  \n            100) Price &lt; 101.5 8   8.997 No ( 0.75000 0.25000 ) *\n            101) Price &gt; 101.5 20   0.000 No ( 1.00000 0.00000 ) *\n           51) CompPrice &gt; 124.5 20  25.900 No ( 0.65000 0.35000 )  \n            102) Price &lt; 119 14  19.410 No ( 0.50000 0.50000 )  \n              204) Advertising &lt; 10.5 9  11.460 No ( 0.66667 0.33333 ) *\n              205) Advertising &gt; 10.5 5   5.004 Yes ( 0.20000 0.80000 ) *\n            103) Price &gt; 119 6   0.000 No ( 1.00000 0.00000 ) *\n       13) Price &gt; 124.5 53  33.120 No ( 0.90566 0.09434 )  \n         26) Population &lt; 393.5 34   0.000 No ( 1.00000 0.00000 ) *\n         27) Population &gt; 393.5 19  21.900 No ( 0.73684 0.26316 )  \n           54) CompPrice &lt; 143.5 13   7.051 No ( 0.92308 0.07692 ) *\n           55) CompPrice &gt; 143.5 6   7.638 Yes ( 0.33333 0.66667 ) *\n      7) ShelveLoc: Good 25  31.340 Yes ( 0.32000 0.68000 )  \n       14) Income &lt; 43 7   8.376 No ( 0.71429 0.28571 ) *\n       15) Income &gt; 43 18  16.220 Yes ( 0.16667 0.83333 )  \n         30) US: No 6   8.318 Yes ( 0.50000 0.50000 ) *\n         31) US: Yes 12   0.000 Yes ( 0.00000 1.00000 ) *\n\n\n\n\n\nIn order to properly evaluate the performance of a classification tree on these data, we must estimate the test error rather than simply computing the training error.\nWe have split the observations into a training set and a test set, and the tree has been built using the training set.\nAfter this, the tree performance is evaluated on the test data. The predict() function can be used for this purpose.\n\ntree.pred=predict(tree.mydataset,mydataset.test,type=\"class\")\nres &lt;- table(tree.pred,High.test)\nres\n\n         High.test\ntree.pred  No Yes\n      No  104  33\n      Yes  13  50\n\naccrcy &lt;- sum(diag(res)/sum(res))\n\nThe accuracy is 0.77 or misclassification error rate is 0.23, which are respectively smaller and biiger than those computed from the tree built on the train data.\n\n\n\nWe know there is a chance that fitting the tree produces some overfitting so we can consider whether pruning the tree could lead to improved results.\nThe function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity. - Cost complexity pruning is used in order to select a sequence of trees for consideration. - We use the argument FUN = prune.misclass in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the cv.tree() function, which is deviance.\nThe cv.tree() function reports the number of terminal nodes of each tree considered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used\n\nset.seed(123987)\ncv.mydataset=cv.tree(tree.mydataset,FUN=prune.misclass)\nnames(cv.mydataset)\n\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n\ncv.mydataset\n\n$size\n[1] 21 19 14  9  8  5  3  2  1\n\n$dev\n[1] 82 80 78 78 78 76 76 84 82\n\n$k\n[1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n\nNote that, despite the name, dev corresponds to the cross-validation error rate in this instance.\nThe output shows how, as the size of the tree increases, so does the deviance.\nThis can be better visualized by plotting the error rate as a function of sizeand k.\n\npar(mfrow=c(1,2))\nplot(cv.mydataset$size,cv.mydataset$dev,type=\"b\")\nplot(cv.mydataset$k,cv.mydataset$dev,type=\"b\")\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nThese plots can be used to suggest the best tree, but it can also be chosen automatically by taking the minimal value \\(k\\) from the output of the cv.tree function.\n\nmyBest &lt;- cv.mydataset$size[which.min(cv.mydataset$dev)]\n\nNow, the prune.misclass() function can be used to prune the tree and obtain a “best tree”. If we decide to call the best tree the one that has reached the smallest deviance we can proceed as follows:\n\nprune.mydataset=prune.misclass(tree.mydataset,best=myBest)\n\n\nplot(prune.mydataset)\ntext(prune.mydataset,pretty=0)\n\nThe tree is clearly smaller than the original one, but how well does this pruned tree perform on the test data set?\n\nprunedTree.pred=predict(prune.mydataset,mydataset.test,type=\"class\")\nprunedRes &lt;- table(prunedTree.pred,High.test)\nprunedRes\n\n               High.test\nprunedTree.pred No Yes\n            No  82  16\n            Yes 35  67\n\nprunedAccrcy &lt;- sum(diag(prunedRes)/sum(prunedRes))\n\nThe accuracy is 0.745.\nIf we increase the value of best, for example 21 terminal nodes, we obtain a larger pruned tree with lower classification accuracy:\n\nprune.mydataset=prune.misclass(tree.mydataset, \n                               best = cv.mydataset$size[1])\nplot(prune.mydataset)\ntext(prune.mydataset, pretty=0)\n\n\n\n\nOther classification pruned tree\n\n\n\n\n\nptree.pred=predict(prune.mydataset, mydataset.test, type=\"class\")\npres &lt;- table(ptree.pred, High.test)\npres\n\n          High.test\nptree.pred  No Yes\n       No  104  31\n       Yes  13  52\n\npaccrcy &lt;- sum(diag(pres)/sum(pres))\n\nThe accuracy is 0.78.\nIn conclusion It can be seen that the difference in accuracy between the pruned tree and the original one is small. Indeed, changing the seed for splitting can lead to both smaller or bigger accuracy in the pruned tree than in the original one.\nObviously, the pruned tree is smaller so even if the original tree is slightly more accurate than the pruned one we might prefer the second one, because it is relying on less data to produce almost the same accuracy, whic is something that most users usually prefer.\n\n\n\nA reasonable question is how would the accuracy of the trees be affected if, instead of categorizing sales we had used it “as.is”, building a regression tree instead.\nAlthough it may seem straightforward to answer this question by building a regression tree using the approach described in next section, the fact is that it is no so immediate as it may seem.\nThe reason for this is that, if we wish to compare the perfomance of both approaches we need a common measure of accuracy. For regression trees the Mean Square Error is generally used, while accuracy or some other measures derived from the confusion matrix are common for classification trees. Comparing those two measures, however, is not straightforward. One may think of relying on some kind of information measure, that can be computed on both regresion and classification trees such as entropy or Kullback-Leiber divergence, but the problem then is how to derive such measure for both the classification and the regression trees."
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#introduction",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#introduction",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "This example has been adapted from the book “Introduction to Statistical Learning with R”, lab 8.3.\nThe authors have decided to use the R tree package, which is not the most powerful R package for trees, but offers a good compromise between power and flexibility.\nThe lab relies on the Carseats dataset, a simulated dataset, that is included with the book’s package, containing several variables about sales of child car seats at different stores.\n\nrequire(ISLR2)\ndata(\"Carseats\")\n# help(\"Carseats\")\n\nA data frame with 400 observations on the following 11 variables.\n\nSales: Unit sales (in thousands) at each location\nCompPrice: Price charged by competitor at each location\nIncome: Community income level (in thousands of dollars)\nAdvertising: Local advertising budget for company at each location (in thousands of dollars)\nPopulation: Population size in region (in thousands)\nPrice: Price company charges for car seats at each site\nShelveLoc: A factor with levels Bad, Good and Medium indicating the quality of the shelving location for the car seats at each site\nAge: Average age of the local population\nEducation: Education level at each location\nUrban: A factor with levels No and Yes to indicate whether the store is in an urban or rural location\nUS: A factor with levels No and Yes to indicate whether the store is in the US or not\n\nThe first part of the lab will aim at predicting the variable sales.\nIn order to apply classification trees first, we start by categorizing the salesvariable. This is not usually seen as a good strategy, so take it only for didactical purpose."
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#data-description",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#data-description",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "We use a generic name for the dataset, in order to facilitate code reuse.\n\nmyDescription &lt;- \"The data are a simulated data set containing sales of child car seats at different stores [@james2013introduction]\"\nmydataset &lt;- Carseats\n\n\nn &lt;- nrow(mydataset)\np &lt;- ncol(mydataset)\n\nThere are 400 rows and 11 columns.\nThe variable Sales is categorized creating a new variable, High, which takes on a value of Yes if the Sales variable exceeds 8, and a value of No otherwise.\n\n# as.factor() changes the type of variable to factor\nmydataset$High=as.factor(ifelse(mydataset$Sales&lt;=8,\"No\",\"Yes\"))\n\nThe number of observations for each class is:\n\nkable(table(mydataset$High), caption= \"Number of observations for each class\", col.names = c('High','Freq'))\n\n\nNumber of observations for each class\n\n\nHigh\nFreq\n\n\n\n\nNo\n236\n\n\nYes\n164\n\n\n\n\n\nThe aim is of this study is to predict the categorical values of sales (High) using all variables but Sales.\nIt is a classification problem and we will build a classification tree model.\n\n\nThis is a short data set summary\n\nsummary(mydataset)\n\n     Sales          CompPrice       Income        Advertising    \n Min.   : 0.000   Min.   : 77   Min.   : 21.00   Min.   : 0.000  \n 1st Qu.: 5.390   1st Qu.:115   1st Qu.: 42.75   1st Qu.: 0.000  \n Median : 7.490   Median :125   Median : 69.00   Median : 5.000  \n Mean   : 7.496   Mean   :125   Mean   : 68.66   Mean   : 6.635  \n 3rd Qu.: 9.320   3rd Qu.:135   3rd Qu.: 91.00   3rd Qu.:12.000  \n Max.   :16.270   Max.   :175   Max.   :120.00   Max.   :29.000  \n   Population        Price        ShelveLoc        Age          Education   \n Min.   : 10.0   Min.   : 24.0   Bad   : 96   Min.   :25.00   Min.   :10.0  \n 1st Qu.:139.0   1st Qu.:100.0   Good  : 85   1st Qu.:39.75   1st Qu.:12.0  \n Median :272.0   Median :117.0   Medium:219   Median :54.50   Median :14.0  \n Mean   :264.8   Mean   :115.8                Mean   :53.32   Mean   :13.9  \n 3rd Qu.:398.5   3rd Qu.:131.0                3rd Qu.:66.00   3rd Qu.:16.0  \n Max.   :509.0   Max.   :191.0                Max.   :80.00   Max.   :18.0  \n Urban       US       High    \n No :118   No :142   No :236  \n Yes:282   Yes:258   Yes:164  \n                              \n                              \n                              \n                              \n\n\nAn improved description:\n\nskimr::skim(mydataset)\n\n\nData summary\n\n\nName\nmydataset\n\n\nNumber of rows\n400\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nShelveLoc\n0\n1\nFALSE\n3\nMed: 219, Bad: 96, Goo: 85\n\n\nUrban\n0\n1\nFALSE\n2\nYes: 282, No: 118\n\n\nUS\n0\n1\nFALSE\n2\nYes: 258, No: 142\n\n\nHigh\n0\n1\nFALSE\n2\nNo: 236, Yes: 164\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSales\n0\n1\n7.50\n2.82\n0\n5.39\n7.49\n9.32\n16.27\n▁▆▇▃▁\n\n\nCompPrice\n0\n1\n124.97\n15.33\n77\n115.00\n125.00\n135.00\n175.00\n▁▅▇▃▁\n\n\nIncome\n0\n1\n68.66\n27.99\n21\n42.75\n69.00\n91.00\n120.00\n▇▆▇▆▅\n\n\nAdvertising\n0\n1\n6.64\n6.65\n0\n0.00\n5.00\n12.00\n29.00\n▇▃▃▁▁\n\n\nPopulation\n0\n1\n264.84\n147.38\n10\n139.00\n272.00\n398.50\n509.00\n▇▇▇▇▇\n\n\nPrice\n0\n1\n115.80\n23.68\n24\n100.00\n117.00\n131.00\n191.00\n▁▂▇▆▁\n\n\nAge\n0\n1\n53.32\n16.20\n25\n39.75\n54.50\n66.00\n80.00\n▇▆▇▇▇\n\n\nEducation\n0\n1\n13.90\n2.62\n10\n12.00\n14.00\n16.00\n18.00\n▇▇▃▇▇"
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#preprocess",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#preprocess",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "It is very common that the data need to be preprocessed before training the model*\nIn this case, there seem to be no missing values, no outliers and most variables are decently symmetrical, so no cleaning or preprocessing are required."
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#traintest-partition-of-data",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#traintest-partition-of-data",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "In order to properly evaluate the performance of a model, we must estimate the error rather than simply computing the training error.\nWith this aim in mind we proceed as follows:\n\nsplit the observations into a training set and a test set,\nbuild the model using the training set, and\nevaluate its performance on the test data.\n\n\nset.seed(2)\npt &lt;- 1/2\ntrain &lt;- sample(1:nrow(mydataset),pt*nrow(mydataset))\nmydataset.test &lt;- mydataset[-train,]\nHigh.test &lt;-  mydataset[-train,\"High\"]\n\nThe train and tets set have 200 200 observations respectively.\nIn train data, the number of observations for each class is:\n\nkableExtra::kable(table(mydataset[train,\"High\"]), caption= \"Train data: number of observations for each class\", col.names = c('High','Freq'))\n\n\nTrain data: number of observations for each class\n\n\nHigh\nFreq\n\n\n\n\nNo\n119\n\n\nYes\n81"
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#train-model",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#train-model",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "We now use the tree() function to fit a classification tree in order to predict High using all variables but Sales using only de train set.\n\nlibrary(tree)\ntree.mydataset=tree(High~.-Sales, mydataset,\n                    subset=train, \n                    split=\"deviance\")\n\n\n\n\n\nThe summary() function lists the variables that are used as internal nodes in the tree, the number of terminal nodes, and the training error rate\n\nsummary(tree.mydataset)\n\n\nClassification tree:\ntree(formula = High ~ . - Sales, data = mydataset, subset = train, \n    split = \"deviance\")\nVariables actually used in tree construction:\n[1] \"Price\"       \"Population\"  \"ShelveLoc\"   \"Age\"         \"Education\"  \n[6] \"CompPrice\"   \"Advertising\" \"Income\"      \"US\"         \nNumber of terminal nodes:  21 \nResidual mean deviance:  0.5543 = 99.22 / 179 \nMisclassification error rate: 0.115 = 23 / 200 \n\n# summary(tree.mydataset2)\n\nFor classification trees the deviance of a tree (roughly equivalent to the concept of impurity) is defined as the sum over all terminal leaves of: \\[\n-2 \\sum_m \\sum_k n_{mk} log(\\hat{p}_{mk}),\n\\]\nwhere \\(n_{mk}\\) is the number of observations in the mth terminal node that belong to the kth class.\nThe residual mean deviance reported is simply the deviance divided by \\(n - |T_0|\\) where \\(T_0\\) is the number of terminal nodes."
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#plot-the-tree",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#plot-the-tree",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "The next step is display the tree graphically. We use the plot() function to display the tree structure, and the text()function to display the node labels.\n\nplot(tree.mydataset)\ntext(tree.mydataset,pretty=0, cex=0.6)\n\n\n\n\nClassification tree\n\n\n\n\nIt is also possible to show a R print output corresponding to each branch of the tree.\n\ntree.mydataset\n\nnode), split, n, deviance, yval, (yprob)\n      * denotes terminal node\n\n  1) root 200 270.000 No ( 0.59500 0.40500 )  \n    2) Price &lt; 96.5 40  47.050 Yes ( 0.27500 0.72500 )  \n      4) Population &lt; 414 31  40.320 Yes ( 0.35484 0.64516 )  \n        8) ShelveLoc: Bad,Medium 25  34.300 Yes ( 0.44000 0.56000 )  \n         16) Age &lt; 64.5 17  20.600 Yes ( 0.29412 0.70588 )  \n           32) Education &lt; 13.5 7   0.000 Yes ( 0.00000 1.00000 ) *\n           33) Education &gt; 13.5 10  13.860 Yes ( 0.50000 0.50000 )  \n             66) Education &lt; 16.5 5   5.004 No ( 0.80000 0.20000 ) *\n             67) Education &gt; 16.5 5   5.004 Yes ( 0.20000 0.80000 ) *\n         17) Age &gt; 64.5 8   8.997 No ( 0.75000 0.25000 ) *\n        9) ShelveLoc: Good 6   0.000 Yes ( 0.00000 1.00000 ) *\n      5) Population &gt; 414 9   0.000 Yes ( 0.00000 1.00000 ) *\n    3) Price &gt; 96.5 160 201.800 No ( 0.67500 0.32500 )  \n      6) ShelveLoc: Bad,Medium 135 154.500 No ( 0.74074 0.25926 )  \n       12) Price &lt; 124.5 82 107.700 No ( 0.63415 0.36585 )  \n         24) Age &lt; 49.5 34  45.230 Yes ( 0.38235 0.61765 )  \n           48) CompPrice &lt; 130.5 21  28.680 No ( 0.57143 0.42857 )  \n             96) Population &lt; 134.5 6   0.000 No ( 1.00000 0.00000 ) *\n             97) Population &gt; 134.5 15  20.190 Yes ( 0.40000 0.60000 )  \n              194) Population &lt; 343 7   5.742 Yes ( 0.14286 0.85714 ) *\n              195) Population &gt; 343 8  10.590 No ( 0.62500 0.37500 ) *\n           49) CompPrice &gt; 130.5 13   7.051 Yes ( 0.07692 0.92308 ) *\n         25) Age &gt; 49.5 48  46.330 No ( 0.81250 0.18750 )  \n           50) CompPrice &lt; 124.5 28  14.410 No ( 0.92857 0.07143 )  \n            100) Price &lt; 101.5 8   8.997 No ( 0.75000 0.25000 ) *\n            101) Price &gt; 101.5 20   0.000 No ( 1.00000 0.00000 ) *\n           51) CompPrice &gt; 124.5 20  25.900 No ( 0.65000 0.35000 )  \n            102) Price &lt; 119 14  19.410 No ( 0.50000 0.50000 )  \n              204) Advertising &lt; 10.5 9  11.460 No ( 0.66667 0.33333 ) *\n              205) Advertising &gt; 10.5 5   5.004 Yes ( 0.20000 0.80000 ) *\n            103) Price &gt; 119 6   0.000 No ( 1.00000 0.00000 ) *\n       13) Price &gt; 124.5 53  33.120 No ( 0.90566 0.09434 )  \n         26) Population &lt; 393.5 34   0.000 No ( 1.00000 0.00000 ) *\n         27) Population &gt; 393.5 19  21.900 No ( 0.73684 0.26316 )  \n           54) CompPrice &lt; 143.5 13   7.051 No ( 0.92308 0.07692 ) *\n           55) CompPrice &gt; 143.5 6   7.638 Yes ( 0.33333 0.66667 ) *\n      7) ShelveLoc: Good 25  31.340 Yes ( 0.32000 0.68000 )  \n       14) Income &lt; 43 7   8.376 No ( 0.71429 0.28571 ) *\n       15) Income &gt; 43 18  16.220 Yes ( 0.16667 0.83333 )  \n         30) US: No 6   8.318 Yes ( 0.50000 0.50000 ) *\n         31) US: Yes 12   0.000 Yes ( 0.00000 1.00000 ) *"
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#prediction",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#prediction",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "In order to properly evaluate the performance of a classification tree on these data, we must estimate the test error rather than simply computing the training error.\nWe have split the observations into a training set and a test set, and the tree has been built using the training set.\nAfter this, the tree performance is evaluated on the test data. The predict() function can be used for this purpose.\n\ntree.pred=predict(tree.mydataset,mydataset.test,type=\"class\")\nres &lt;- table(tree.pred,High.test)\nres\n\n         High.test\ntree.pred  No Yes\n      No  104  33\n      Yes  13  50\n\naccrcy &lt;- sum(diag(res)/sum(res))\n\nThe accuracy is 0.77 or misclassification error rate is 0.23, which are respectively smaller and biiger than those computed from the tree built on the train data."
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#pruning-the-tree-tunning-model",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#pruning-the-tree-tunning-model",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "We know there is a chance that fitting the tree produces some overfitting so we can consider whether pruning the tree could lead to improved results.\nThe function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity. - Cost complexity pruning is used in order to select a sequence of trees for consideration. - We use the argument FUN = prune.misclass in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the cv.tree() function, which is deviance.\nThe cv.tree() function reports the number of terminal nodes of each tree considered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used\n\nset.seed(123987)\ncv.mydataset=cv.tree(tree.mydataset,FUN=prune.misclass)\nnames(cv.mydataset)\n\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n\ncv.mydataset\n\n$size\n[1] 21 19 14  9  8  5  3  2  1\n\n$dev\n[1] 82 80 78 78 78 76 76 84 82\n\n$k\n[1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n\nNote that, despite the name, dev corresponds to the cross-validation error rate in this instance.\nThe output shows how, as the size of the tree increases, so does the deviance.\nThis can be better visualized by plotting the error rate as a function of sizeand k.\n\npar(mfrow=c(1,2))\nplot(cv.mydataset$size,cv.mydataset$dev,type=\"b\")\nplot(cv.mydataset$k,cv.mydataset$dev,type=\"b\")\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nThese plots can be used to suggest the best tree, but it can also be chosen automatically by taking the minimal value \\(k\\) from the output of the cv.tree function.\n\nmyBest &lt;- cv.mydataset$size[which.min(cv.mydataset$dev)]\n\nNow, the prune.misclass() function can be used to prune the tree and obtain a “best tree”. If we decide to call the best tree the one that has reached the smallest deviance we can proceed as follows:\n\nprune.mydataset=prune.misclass(tree.mydataset,best=myBest)\n\n\nplot(prune.mydataset)\ntext(prune.mydataset,pretty=0)\n\nThe tree is clearly smaller than the original one, but how well does this pruned tree perform on the test data set?\n\nprunedTree.pred=predict(prune.mydataset,mydataset.test,type=\"class\")\nprunedRes &lt;- table(prunedTree.pred,High.test)\nprunedRes\n\n               High.test\nprunedTree.pred No Yes\n            No  82  16\n            Yes 35  67\n\nprunedAccrcy &lt;- sum(diag(prunedRes)/sum(prunedRes))\n\nThe accuracy is 0.745.\nIf we increase the value of best, for example 21 terminal nodes, we obtain a larger pruned tree with lower classification accuracy:\n\nprune.mydataset=prune.misclass(tree.mydataset, \n                               best = cv.mydataset$size[1])\nplot(prune.mydataset)\ntext(prune.mydataset, pretty=0)\n\n\n\n\nOther classification pruned tree\n\n\n\n\n\nptree.pred=predict(prune.mydataset, mydataset.test, type=\"class\")\npres &lt;- table(ptree.pred, High.test)\npres\n\n          High.test\nptree.pred  No Yes\n       No  104  31\n       Yes  13  52\n\npaccrcy &lt;- sum(diag(pres)/sum(pres))\n\nThe accuracy is 0.78.\nIn conclusion It can be seen that the difference in accuracy between the pruned tree and the original one is small. Indeed, changing the seed for splitting can lead to both smaller or bigger accuracy in the pruned tree than in the original one.\nObviously, the pruned tree is smaller so even if the original tree is slightly more accurate than the pruned one we might prefer the second one, because it is relying on less data to produce almost the same accuracy, whic is something that most users usually prefer."
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#predicting-car-sales-with-regression-trees",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#predicting-car-sales-with-regression-trees",
    "title": "Decision Trees Lab 1",
    "section": "",
    "text": "A reasonable question is how would the accuracy of the trees be affected if, instead of categorizing sales we had used it “as.is”, building a regression tree instead.\nAlthough it may seem straightforward to answer this question by building a regression tree using the approach described in next section, the fact is that it is no so immediate as it may seem.\nThe reason for this is that, if we wish to compare the perfomance of both approaches we need a common measure of accuracy. For regression trees the Mean Square Error is generally used, while accuracy or some other measures derived from the confusion matrix are common for classification trees. Comparing those two measures, however, is not straightforward. One may think of relying on some kind of information measure, that can be computed on both regresion and classification trees such as entropy or Kullback-Leiber divergence, but the problem then is how to derive such measure for both the classification and the regression trees."
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#the-car-sales-problem-again",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#the-car-sales-problem-again",
    "title": "Decision Trees Lab 1",
    "section": "The Car Sales problem (again)",
    "text": "The Car Sales problem (again)\nEven if we do not aim at comparing regression and classification problems, the carseats problem proivides a good example on how to build and optimize a regression tree.\nRemember our goal is to predict car sales from a simulated data set containing sales of child car seats at different stores [@james2013introduction]. In order to make sections reproducible, we reload the package and the data.\n\nGet the Data\n\nrequire(ISLR2)\ndata(\"Carseats\")\nmydataset &lt;- Carseats\n\n\n\nCreate train/test sets\nWe split original data into test and training sets. Package resample allows to do a weighted splitting to enbsure that no class is underrepresented due to chance. If sample size is high this can usually be ignored.\n\n# Split the data into training and test sets\nset.seed(2)\npt &lt;- 1/2\ntrain &lt;- sample(1:nrow(mydataset), pt * nrow(mydataset))\nmydataset.test &lt;- mydataset[-train,]\nsales.test &lt;- mydataset$Sales[-train]\n\n\n\nBuild (and check) the model\n\n# Fit the regression tree using the Sales variable\n\ntree.mydataset &lt;- tree(Sales ~ . , mydataset,\n                       subset = train)\n\n# Summary of the fitted regression tree\nsummary(tree.mydataset)\n\n\nRegression tree:\ntree(formula = Sales ~ ., data = mydataset, subset = train)\nVariables actually used in tree construction:\n[1] \"Price\"       \"ShelveLoc\"   \"CompPrice\"   \"Age\"         \"Advertising\"\n[6] \"Population\" \nNumber of terminal nodes:  14 \nResidual mean deviance:  2.602 = 484 / 186 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-4.71700 -1.08700 -0.01026  0.00000  1.11300  4.06600 \n\n\n\n# Plot the regression tree\nplot(tree.mydataset)\ntext(tree.mydataset, pretty = 0, cex = 0.6)\n\n\n\n\n\n\n\n\n\n\nMake prediction\n\n# Predict using the test data\ntree.pred &lt;- predict(tree.mydataset, mydataset.test)\n\n\n\nEstimate prediction error\nA common measure of prediction error is the Mean Square Error.\nNotice that it is computed from a direct substraction between the predicted sales and the original values in the test subset.\n\nmse1 &lt;- mean((tree.pred - sales.test)^2)\nmse1\n\n[1] 4.471569\n\n\nThe mean squared error obtained from the original tree is 4.4715694.\n\n\nOptimize the tree\nIn order to optimize the trune we first compute the best cost-complexity parameter using cross-validation and then use it to prune the tree.\n\n# Prune the regression tree\nset.seed(123987)\ncv.mydataset &lt;- cv.tree(tree.mydataset, FUN = prune.tree)\nnames(cv.mydataset)\n\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n\ncv.mydataset\n\n$size\n [1] 14 13 12 11 10  9  8  7  6  4  3  2  1\n\n$dev\n [1] 1146.347 1178.392 1178.275 1201.676 1239.316 1217.896 1242.089 1253.068\n [9] 1202.806 1211.749 1206.363 1295.017 1578.720\n\n$k\n [1]      -Inf  16.92509  19.38585  23.44178  29.89370  36.28493  50.16562\n [8]  54.84825  65.75957  80.79945  90.11022 179.77305 277.78708\n\n$method\n[1] \"deviance\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n\nBefore selecting the best \\(\\alpha\\) value it may be useful to plot the MSE as a function of the tree size or of \\(\\alpha\\) itself. Notice that \\(\\alpha\\) is named as “\\(k\\)” in the tree package.\n\n# Plot the cross-validation error\npar(mfrow = c(1, 2))\nplot(cv.mydataset$size, cv.mydataset$dev, type = \"b\")\nplot(cv.mydataset$k, cv.mydataset$dev, type = \"b\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\nIt seems clear that, in this case, the smallest error is attained when the tree is not pruned (size=14), so the “best” value of \\(\\alpha\\) leads to not pruning the tree.\n\n# Choose the best tree size\nmyBest &lt;- cv.mydataset$size[which.min(cv.mydataset$dev)]\n\n# Prune the tree with the best size\nprune.mydataset &lt;- prune.tree(tree.mydataset, \n                              best = myBest)\n\n# Plot the pruned regression tree\nplot(prune.mydataset)\ntext(prune.mydataset, pretty = 0)\n\n\n\n\n\n\n\n# Predict using the pruned tree\nprunedTree.pred &lt;- predict(prune.mydataset, mydataset.test)\n\n# Calculate mean squared error for pruned tree\nprunedMSE &lt;- mean((prunedTree.pred - sales.test)^2)\nprunedMSE\n\n[1] 4.471569\n\n\nIn this case, pruning does not improve the tree and the best tree is the one returned by the initial tun of the algorithm.\nIf however, we look for a compromise between the tree size and the deviance we can choose, based on the cv plots, a size of 6 or even 3:\n\n# Prune the tree with the best size\npruneto5.mydataset &lt;- prune.tree(tree.mydataset, \n                              best = 6)\n\n# Plot the pruned regression tree\nplot(pruneto5.mydataset)\ntext(pruneto5.mydataset, pretty = 0)\n\n\n\n\n\n\n\n# Predict using the pruned tree\nprunedTree5.pred &lt;- predict(pruneto5.mydataset, mydataset.test)\n\n# Calculate mean squared error for pruned tree\nprunedMSE5 &lt;- mean((prunedTree5.pred - sales.test)^2)\nprunedMSE5\n\n[1] 5.001169\n\n\n\n# Prune the tree with the best size\npruneto3.mydataset &lt;- prune.tree(tree.mydataset, \n                              best = 3)\n\n# Plot the pruned regression tree\nplot(pruneto3.mydataset)\ntext(pruneto3.mydataset, pretty = 0)\n\n\n\n\n\n\n\n# Predict using the pruned tree\nprunedTree3.pred &lt;- predict(pruneto3.mydataset, mydataset.test)\n\n# Calculate mean squared error for pruned tree\nprunedMSE3 &lt;- mean((prunedTree3.pred - sales.test)^2)\nprunedMSE3\n\n[1] 6.555128\n\n\nClearly, the best compromise seems to prune with a size of 5, which hardly increases the MSE while providinga good simplification of the tree"
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#predicting-boston-house-prices",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#predicting-boston-house-prices",
    "title": "Decision Trees Lab 1",
    "section": "Predicting Boston house prices",
    "text": "Predicting Boston house prices\nThis example is borrowed from [@amat2017].\nThe Boston dataset available in the MASS package contains housing prices for the city of Boston, as well as socioeconomic information for the neighborhood in which they are located.\n\nlibrary(ISLR2)\ndata(\"Boston\")\ndatos &lt;- Boston\nhead(datos, 3)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat medv\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98 24.0\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14 21.6\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03 34.7\n\n\nOur goal is to fit a regression model that allows predicting the average price of a home (medv) based on the available variables.\nA quick visualization of the available variables shows that, not only they are of mixed types, but also the relation between them is far from linear inmost if not all cases.\n\ncolor &lt;- adjustcolor(\"forestgreen\", alpha.f = 0.5)\nps &lt;- function(x, y, ...) {  # custom panel function\n  panel.smooth(x, y, col = color, col.smooth = \"black\", \n               cex = 0.7, lwd = 2)\n}\nnc&lt;- ncol(datos)\npairs(datos[,c(1:6,nc)], cex = 0.7, upper.panel = ps, col = color)\n\n\n\n\n\n\n\n# pairs(datos[,c(7:14)], cex = 0.7, upper.panel = ps, col = color)\n\nThis is a good scenario to consider regression trees as a good option.\n\nModel fitting\nCreate a train and test sets\n\nset.seed(123)\ntrain &lt;- sample(1:nrow(datos), size = nrow(datos)/2)\ndatos_train &lt;- datos[train,]\ndatos_test  &lt;- datos[-train,]\n\nWe use the tree function of the tree package to build the model. This function grows the tree until it meets a stop condition. By default, these conditions are:\n\nmincut: minimum number of observations that at least one of the child nodes must have for the division to occur.\nminsize: minimum number of observations a node must have in order for it to be split.\n\n\nset.seed(123)\nregTree&lt;- tree::tree(\n                    formula = medv ~ .,\n                    data    = datos_train,\n                    split   = \"deviance\",\n                    mincut  = 20,\n                    minsize = 50\n                  )\nsummary(regTree)\n\n\nRegression tree:\ntree::tree(formula = medv ~ ., data = datos_train, split = \"deviance\", \n    mincut = 20, minsize = 50)\nVariables actually used in tree construction:\n[1] \"rm\"    \"lstat\" \"dis\"   \"tax\"  \nNumber of terminal nodes:  6 \nResidual mean deviance:  20.56 = 5078 / 247 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-14.5500  -2.8680  -0.3628   0.0000   2.0050  22.1300 \n\n\nThe summary shows that the trained tree has a total of 6 terminal nodes and that the variables rm, lstat, dis and tax have been used as predictors.\nIn the context of regression trees, the Residual mean deviance term is the residual sum of squares divided by (number of observations - number of terminal nodes). The smaller the deviance, the better the fit of the tree to the training observations.\nThe tree can be visualized:\n\npar(mar = c(1,1,1,1))\nplot(x = regTree, type = \"proportional\")\ntext(x = regTree, splits = TRUE, pretty = 0, cex = 0.8, col = \"firebrick\")\n\n\n\n\n\n\n\n\n\n\nPrunning the tree\nWe use the cv.tree function that uses cross validation to identify the optimal penalty value. By default, this function relies on the deviance to guide the pruning process.\nWe grow the tree again with less restrictive parameters so we have a big tree to prune:\n\nregTree2&lt;- tree::tree(\n                    formula = medv ~ .,\n                    data    = datos_train,\n                    split   = \"deviance\",\n                    mincut  = 1,\n                    minsize = 2,\n                    mindev  = 0\n                  )\n\n\nset.seed(123)\ncv_regTree2 &lt;- tree::cv.tree(regTree2, K = 5)\n\nThe function returns an object cv_regTree2 containing:\n\nsize: The size (number of terminal nodes) of each tree.\ndev: The cross-validation test error estimate for each tree size.\nk: The range of penalty values \\(\\alpha\\) evaluated.\nmethod: The criteria used to select the best tree.\n\nThese can be used to visualize and understand the optimization performed.\n\noptSize &lt;- rev(cv_regTree2$size)[which.min(rev(cv_regTree2$dev))]\npaste(\"Optimal size obtained is:\", optSize)\n\n[1] \"Optimal size obtained is: 10\"\n\n\n\nlibrary(ggplot2)\nlibrary(ggpubr)\n\n\nresultados_cv &lt;- data.frame(\n                   n_nodes  = cv_regTree2$size,\n                   deviance = cv_regTree2$dev,\n                   alpha    = cv_regTree2$k\n                 )\n\np1 &lt;- ggplot(data = resultados_cv, aes(x = n_nodes, y = deviance)) +\n      geom_line() + \n      geom_point() +\n      geom_vline(xintercept = optSize, color = \"red\") +\n      labs(title = \"Error vs tree size\") +\n      theme_bw() \n  \np2 &lt;- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +\n      geom_line() + \n      geom_point() +\n      labs(title = \"Error vs penalization (alpha)\") +\n      theme_bw() \n\nggarrange(p1, p2)\n\n\n\n\n\n\n\n\nOnce the optimal value identified, the final pruning is applied with the prune.tree function. This function also accepts the optimal value of \\(\\alpha\\) instead of size.\n\nfinalTree &lt;- tree::prune.tree(\n                  tree = regTree2,\n                  best = optSize\n               )\n\npar(mar = c(1,1,1,1))\nplot(x = finalTree, type = \"proportional\")\ntext(x = finalTree, splits = TRUE, pretty = 0, cex = 0.8, col = \"firebrick\")\n\n\n\n\n\n\n\n\n\n\nPredicting and checking model accuracy\nWe can use both, original and pruned trees to predict the data for the test set.\nThe quality of the prediction is based in the Root Mean Square.\nFor the original tree one has:\n\npredicciones &lt;- predict(regTree, newdata = datos_test)\ntest_rmse    &lt;- sqrt(mean((predicciones - datos_test$medv)^2))\npaste(\"Error de test (rmse) del árbol inicial:\", round(test_rmse,2))\n\n[1] \"Error de test (rmse) del árbol inicial: 5.74\"\n\n\nAnd for the final tree:\n\npredicciones_finales &lt;- predict(finalTree, newdata = datos_test)\ntest_rmse    &lt;- sqrt(mean((predicciones_finales - datos_test$medv)^2))\npaste(\"Error de test (rmse) del árbol final:\", round(test_rmse,2))\n\n[1] \"Error de test (rmse) del árbol final: 5.13\"\n\n\nThat is The error associated with the prediction has slightly decreased, while the tree is much simpler.\nThat is what we ideal are aiming at!"
  },
  {
    "objectID": "labs/Lab-C2.2-Decision_Trees-1.html#comparison-between-caret-rpart-and-tree",
    "href": "labs/Lab-C2.2-Decision_Trees-1.html#comparison-between-caret-rpart-and-tree",
    "title": "Decision Trees Lab 1",
    "section": "Comparison between caret, rpart, and tree",
    "text": "Comparison between caret, rpart, and tree\nTwo popular packages for working with decision trees are rpart and tree. Both offer functionalities for building and visualizing decision trees. The table below shows a comparison between the main functions of these packages, as well as caret, which is a generic framework for performing classification and prediction tasks, including trees.\n\nTable: Comparison of important functions for working with decision trees\n\n\n\n\n\n\n\n\n\nFunction / Package\ntree\nrpart\ncaret\n\n\n\n\nBuilding Decision Tree\ntree()\nrpart()\ntrain() with method = “rpart”\n\n\nVisualizing Decision Tree\n-\nplot()\nplot() with type = “text”\n\n\nPruning Decision Tree\ncv.tree()\nprune()\ntrain() with method = “rpart” and tuneLength &gt; 1\n\n\nEvaluating Model Performance\n-\npredict()\ntrain() with method = “rpart” and metric = “Accuracy”\n\n\nHandling Missing Values\nna.action\nna.action\npreProcess() with method = “medianImpute”\n\n\nTuning Hyperparameters\n-\nrpart.control()\ntrain() with method = “rpart” and tuneGrid argument\n\n\nVisualizing Variable Importance\n-\nimportance()\nvarImp()\n\n\n\n\n\nExamples of usage:\n\n\n\n\n\n\n\n\n\nFunction / Package\ntree\nrpart\ncaret\n\n\n\n\nBuilding Decision Tree\ntree(Species ~ ., data = iris)\nrpart(Species ~ ., data = iris)\ntrain(Species ~ ., method = \"rpart\", data = iris)\n\n\nVisualizing Decision Tree\n-\nplot(fit)\nplot(fit, type = \"text\")\n\n\nPruning Decision Tree\ncv.tree(Species ~ ., data = iris)\nprune(fit, cp = 0.02)\ntrain(Species ~ ., method = \"rpart\", data = iris, tuneLength = 5)\n\n\nEvaluating Model Performance\n-\npred &lt;- predict(fit, iris, type = \"class\")\ntrain(Species ~ ., method = \"rpart\", data = iris, metric = \"Accuracy\")\n\n\nHandling Missing Values\ntree(Species ~ ., data = na.omit(iris))\nrpart(Species ~ ., data = na.omit(iris), na.action = na.rpart)\npreProcess(iris, method = \"medianImpute\")\n\n\nTuning Hyperparameters\n-\nrpart(Species ~ ., data = iris, control = rpart.control(cp = c(0.001, 0.01, 0.1)))\ntrain(Species ~ ., method = \"rpart\", data = iris, tuneGrid = expand.grid(cp = c(0.001, 0.01, 0.1)))\n\n\nVisualizing Variable Importance\n-\nimportance(fit)\nvarImp(fit)\n\n\n\nThese examples illustrate how to perform various tasks related to decision trees using the tree, rpart, and caret packages. Each package has its own syntax and set of functions, so they can be used according to the user’s needs and preferences."
  },
  {
    "objectID": "labs/Lab-C2.3-R-ISLRch8-baggboost..html",
    "href": "labs/Lab-C2.3-R-ISLRch8-baggboost..html",
    "title": "Lab: Tree-based methods",
    "section": "",
    "text": "This lab presents some examples of applying shallow and ensemble-based trees.\nIt is adapted directly from lab 8 in “Introduction to Statistical Learning with R examples”, 2nd edition, and it can be run in parallel in Python or in R using the respective notebooks for each language downloadable from the book’s website.\n\n\n\nif(!require(tree)) install.packages(\"tree\", dep=TRUE)\nif(!require(ISLR2)) install.packages(\"ISLR2\", dep=TRUE)"
  },
  {
    "objectID": "labs/Lab-C2.3-R-ISLRch8-baggboost..html#install-load-required-libraries",
    "href": "labs/Lab-C2.3-R-ISLRch8-baggboost..html#install-load-required-libraries",
    "title": "Lab: Tree-based methods",
    "section": "",
    "text": "if(!require(tree)) install.packages(\"tree\", dep=TRUE)\nif(!require(ISLR2)) install.packages(\"ISLR2\", dep=TRUE)"
  }
]