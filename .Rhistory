split <- rsample::initial_split(ames, prop = 0.7,
strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
# Chunk 8
library(tree)
ctlPars <-tree.control(nobs =nrow(ames_train),
mincut = 5,
minsize = 10,
mindev = 0.01)
ames_rt1 <-  tree::tree(
formula = Sale_Price ~ .,
data    = ames_train,
split   = "deviance",
control = ctlPars)
summary(ames_rt1)
# Chunk 9: plotAmes1
plot(x = ames_rt1, type = "proportional")
text(x = ames_rt1, splits = TRUE, pretty = 0, cex = 0.6, col = "firebrick")
# Chunk 10
ames_rt1bis <- rpart(
formula = Sale_Price ~ .,
data    = ames_train,
method  = "anova",
control = ctlPars
)
library(rpart.plot)
rpart.plot(ames_rt1bis)
# Chunk 11: pruneAmes1
set.seed(123)
cv_ames_rt1 <- tree::cv.tree(ames_rt1, K = 5)
optSize <- rev(cv_ames_rt1$size)[which.min(rev(cv_ames_rt1$dev))]
paste("Optimal size obtained is:", optSize)
# Chunk 12: plotPruneAmes
library(ggplot2)
library(ggpubr)
resultados_cv <- data.frame(
n_nodes  = cv_ames_rt1$size,
deviance = cv_ames_rt1$dev,
alpha    = cv_ames_rt1$k
)
p1 <- ggplot(data = resultados_cv, aes(x = n_nodes, y = deviance)) +
geom_line() +
geom_point() +
geom_vline(xintercept = optSize, color = "red") +
labs(title = "Error vs tree size") +
theme_bw()
p2 <- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +
geom_line() +
geom_point() +
labs(title = "Error vs penalization (alpha)") +
theme_bw()
ggarrange(p1, p2)
# Chunk 13: fitAmesRT2
ctlPars2 <-tree.control(nobs=nrow(ames_train), mincut = 1, minsize = 2, mindev = 0)
ames_rt2 <-  tree::tree(
formula = Sale_Price ~ .,
data    = ames_train,
split   = "deviance",
control = ctlPars2)
summary(ames_rt2)
# Chunk 14: pruneAmes2
set.seed(123)
cv_ames_rt2 <- tree::cv.tree(ames_rt2, K = 5)
optSize2 <- rev(cv_ames_rt2$size)[which.min(rev(cv_ames_rt2$dev))]
paste("Optimal size obtained is:", optSize2)
# Chunk 15
prunedTree2 <- tree::prune.tree(
tree = ames_rt2,
best = optSize2
)
summary(prunedTree2)
# Chunk 16
res_cv2 <- data.frame(
n_nodes  = cv_ames_rt2$size,
deviance = cv_ames_rt2$dev,
alpha    = cv_ames_rt2$k
)
p1 <- ggplot(data = res_cv2, aes(x = n_nodes, y = deviance)) +
geom_line() +
geom_point() +
geom_vline(xintercept = optSize2, color = "red") +
labs(title = "Error vs tree size") +
theme_bw()
p2 <- ggplot(data = res_cv2, aes(x = alpha, y = deviance)) +
geom_line() +
geom_point() +
labs(title = "Error vs penalization (alpha)") +
theme_bw()
ggarrange(p1, p2)
# Chunk 17: ames_rt_pred1
ames_rt_pred1 <- predict(ames_rt1, newdata = ames_test)
test_rmse1    <- sqrt(mean((ames_rt_pred1 - ames_test$Sale_Price)^2))
paste("Error test (rmse) for initial tree:", round(test_rmse1,2))
# Chunk 18: ames_rt_pred2
ames_rt_pred2 <- predict(ames_rt2, newdata = ames_test)
test_rmse2    <- sqrt(mean((ames_rt_pred2 - ames_test$Sale_Price)^2))
paste("Error test (rmse) for big tree:", round(test_rmse2,2))
# Chunk 19: ames_pruned_pred
ames_pruned_pred <- predict(prunedTree2, newdata = ames_test)
test_rmse3    <- sqrt(mean((ames_pruned_pred - ames_test$Sale_Price)^2))
paste("Error test (rmse) for pruned tree:", round(test_rmse3,2))
improvement <- (test_rmse3-test_rmse2)/test_rmse2*100
# Chunk 20
errTable <- data.frame(Model=character(),  RMSE=double())
errTable[1, ] <-  c("Default Regression Tree", round(test_rmse1,2))
errTable[2, ] <-  c("Big Regression Tree", round(test_rmse2,2))
errTable[3, ] <-  c("Optimally pruned Regression Tree", round(test_rmse3,2))
# Chunk 21
kableExtra::kable(errTable) %>% kableExtra::kable_styling()
# Chunk 22
vip(ames_rt1bis, num_features = 40, bar = FALSE)
# Chunk 23: fitbaggedTrees1
# make bootstrapping reproducible
set.seed(123)
library(randomForest)
bag.Ames <- randomForest(Sale_Price ~ .,
data = ames_train,
mtry = ncol(ames_train-1),
ntree = 100,
importance = TRUE)
show(bag.Ames)
# Chunk 24: predictBagTrain
yhattrain.bag <- predict(bag.Ames, newdata = ames_train)
# train_mse_bag  <- sqrt(mean(yhattrain.bag - ames_train$Sale_Price)^2)
train_rmse_bag <- sqrt(mean((yhattrain.bag - ames_train$Sale_Price)^2))
showError<- paste("Error train (rmse) for bagged tree:", round(train_rmse_bag,6))
plot(yhattrain.bag, ames_train$Sale_Price, main=showError)
abline(0, 1)
# Chunk 25: predictBagTest
yhat.bag <- predict(bag.Ames, newdata = ames_test)
# test_mse_bag  <- sqrt(mean(yhat.bag - ames_test$Sale_Price)^2)
test_rmse_bag  <- sqrt(mean((yhat.bag - ames_test$Sale_Price)^2))
showError<- paste("Error test (rmse) for bagged tree:", round(test_rmse_bag,4))
plot(yhat.bag, ames_test$Sale_Price, main=showError)
abline(0, 1)
# Chunk 26: predictBagOOB
oob_err<- sqrt(mean((bag.Ames$predicted-ames_train$Sale_Price)^2))
showError <- paste("Out of bag error for bagged tree:", round(oob_err,4))
plot(bag.Ames$predicted, ames_train$Sale_Price, main=showError)
abline(0, 1)
# Chunk 27
errTable <- data.frame(Model=character(),  RMSE=double())
errTable[1, ] <-  c("Default Regression Tree", round(test_rmse1,2))
errTable[2, ] <-  c("Big Regression Tree", round(test_rmse2,2))
errTable[3, ] <-  c("Optimally pruned Regression Tree", round(test_rmse3,2))
errTable[4, ] <-  c("Bagged Tree with Train Data", round(train_rmse_bag,2))
errTable[5, ] <-  c("Bagged Tree with Test Data", round(test_rmse_bag,2))
errTable[6, ] <-  c("Bagged Tree with OOB error rate", round(oob_err,2))
# Chunk 28
knitr::include_graphics("images/baggingRSME.png")
# Chunk 29
require(dplyr)
VIP <- importance(bag.Ames)
VIP <- VIP[order(VIP[,1], decreasing = TRUE),]
head(VIP, n=30)
# Chunk 30
invVIP <-VIP[order(VIP[,1], decreasing = FALSE),1]
tVIP<- tail(invVIP, n=15)
barplot(tVIP, horiz = TRUE, cex.names=0.5)
# Chunk 31
library(vip)
vip(bag.Ames, num_features = 40, bar = FALSE)
# Chunk 32: fitRF
# make bootstrapping reproducible
set.seed(123)
require(randomForest)
RF.Ames <- randomForest(Sale_Price ~ .,
data = ames_train,
importance = TRUE)
# Chunk 33
show(RF.Ames)
# Chunk 34: predictRF
yhat.rf <- predict(RF.Ames, newdata = ames_test)
plot(yhat.rf, ames_test$Sale_Price)
abline(0, 1)
test_rmse_rf  <- sqrt(mean((yhat.rf - ames_test$Sale_Price)^2))
paste("Error test (rmse) for Random Forest:", round(test_rmse_rf,2))
errTable[7, ] <-  c("Random Forest (defaults)", round(test_rmse_rf,2))
errTable
num_trees_range <- seq(100, 400, 100)
num_vars_range <- floor(ncol(ames_train)/(seq(2,4,1)))
RFerrTable <- data.frame(Model=character(),
NumTree=integer(), NumVar=integer(),
RMSE=double())
numTrees <- num_trees_range[i]
i<- 1
j<- 1
numTrees <- num_trees_range[i]
numVars <- num_vars_range [j] # floor(ncol(ames_train)/3) # default
RF.Ames.n <- randomForest(Sale_Price ~ .,
data = ames_train,
mtry = numVars,
ntree= numTrees,
importance = TRUE)
yhat.rf <- predict(RF.Ames.n, newdata = ames_test)
oob.rf <- RF.Ames.n$predicted
test_rmse_rf  <- sqrt(mean((yhat.rf - ames_test$Sale_Price)^2))
RFerrTable[errValue, ] <-  c("Random Forest",
NumTree = numTrees, NumVar = numVars,
RMSE = round(test_rmse_rf,2))
errValue <- 1
RFerrTable[errValue, ] <-  c("Random Forest",
NumTree = numTrees, NumVar = numVars,
RMSE = round(test_rmse_rf,2))
errValue <- errValue+1
system.time(
for (i in seq_along(num_trees_range)){
for (j in seq_along(num_vars_range)) {
numTrees <- num_trees_range[i]
numVars <- num_vars_range [j] # floor(ncol(ames_train)/3) # default
RF.Ames.n <- randomForest(Sale_Price ~ .,
data = ames_train,
mtry = numVars,
ntree= numTrees,
importance = TRUE)
yhat.rf <- predict(RF.Ames.n, newdata = ames_test)
oob.rf <- RF.Ames.n$predicted
test_rmse_rf  <- sqrt(mean((yhat.rf - ames_test$Sale_Price)^2))
RFerrTable[errValue, ] <-  c("Random Forest",
NumTree = numTrees, NumVar = numVars,
RMSE = round(test_rmse_rf,2))
errValue <- errValue+1
}
)
RFerrTable %>% kableExtra::kable()
optimRFErr<- RFerrTable[RFerrTable$RMSE==min(RFerrTable$RMSE),]
errTable[8, ] <-  c("Random Forest (Optimized)", round(optimRFErr,2))
optimRFErr
RFerrTable[RFerrTable$RMSE==min(RFerrTable$RMSE),]
bestRF <- which(RFerrTable$RMSE==min(RFerrTable$RMSE))
bestRF
RFerrTable[, 11]
RFerrTable[bestRF,]
errTable[8, ] <-  c("Random Forest (Optimized)", round(bestRF,2))
errTable
RFerrTable[bestRF,"RMSE"]
RFerrTable[bestRF,]
minRFErr <- RFerrTable[bestRF,"RMSE"]
errTable[8, ] <-  c("Random Forest (Optimized)", round(minRFErr,2))
minRFErr
bestRF
bestRF <- which(RFerrTable$RMSE==min(RFerrTable$RMSE))
bestRF
RFerrTable[bestRF,]
minRFErr <- RFerrTable[bestRF,4]
minRFErr
minRFErr <- as.numeric(RFerrTable[bestRF,4])
minRFErr
errTable[8, ] <-  c("Random Forest (Optimized)", round(minRFErr,2))
kableExtra::kable(errTable) %>% kableExtra::kable_styling()
vip(RF.Ames, num_features = 40, bar = FALSE)
opt <- par(mfcol=c(2,1))
vip(bag.Ames, num_features = 40, bar = FALSE)
vip(RF.Ames, num_features = 40, bar = FALSE)
vip(bag.Ames, num_features = 40, bar = FALSE)
vip(RF.Ames, num_features = 40, bar = FALSE)
par(opt)
library(skimr)
skim(ames)
```
library(skimr)
skim(ames)
data(ames, package = "modeldata")
dim(ames)
skim(ames)
is.numeric(ames)
spply(ames, is.numeric())
sapply(ames, is.numeric())
apply(ames, 2, is.numeric())
apply(ames, 2, type)
apply(ames, 2, class)
sapply(ames, class)
sapply(ames, is.numeric)
boxplot(ames[sapply(ames, is.numeric),])
boxplot(ames[,sapply(ames, is.numeric)])
boxplot(ames[,sapply(ames, is.numeric)], las=2, cex.axis=0.5)
boxplot(ames[,sapply(ames, is.numeric)], las=1.5, cex.axis=0.5)
summary(ames$Sale_Price)
stem(ames$Sale_Price/1000)
summary(ames$Sale_Price)
install.packages("kableExtra", dependencies = TRUE)
remotes::install_github("haozhu233/kePrint")
? kable
? knitr::kable
require("ISLR")
require("ISLR2")
install.packages("xgboost", dep=TRUE)
# Chunk 1: packages
# If the package is not installed then it will be installed
if(!require(rsample)) install.packages("rsample", dep=TRUE)
if(!require("modeldata")) install.packages("modeldata")
# if(!require("tidymodels")) install.packages("tidymodels")
if(!require("xgboost")) install.packages("xgboost")
if(!require("gbm")) install.packages("gbm")
# Chunk 2
# Helper packages
library(dplyr)       # for data wrangling
library(ggplot2)     # for awesome plotting
library(modeldata)
library(foreach)     # for parallel processing with for loops
# Modeling packages
# library(tidymodels)
library(xgboost)
library(gbm)
# Chunk 3
dim(ames)
boxplot(ames)
# Chunk 4
require(dplyr)
ames <- ames %>% mutate(Sale_Price = Sale_Price/1000)
boxplot(ames)
# Chunk 5
# Stratified sampling with the rsample package
set.seed(123)
split <- rsample::initial_split(ames, prop = 0.7,
strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
boostResult_cv <- xgb.cv(
data      = ames_train,
params    = list(eta = 0.3, max_depth = 6, subsample = 1),
nrounds   = 500,
nfold     = 5,
metrics   = "rmse",
verbose   = 0
)
# Extraer etiquetas antes de la conversión a xgb.DMatrix
train_labels <- ames_train$Sale_Price
test_labels <- ames_test$Sale_Price
# Convertir a xgb.DMatrix
ames_train_matrix <- xgb.DMatrix(
data = as.matrix(ames_train %>% select(-Sale_Price)),
label = train_labels
)
ames_test_matrix <- xgb.DMatrix(
data = as.matrix(ames_test %>% select(-Sale_Price)),
label = test_labels
)
str(ames_train)
ames_train_num <- model.matrix(Sale_Price ~ . , data = ames_train)[,-1]
ames_test_num <- model.matrix(Sale_Price ~ . , data = ames_test)[,-1]
train_labels <- ames_train$Sale_Price
test_labels <- ames_test$Sale_Price
ames_train_matrix <- xgb.DMatrix(
data = ames_train_num,
label = train_labels
)
ames_test_matrix <- xgb.DMatrix(
data = ames_test_num,
label = test_labels
)
boostResult_cv <- xgb.cv(
data = ames_train_matrix,
params = list(eta = 0.3, max_depth = 6, subsample = 1, objective = "reg:squarederror"),
nrounds = 500,
nfold = 5,
metrics = "rmse",
verbose = 0
)
boostResult_cv <- boostResult_cv$evaluation_log
print(boostResult_cv)
ggplot(data = boostResult_cv) +
geom_line(aes(x = iter, y = train_rmse_mean, color = "train rmse")) +
geom_line(aes(x = iter, y = test_rmse_mean, color = "cv rmse")) +
geom_point(
data = slice_min(boostResult_cv, order_by = test_rmse_mean, n = 1),
aes(x = iter, y = test_rmse_mean),
color = "firebrick"
) +
labs(
title = "Evolution of cv-error vs number of trees",
x     = "number of trees",
y     = "cv-error (rmse)",
color = ""
) +
theme_bw() +
theme(legend.position = "bottom")
paste("Optimal number of rounds (nrounds):", slice_min(boostResult_cv, order_by = test_rmse_mean, n = 1)$iter)
eta_range          <- c(0.001, 0.01, 0.1, 0.3)
df_results_cv   <- data.frame()
for(i in seq_along(eta_range)){
set.seed(123)
results_cv <- xgb.cv(
data    = ames_train,
params  = list(eta = eta_range[i],
max_depth = 6, subsample = 1),
nrounds = 4000,
nfold   = 5,
metrics = "rmse",
verbose = 0
)
results_cv <- results_cv$evaluation_log
results_cv <- results_cv %>%
select(iter, test_rmse_mean) %>%
mutate(eta = as.character(eta_range[i]))
df_results_cv <- df_results_cv %>% bind_rows(results_cv)
}
# Rango de valores para la tasa de aprendizaje (eta)
eta_range <- c(0.001, 0.01, 0.1, 0.3)
df_results_cv <- data.frame()
for (i in seq_along(eta_range)) {
set.seed(123)
# Validación cruzada con el eta actual
results_cv <- xgb.cv(
data = ames_train_matrix,  # ✅ Usamos el xgb.DMatrix correcto
params = list(
eta = eta_range[i],
max_depth = 6,
subsample = 1,
objective = "reg:squarederror"
),
nrounds = 4000,
nfold = 5,
metrics = "rmse",
verbose = 0
)
# Extraer la evaluación de RMSE y registrar resultados
results_cv <- results_cv$evaluation_log
results_cv <- results_cv %>%
select(iter, test_rmse_mean) %>%
mutate(eta = as.character(eta_range[i]))  # Guardamos el eta usado
df_results_cv <- df_results_cv %>% bind_rows(results_cv)
}
df_results_cv
ggplot(data = df_results_cv) +
geom_line(aes(x = iter, y = test_rmse_mean, color = eta)) +
labs(
title = "Evolución del error en validación cruzada vs tasa de aprendizaje (eta)",
x = "Número de iteraciones",
y = "Error RMSE en validación cruzada",
color = "Eta"
) +
theme_bw() +
theme(legend.position = "bottom")
train_labels <- ames_train$Sale_Price
test_labels <- ames_test$Sale_Price
ames_train_matrix <- xgb.DMatrix(
data = as.matrix(ames_train[, !names(ames_train) %in% "Sale_Price"]),
label = train_labels
)
train_labels <- ames_train$Sale_Price
test_labels <- ames_test$Sale_Price
ames_train_matrix <- xgb.DMatrix(
data = as.matrix(ames_train[, !names(ames_train) %in% "Sale_Price"]),
label = train_labels
)
train_labels <- ames_train$Sale_Price
test_labels <- ames_test$Sale_Price
ames_train_matrix <- xgb.DMatrix(
data = as.matrix(ames_train[, !names(ames_train) %in% "Sale_Price"]),
label = train_labels
)
# Convertir variables categóricas a dummy variables usando model.matrix()
ames_train_num <- model.matrix(Sale_Price ~ . , data = ames_train)[,-1]
ames_test_num <- model.matrix(Sale_Price ~ . , data = ames_test)[,-1]
# Extraer etiquetas de Sale_Price
train_labels <- ames_train$Sale_Price
test_labels <- ames_test$Sale_Price
# Convertir a xgb.DMatrix
ames_train_matrix <- xgb.DMatrix(
data = ames_train_num,
label = train_labels
)
ames_test_matrix <- xgb.DMatrix(
data = ames_test_num,
label = test_labels
)
# Definir rango de hiperparámetros para la optimización
eta_values <- c(0.01, 0.05, 0.1, 0.3)
nrounds_values <- c(500, 1000, 2000)
best_rmse <- Inf
best_params <- list()
cv_results_df <- data.frame()
set.seed(123)
for (eta in eta_values) {
for (nrounds in nrounds_values) {
cv_results <- xgb.cv(
data = ames_train_matrix,
params = list(
eta = eta,
max_depth = 6,
subsample = 0.8,
colsample_bytree = 0.8,
objective = "reg:squarederror"
),
nrounds = nrounds,
nfold = 5,
metrics = "rmse",
verbose = 0,
early_stopping_rounds = 10
)
if (is.null(cv_results)) next
results_row <- data.frame(
eta = eta,
nrounds = nrounds,
min_rmse = min(cv_results$evaluation_log$test_rmse_mean),
best_nrounds = cv_results$evaluation_log$iter[which.min(cv_results$evaluation_log$test_rmse_mean)]
)
cv_results_df <- bind_rows(cv_results_df, results_row)
if (results_row$min_rmse < best_rmse) {
best_rmse <- results_row$min_rmse
best_params <- list(
eta = results_row$eta,
nrounds = results_row$best_nrounds
)
}
cat("\n, Best hyperparameters values found:\n")
cat("Eta:", best_params$eta, "\n")
cat("Nrounds:", best_params$nrounds, "\n")
cat("RMSE mínimo:", round(best_rmse, 4), "\n")
# View(cv_results_df)
