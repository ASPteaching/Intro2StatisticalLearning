p2 <- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +
geom_line() +
geom_point() +
labs(title = "Error vs penalization (alpha)") +
theme_bw()
ggarrange(p1, p2)
# Chunk 13: fitAmesRT2
ctlPars2 <-tree.control(nobs=nrow(ames_train), mincut = 1, minsize = 2, mindev = 0)
ames_rt2 <-  tree::tree(
formula = Sale_Price ~ .,
data    = ames_train,
split   = "deviance",
control = ctlPars2)
summary(ames_rt2)
# Chunk 14: pruneAmes2
set.seed(123)
cv_ames_rt2 <- tree::cv.tree(ames_rt2, K = 5)
optSize2 <- rev(cv_ames_rt2$size)[which.min(rev(cv_ames_rt2$dev))]
paste("Optimal size obtained is:", optSize2)
# Chunk 15
prunedTree2 <- tree::prune.tree(
tree = ames_rt2,
best = optSize2
)
summary(prunedTree2)
# Chunk 16
res_cv2 <- data.frame(
n_nodes  = cv_ames_rt2$size,
deviance = cv_ames_rt2$dev,
alpha    = cv_ames_rt2$k
)
p1 <- ggplot(data = res_cv2, aes(x = n_nodes, y = deviance)) +
geom_line() +
geom_point() +
geom_vline(xintercept = optSize2, color = "red") +
labs(title = "Error vs tree size") +
theme_bw()
p2 <- ggplot(data = res_cv2, aes(x = alpha, y = deviance)) +
geom_line() +
geom_point() +
labs(title = "Error vs penalization (alpha)") +
theme_bw()
ggarrange(p1, p2)
# Chunk 17: ames_rt_pred1
ames_rt_pred1 <- predict(ames_rt1, newdata = ames_test)
test_rmse1    <- sqrt(mean((ames_rt_pred1 - ames_test$Sale_Price)^2))
paste("Error test (rmse) for initial tree:", round(test_rmse1,2))
# Chunk 18: ames_rt_pred2
ames_rt_pred2 <- predict(ames_rt2, newdata = ames_test)
test_rmse2    <- sqrt(mean((ames_rt_pred2 - ames_test$Sale_Price)^2))
paste("Error test (rmse) for big tree:", round(test_rmse2,2))
# Chunk 19: ames_pruned_pred
ames_pruned_pred <- predict(prunedTree2, newdata = ames_test)
test_rmse3    <- sqrt(mean((ames_pruned_pred - ames_test$Sale_Price)^2))
paste("Error test (rmse) for pruned tree:", round(test_rmse3,2))
improvement <- (test_rmse3-test_rmse2)/test_rmse2*100
# Chunk 20
errTable <- data.frame(Model=character(),  RMSE=double())
errTable[1, ] <-  c("Default Regression Tree", round(test_rmse1,2))
errTable[2, ] <-  c("Big Regression Tree", round(test_rmse2,2))
errTable[3, ] <-  c("Optimally pruned Regression Tree", round(test_rmse3,2))
# Chunk 21
kableExtra::kable(errTable) %>% kableExtra::kable_styling()
# Chunk 22
vip(ames_rt1bis, num_features = 40, bar = FALSE)
# Chunk 23: fitbaggedTrees1
# make bootstrapping reproducible
set.seed(123)
library(randomForest)
bag.Ames <- randomForest(Sale_Price ~ .,
data = ames_train,
mtry = ncol(ames_train-1),
ntree = 200,
importance = TRUE)
# Chunk 24
show(bag.Ames)
# Chunk 25: predictBagTrain
yhattrain.bag <- predict(bag.Ames, newdata = ames_train)
train_mse_bag  <- sqrt(mean(yhattrain.bag - ames_train$Sale_Price)^2)
showError<- paste("Error train (rmse) for bagged tree:", round(train_mse_bag,6))
plot(yhattrain.bag, ames_train$Sale_Price, main=showError)
abline(0, 1)
# Chunk 26: predictBagTest
yhat.bag <- predict(bag.Ames, newdata = ames_test)
test_mse_bag  <- sqrt(mean(yhat.bag - ames_test$Sale_Price)^2)
showError<- paste("Error test (rmse) for bagged tree:", round(test_mse_bag,4))
plot(yhat.bag, ames_test$Sale_Price, main=showError)
abline(0, 1)
# Chunk 27: predictBagOOB
oob_err<- sqrt(mean((bag.Ames$predicted-ames_train$Sale_Price)^2))
showError <- paste("Out of bag error for bagged tree:", round(oob_err,4))
plot(bag.Ames$predicted, ames_train$Sale_Price, main=showError)
abline(0, 1)
errTable <- data.frame(Model=character(),  RMSE=double())
errTable[1, ] <-  c("Default Regression Tree", round(test_rmse1,2))
errTable[2, ] <-  c("Big Regression Tree", round(test_rmse2,2))
errTable[3, ] <-  c("Optimally pruned Regression Tree", round(test_rmse3,2))
errTable[4, ] <-  c("Bagged Tree with Train Data", round(train_mse_bag,2))
errTable[5, ] <-  c("Bagged Tree with Test Data", round(test_mse_bag,2))
errTable[6, ] <-  c("Bagged Tree with OOB error rate", round(oob_err,2))
errTable
yhat.rf <- predict(RF.Ames, newdata = ames_test)
plot(yhat.rf, ames_test$Sale_Price)
abline(0, 1)
test_rmse_rf  <- sqrt(mean((yhat.rf - ames_test$Sale_Price)^2))
paste("Error test (rmse) for Random Forest:", round(test_rmse_rf,2))
errTable[7, ] <-  c("Random Forest (defaults)", round(test_rmse_rf,2))
errTable
train_rmse_bag <- sqrt(mean((yhattrain.bag - ames_train$Sale_Price)^2))
showError<- paste("Error train (rmse) for bagged tree:", round(train_mse_bag,6))
plot(yhattrain.bag, ames_train$Sale_Price, main=showError)
abline(0, 1)
yhattrain.bag <- predict(bag.Ames, newdata = ames_train)
# train_mse_bag  <- sqrt(mean(yhattrain.bag - ames_train$Sale_Price)^2)
train_rmse_bag <- sqrt(mean((yhattrain.bag - ames_train$Sale_Price)^2))
showError<- paste("Error train (rmse) for bagged tree:", round(train_mse_bag,6))
plot(yhattrain.bag, ames_train$Sale_Price, main=showError)
abline(0, 1)
yhat.bag <- predict(bag.Ames, newdata = ames_test)
# test_mse_bag  <- sqrt(mean(yhat.bag - ames_test$Sale_Price)^2)
test_rmse_bag  <- sqrt(mean((yhat.bag - ames_test$Sale_Price)^2))
showError<- paste("Error test (rmse) for bagged tree:", round(test_mse_bag,4))
plot(yhat.bag, ames_test$Sale_Price, main=showError)
abline(0, 1)
oob_err<- sqrt(mean((bag.Ames$predicted-ames_train$Sale_Price)^2))
showError <- paste("Out of bag error for bagged tree:", round(oob_err,4))
plot(bag.Ames$predicted, ames_train$Sale_Price, main=showError)
abline(0, 1)
errTable <- data.frame(Model=character(),  RMSE=double())
errTable[1, ] <-  c("Default Regression Tree", round(test_rmse1,2))
errTable[2, ] <-  c("Big Regression Tree", round(test_rmse2,2))
errTable[3, ] <-  c("Optimally pruned Regression Tree", round(test_rmse3,2))
errTable[4, ] <-  c("Bagged Tree with Train Data", round(train_mse_bag,2))
errTable[5, ] <-  c("Bagged Tree with Test Data", round(test_mse_bag,2))
errTable[6, ] <-  c("Bagged Tree with OOB error rate", round(oob_err,2))
errTable
# Error en el conjunto de entrenamiento
yhattrain.bag <- predict(bag.Ames, newdata = ames_train)
train_rmse_bag <- sqrt(mean((yhattrain.bag - ames_train$Sale_Price)^2))
# Error en el conjunto de prueba
yhat.bag <- predict(bag.Ames, newdata = ames_test)
test_rmse_bag <- sqrt(mean((yhat.bag - ames_test$Sale_Price)^2))
# Imprimir valores calculados
cat("Train RMSE Bagging (expected reasonable value):", train_rmse_bag, "\n")
cat("Test RMSE Bagging (expected reasonable value):", test_rmse_bag, "\n")
# test_mse_bag  <- sqrt(mean(yhat.bag - ames_test$Sale_Price)^2)
test_rmse_bag  <- sqrt(mean((yhat.bag - ames_test$Sale_Price)^2))
test_rmse_bag
show(bag.Ames)
# make bootstrapping reproducible
set.seed(123)
library(randomForest)
if (!exists("labs/bag.Ames.Rda")){
bag.Ames <- randomForest(Sale_Price ~ .,
data = ames_train,
mtry = ncol(ames_train-1),
ntree = 100,
importance = TRUE)
save(bag.Ames, "labs/bag.Ames.Rda")
}else{
load(file="labs/bag.Ames.Rda")
}
exists("labs/bag.Ames.Rda")
save(bag.Ames, "labs/bag.Ames.Rda")
save(bag.Ames, file= "labs/bag.Ames.Rda")
if (!exists("labs/bag.Ames.Rda")){
bag.Ames <- randomForest(Sale_Price ~ .,
data = ames_train,
mtry = ncol(ames_train-1),
ntree = 100,
importance = TRUE)
save(bag.Ames, file= "labs/bag.Ames.Rda")
}else{
load(file="labs/bag.Ames.Rda")
}
# make bootstrapping reproducible
set.seed(123)
library(randomForest)
if (!exists("labs/bag.Ames.Rda")){
bag.Ames <- randomForest(Sale_Price ~ .,
data = ames_train,
mtry = ncol(ames_train-1),
ntree = 100,
importance = TRUE)
save(bag.Ames, file= "labs/bag.Ames.Rda")
}else{
load(file="labs/bag.Ames.Rda")
}
show(bag.Ames)
sqrt(737.1328)
kableExtra::kable(errTable) %>% kableExtra::kable_styling()
# Chunk 1: packages
# If the package is not installed then it will be installed
if(!require("randomForest")) install.packages("randomForest")
if (!require(vip)) install.packages("vip", dep=TRUE)
if(!require(modeldata))
install.packages("modeldata", dep=TRUE)
if(!require("tree")) install.packages("tree")
if(!require("rsample")) install.packages("rsample")
if(!require("caret")) install.packages("caret")
if(!require("foreach")) install.packages("foreach")
if(!require("ipred")) install.packages("ipred")
# Chunk 2
# Helper packages
library(dplyr)       # for data wrangling
library(ggplot2)     # for awesome plotting
library(modeldata)  # for parallel backend to foreach
library(foreach)     # for parallel processing with for loops
# Modeling packages
library(caret)       # for general model fitting
library(rpart)       # for fitting decision trees
library(ipred)       # for fitting bagged decision trees
# Chunk 3
data(ames, package = "modeldata")
# Chunk 4
dim(ames)
boxplot(ames)
str(ames)
# Chunk 5
summary(ames$Sale_Price)
stem(ames$Sale_Price/1000)
# Chunk 6
require(dplyr)
ames <- ames %>% mutate(Sale_Price = Sale_Price/1000)
dim(ames)
boxplot(ames)
# Chunk 7
if(!require(rsample))
install.packages("rsample", dep=TRUE)
# Stratified sampling with the rsample package
set.seed(123)
split <- rsample::initial_split(ames, prop = 0.7,
strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
# Chunk 8
library(tree)
ctlPars <-tree.control(nobs =nrow(ames_train),
mincut = 5,
minsize = 10,
mindev = 0.01)
ames_rt1 <-  tree::tree(
formula = Sale_Price ~ .,
data    = ames_train,
split   = "deviance",
control = ctlPars)
summary(ames_rt1)
# Chunk 9: plotAmes1
plot(x = ames_rt1, type = "proportional")
text(x = ames_rt1, splits = TRUE, pretty = 0, cex = 0.6, col = "firebrick")
# Chunk 10
ames_rt1bis <- rpart(
formula = Sale_Price ~ .,
data    = ames_train,
method  = "anova",
control = ctlPars
)
library(rpart.plot)
rpart.plot(ames_rt1bis)
# Chunk 11: pruneAmes1
set.seed(123)
cv_ames_rt1 <- tree::cv.tree(ames_rt1, K = 5)
optSize <- rev(cv_ames_rt1$size)[which.min(rev(cv_ames_rt1$dev))]
paste("Optimal size obtained is:", optSize)
# Chunk 12: plotPruneAmes
library(ggplot2)
library(ggpubr)
resultados_cv <- data.frame(
n_nodes  = cv_ames_rt1$size,
deviance = cv_ames_rt1$dev,
alpha    = cv_ames_rt1$k
)
p1 <- ggplot(data = resultados_cv, aes(x = n_nodes, y = deviance)) +
geom_line() +
geom_point() +
geom_vline(xintercept = optSize, color = "red") +
labs(title = "Error vs tree size") +
theme_bw()
p2 <- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +
geom_line() +
geom_point() +
labs(title = "Error vs penalization (alpha)") +
theme_bw()
ggarrange(p1, p2)
# Chunk 13: fitAmesRT2
ctlPars2 <-tree.control(nobs=nrow(ames_train), mincut = 1, minsize = 2, mindev = 0)
ames_rt2 <-  tree::tree(
formula = Sale_Price ~ .,
data    = ames_train,
split   = "deviance",
control = ctlPars2)
summary(ames_rt2)
# Chunk 14: pruneAmes2
set.seed(123)
cv_ames_rt2 <- tree::cv.tree(ames_rt2, K = 5)
optSize2 <- rev(cv_ames_rt2$size)[which.min(rev(cv_ames_rt2$dev))]
paste("Optimal size obtained is:", optSize2)
# Chunk 15
prunedTree2 <- tree::prune.tree(
tree = ames_rt2,
best = optSize2
)
summary(prunedTree2)
# Chunk 16
res_cv2 <- data.frame(
n_nodes  = cv_ames_rt2$size,
deviance = cv_ames_rt2$dev,
alpha    = cv_ames_rt2$k
)
p1 <- ggplot(data = res_cv2, aes(x = n_nodes, y = deviance)) +
geom_line() +
geom_point() +
geom_vline(xintercept = optSize2, color = "red") +
labs(title = "Error vs tree size") +
theme_bw()
p2 <- ggplot(data = res_cv2, aes(x = alpha, y = deviance)) +
geom_line() +
geom_point() +
labs(title = "Error vs penalization (alpha)") +
theme_bw()
ggarrange(p1, p2)
# Chunk 17: ames_rt_pred1
ames_rt_pred1 <- predict(ames_rt1, newdata = ames_test)
test_rmse1    <- sqrt(mean((ames_rt_pred1 - ames_test$Sale_Price)^2))
paste("Error test (rmse) for initial tree:", round(test_rmse1,2))
# Chunk 18: ames_rt_pred2
ames_rt_pred2 <- predict(ames_rt2, newdata = ames_test)
test_rmse2    <- sqrt(mean((ames_rt_pred2 - ames_test$Sale_Price)^2))
paste("Error test (rmse) for big tree:", round(test_rmse2,2))
# Chunk 19: ames_pruned_pred
ames_pruned_pred <- predict(prunedTree2, newdata = ames_test)
test_rmse3    <- sqrt(mean((ames_pruned_pred - ames_test$Sale_Price)^2))
paste("Error test (rmse) for pruned tree:", round(test_rmse3,2))
improvement <- (test_rmse3-test_rmse2)/test_rmse2*100
# Chunk 20
errTable <- data.frame(Model=character(),  RMSE=double())
errTable[1, ] <-  c("Default Regression Tree", round(test_rmse1,2))
errTable[2, ] <-  c("Big Regression Tree", round(test_rmse2,2))
errTable[3, ] <-  c("Optimally pruned Regression Tree", round(test_rmse3,2))
# Chunk 21
kableExtra::kable(errTable) %>% kableExtra::kable_styling()
# Chunk 22
vip(ames_rt1bis, num_features = 40, bar = FALSE)
# Chunk 23: fitbaggedTrees1
# make bootstrapping reproducible
set.seed(123)
library(randomForest)
bag.Ames <- randomForest(Sale_Price ~ .,
data = ames_train,
mtry = ncol(ames_train-1),
ntree = 100,
importance = TRUE)
show(bag.Ames)
# Chunk 24: predictBagTrain
yhattrain.bag <- predict(bag.Ames, newdata = ames_train)
# train_mse_bag  <- sqrt(mean(yhattrain.bag - ames_train$Sale_Price)^2)
train_rmse_bag <- sqrt(mean((yhattrain.bag - ames_train$Sale_Price)^2))
showError<- paste("Error train (rmse) for bagged tree:", round(train_rmse_bag,6))
plot(yhattrain.bag, ames_train$Sale_Price, main=showError)
abline(0, 1)
# Chunk 25: predictBagTest
yhat.bag <- predict(bag.Ames, newdata = ames_test)
# test_mse_bag  <- sqrt(mean(yhat.bag - ames_test$Sale_Price)^2)
test_rmse_bag  <- sqrt(mean((yhat.bag - ames_test$Sale_Price)^2))
showError<- paste("Error test (rmse) for bagged tree:", round(test_rmse_bag,4))
plot(yhat.bag, ames_test$Sale_Price, main=showError)
abline(0, 1)
# Chunk 26: predictBagOOB
oob_err<- sqrt(mean((bag.Ames$predicted-ames_train$Sale_Price)^2))
showError <- paste("Out of bag error for bagged tree:", round(oob_err,4))
plot(bag.Ames$predicted, ames_train$Sale_Price, main=showError)
abline(0, 1)
# Chunk 27
errTable <- data.frame(Model=character(),  RMSE=double())
errTable[1, ] <-  c("Default Regression Tree", round(test_rmse1,2))
errTable[2, ] <-  c("Big Regression Tree", round(test_rmse2,2))
errTable[3, ] <-  c("Optimally pruned Regression Tree", round(test_rmse3,2))
errTable[4, ] <-  c("Bagged Tree with Train Data", round(train_rmse_bag,2))
errTable[5, ] <-  c("Bagged Tree with Test Data", round(test_rmse_bag,2))
errTable[6, ] <-  c("Bagged Tree with OOB error rate", round(oob_err,2))
# Chunk 28
knitr::include_graphics("images/baggingRSME.png")
# Chunk 29
require(dplyr)
VIP <- importance(bag.Ames)
VIP <- VIP[order(VIP[,1], decreasing = TRUE),]
head(VIP, n=30)
# Chunk 30
invVIP <-VIP[order(VIP[,1], decreasing = FALSE),1]
tVIP<- tail(invVIP, n=15)
barplot(tVIP, horiz = TRUE, cex.names=0.5)
# Chunk 31
library(vip)
vip(bag.Ames, num_features = 40, bar = FALSE)
# Chunk 32: fitRF
# make bootstrapping reproducible
set.seed(123)
require(randomForest)
RF.Ames <- randomForest(Sale_Price ~ .,
data = ames_train,
importance = TRUE)
# Chunk 33
show(RF.Ames)
# Chunk 34: predictRF
yhat.rf <- predict(RF.Ames, newdata = ames_test)
plot(yhat.rf, ames_test$Sale_Price)
abline(0, 1)
test_rmse_rf  <- sqrt(mean((yhat.rf - ames_test$Sale_Price)^2))
paste("Error test (rmse) for Random Forest:", round(test_rmse_rf,2))
errTable[7, ] <-  c("Random Forest (defaults)", round(test_rmse_rf,2))
errTable
num_trees_range <- seq(100, 400, 100)
num_vars_range <- floor(ncol(ames_train)/(seq(2,4,1)))
RFerrTable <- data.frame(Model=character(),
NumTree=integer(), NumVar=integer(),
RMSE=double())
numTrees <- num_trees_range[i]
i<- 1
j<- 1
numTrees <- num_trees_range[i]
numVars <- num_vars_range [j] # floor(ncol(ames_train)/3) # default
RF.Ames.n <- randomForest(Sale_Price ~ .,
data = ames_train,
mtry = numVars,
ntree= numTrees,
importance = TRUE)
yhat.rf <- predict(RF.Ames.n, newdata = ames_test)
oob.rf <- RF.Ames.n$predicted
test_rmse_rf  <- sqrt(mean((yhat.rf - ames_test$Sale_Price)^2))
RFerrTable[errValue, ] <-  c("Random Forest",
NumTree = numTrees, NumVar = numVars,
RMSE = round(test_rmse_rf,2))
errValue <- 1
RFerrTable[errValue, ] <-  c("Random Forest",
NumTree = numTrees, NumVar = numVars,
RMSE = round(test_rmse_rf,2))
errValue <- errValue+1
system.time(
for (i in seq_along(num_trees_range)){
for (j in seq_along(num_vars_range)) {
numTrees <- num_trees_range[i]
numVars <- num_vars_range [j] # floor(ncol(ames_train)/3) # default
RF.Ames.n <- randomForest(Sale_Price ~ .,
data = ames_train,
mtry = numVars,
ntree= numTrees,
importance = TRUE)
yhat.rf <- predict(RF.Ames.n, newdata = ames_test)
oob.rf <- RF.Ames.n$predicted
test_rmse_rf  <- sqrt(mean((yhat.rf - ames_test$Sale_Price)^2))
RFerrTable[errValue, ] <-  c("Random Forest",
NumTree = numTrees, NumVar = numVars,
RMSE = round(test_rmse_rf,2))
errValue <- errValue+1
}
)
RFerrTable %>% kableExtra::kable()
optimRFErr<- RFerrTable[RFerrTable$RMSE==min(RFerrTable$RMSE),]
errTable[8, ] <-  c("Random Forest (Optimized)", round(optimRFErr,2))
optimRFErr
RFerrTable[RFerrTable$RMSE==min(RFerrTable$RMSE),]
bestRF <- which(RFerrTable$RMSE==min(RFerrTable$RMSE))
bestRF
RFerrTable[, 11]
RFerrTable[bestRF,]
errTable[8, ] <-  c("Random Forest (Optimized)", round(bestRF,2))
errTable
RFerrTable[bestRF,"RMSE"]
RFerrTable[bestRF,]
minRFErr <- RFerrTable[bestRF,"RMSE"]
errTable[8, ] <-  c("Random Forest (Optimized)", round(minRFErr,2))
minRFErr
bestRF
bestRF <- which(RFerrTable$RMSE==min(RFerrTable$RMSE))
bestRF
RFerrTable[bestRF,]
minRFErr <- RFerrTable[bestRF,4]
minRFErr
minRFErr <- as.numeric(RFerrTable[bestRF,4])
minRFErr
errTable[8, ] <-  c("Random Forest (Optimized)", round(minRFErr,2))
kableExtra::kable(errTable) %>% kableExtra::kable_styling()
vip(RF.Ames, num_features = 40, bar = FALSE)
opt <- par(mfcol=c(2,1))
vip(bag.Ames, num_features = 40, bar = FALSE)
vip(RF.Ames, num_features = 40, bar = FALSE)
vip(bag.Ames, num_features = 40, bar = FALSE)
vip(RF.Ames, num_features = 40, bar = FALSE)
par(opt)
library(skimr)
skim(ames)
```
library(skimr)
skim(ames)
data(ames, package = "modeldata")
dim(ames)
skim(ames)
is.numeric(ames)
spply(ames, is.numeric())
sapply(ames, is.numeric())
apply(ames, 2, is.numeric())
apply(ames, 2, type)
apply(ames, 2, class)
sapply(ames, class)
sapply(ames, is.numeric)
boxplot(ames[sapply(ames, is.numeric),])
boxplot(ames[,sapply(ames, is.numeric)])
boxplot(ames[,sapply(ames, is.numeric)], las=2, cex.axis=0.5)
boxplot(ames[,sapply(ames, is.numeric)], las=1.5, cex.axis=0.5)
summary(ames$Sale_Price)
stem(ames$Sale_Price/1000)
summary(ames$Sale_Price)
