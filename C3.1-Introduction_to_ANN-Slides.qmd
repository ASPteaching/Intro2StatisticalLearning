---
title: "ARTIFICIAL NEURAL NETWORKS"
subtitle: 'Neural Networks, Deep Learning and<br> Artificial Intelligence'
format:
  revealjs: 
    incremental: false  
    transition: slide
    background-transition: fade
    transition-speed: slow
    scrollable: true
    menu:
      side: left
      width: half
      numbers: true
    slide-number: c/t
    show-slide-number: all
    progress: true
    css: "css4CU.css"
    theme: sky
knit:
  quarto:
    chunk_options:
      echo: true
      cache: false
      prompt: false
      tidy: true
      comment: NA
      message: false
      warning: false
    knit_options:
      width: 75
bibliography: "DeepLearning.bib"
editor_options: 
  chunk_output_type: console
---

# From Artificial Neural Networks to Artifial Intelligence

## Historical Background (1)

-   In the post-pandemic world, a lightning rise of AI, with a mess of realities and promises is impacting society.

-   Since ChatGPT entered the scene everybody has an experience, an opinion, or a fear on the topic.

![](https://bernardmarr.com/wp-content/uploads/2022/04/The-Dangers-Of-Not-Aligning-Artificial-Intelligence-With-Human-Values.jpg){fig-align="center" width="100%"}

## Is it just machine learning?

- Most tasks performed by AI can be described as Classification or  Prediction used in applications as:
   
   - Recommendation systems,
   - Image recognition, Image generation
   - Natural language processing

-   AI relies on machine learning algorithms, to make predictions based on large amounts of data.

-   AI has far-reaching implications beyond its predictive capabilities, including ethical, social or technological.

## AI, ANNs and Deep learning

- In many contexts, talking about AI means talking about 
*Deep Learning (DL)*.

- DL is a successful AI model which has powered many application such as *self-driving cars, voice assistants, and medical diagnosis systems*.

- DL originates in the field of *Artificial Neural Networks*

- But DL extends the basic principles of ANNs by:

    -   Adding complex architectures and algorithms and
    -   At the same time becoming more automatic


## The early history of AI (1)


```{r, out.width="90%", fig.cap= "[A Quick History of AI, ML and DL](https://nerdyelectronics.com/a-quick-history-of-ai-ml-and-dl/)"}
knitr::include_graphics("images/AIHistory1.jpg")
```


## Milestones in the history of DL

We can see several hints worth to account for:

-   The **Perceptron** and the first **Artificial Neural Network** where the basic building block was introduced.

-   The **Multilayered perceptron** and back-propagation where complex architectures were suggested to improve the capabilities.

-   **Deep Neural Networks**, with many hidden layers, and auto-tunability capabilities.

## From ANN to Deep learning

![Why Deep Learning Now?](images/WhyDLNow.png){fig-align="center" width="100%"} 

:::{.font90}
Source: [Alex Amini's MIT Introduction to Deep Learning' course](introtodeeplearning.com)
:::

## Success stories

Success stories such as

- the development of self-driving cars,

- the use of AI in medical diagnosis, and

- online shopping personalized recommendations

have also contributed to the widespread adoption of AI.

## Not to talk abou the fears

:::: {.columns}

::: {.column width='60%'}
- AI also comes with fears from multiple sources from science fiction to religion

  - Mass unemployment
  
  - Loss of privacity
  
  - AI bias
  
  - AI fakes
  
  - Or, simply, AI takeover

:::

::: {.column width='40%'}

<br>
```{r, fig.align='center'}
knitr::include_graphics("images/2.1-Introduction_to_ANN-Slides_insertimage_2.png")
```


:::

::::


## Back to science

Where/How does it all fit?

```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/AI-ML-DL-1.jpg")
```

## AI, ML, DL ...

-   **Artificial intelligence**: Ability of a computer to perform tasks commonly associated with intelligent beings.

-   **Machine learning**: study of algorithms that learn from examples and experience instead of relying on hard-coded rules and make predictions on new data

-   **Deep learning**: sub field of ML focusing on learning data representations as successive successive layers of increasingly meaningful representations.

## How does DL improve on ML

```{r, fig.align='center', out.width="100%", fig.cap="[ML and DL Approaches for Brain Disease Diagnosis](https://ieeexplore.ieee.org/document/9363896)"}
knitr::include_graphics("images/ML_vs_DL-2.png")
```


::: {.notes}
-   DNN: feature extraction and classification without (or with much les) human intervention.
-   DNN improves with data availability, without seemingly reaching plateaus.
:::

## Size does matter!

![An illustration of the performance comparison between deep learning (DL) and other machine learning (ML) algorithms, where DL modeling from large amounts of data can increase the performance](images/PerformanceVsAmountOfData.png){fig-align="center" width="100%"}

## The impact of Deep learning {.smaller}

-   Near-human-level image classification

-   Near-human-level speech transcription

-   Near-human-level handwriting transcription

-   Dramatically improved machine translation

-   Dramatically improved text-to-speech conversion

-   Digital assistants such as Google Assistant and Amazon Alexa

-   Near-human-level autonomous driving

-   Improved ad targeting, as used by Google, Baidu, or Bing

-   Improved search results on the web

-   Ability to answer natural language questions

-   Superhuman Go playing

## Not all that glitters is gold ...

-   According to F. Chollet, the developer of Keras,

    -   "*we shouldn't believe the short-term hype, but should believe in the long-term vision*.
    -   *It may take a while for AI to be deployed to its true potential---a potential the full extent of which no one has yet dared to dream*
    -   *but AI is coming, and it will transform our world in a fantastic way*".

# The **A**rtificial **N**eurone


## Emulating biological neurons


:::: {.columns}

::: {.column width='50%'}

```{r, fig.align='center', out.width="100%", fig.cap="[A biological Neuron]()"}
knitr::include_graphics("images/NaturalNeuron.png")
```

:::

::: {.column width='50%'}

```{r, fig.align='center', out.width="100%", fig.cap="[MuCulloch & Pitts proposal]()"}
knitr::include_graphics("images/MacCulloghPitts-Neuron.png")
```

:::

::::

- The first model of an artifial neurone was proposed by Mc Cullough & Pitts in 1943


## Mc Cullough's neuron

-   It may be divided into 2 parts.
    -   The first part, $g$,takes an input (as the dendrites of a neuron would do),
    -   It performs an aggregation and
    -   based on the aggregated value the second part, $f$, makes a decision.

:::{.font70}
- See [this link for an explanation](https://medium.com/towards-data-science/mcculloch-pitts-model-5fdf65ac5dd1) on how it can emulate logical operations such as AND, OR or NOT, but not XOR.
- See [this link]() for a nice clarifying example
:::

## Limitations

This first attempt to emulate neurons succeeded but with limitations:

-   What about non-Boolean (say, real) inputs?

-   What if all inputs are not equal?

-   What if we want to assign more importance to some inputs?

-   What about functions which are not linearly separable? Say XOR function

## Overcoming the limitations

-   To overcome these limitations Rosenblatt, proposed the perceptron model, or  *artificial neuron*, in 1958.

-   Generalizes McCullough-Pitts neuron in that *weights and thresholds can be learnt over time*.

    -   It takes a weighted sum of the inputs and
    -   It sets the output to iff the sum is more than an arbitrary threshold (**$\theta$**).

## Rosenblatt's perceptron

[![](images/RosenblattPerceptron1.png){width="100%"}](https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d)

## Rosenblatt's perceptron

:::{.font80}
-   Instead of hand coding the thresholding parameter $\theta$,
-   It is added as one of the inputs, with the weight $w_0=-\theta$.
:::

[![](images/RosenblattPerceptron2.png){width="100%"}](https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d)

## Comparison between the two

[![](images/McCullaughVSRosenblattPerceptron.png){width="100%"}](https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d)


## Comparison between the two

The Perceptron represents an improvement over McCullough-Pitts' neuron:

- It admits real-valued inputs.
- Both weights and the threshold can be learned.

However, there are still limitations:

- The output is determined by a **step function**, which limits its ability to model complex relationships.
- A **single perceptron can only classify linearly separable functions**.

## ANs and Activation Functions

- The Perceptron can be generalized by *Artificial Neurones* which can use more general functions, called *Activation Functions* to produce their output.

  - Activation functions are built in a way that they allow neurons to produce continuous and non-linear outputs.
  
- It must be noted however that *a single artificial neurone, even with a different activation function, still cannot model no-linear separable problems like XOR.*




## Activation in biological neurons

-   Biological neurons are specialized cells that transmit signals to communicate with each other.
- Neuron's activation is based on releasing *neurotransmitters*, chemicals that transmit signals between nerve cells.
    -   When the signal reaching the neuron exceeds a certain threshold, it releases neurotransmitters to continue the communication process.

## Activation functions in AN

-   Analogously, *activation functions* in AN are functions to decide if the AN it is activated or not.
-   AN's activation function is a mathematical function applied to the neuron's input to produce an output.
    -   In practice it may go from simple to complex functions that can learn  patterns in the data.
    -   Activation functions can incorporate non-linearity, improving over linear classifiers.

## Activation function

```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/ActivationFunction0.png")
```

## Artificial Neuron

With all these ideas in mind we can now define an Artificial Neuron as a *computational unit* that :

-   takes as input $x=(x_0,x_1,x_2,x_3),\ (x_0 = +1 \equiv bias)$,

-   outputs $h_{\theta}(x) = f(\theta^\intercal x) = f(\sum_i \theta_ix_i)$,

-   where $f:\mathbb{R}\mapsto \mathbb{R}$ is called the **activation function**.

## Activation functions

:::{.font90}

-   Goal of activation function is to provide the neuron with *the capability of producing the required outputs*.

-   Flexible enough to produce

    -   Either linear or non-linear transformations.
    -   Output in the desired range (\[0,1\], {-1,1}, $\mathbb{R}^+$...)

-   Usually chosen from a (small) set of possibilities.

    -   Sigmoid function
    -   Hyperbolic tangent, or `tanh`, function
    -   ReLU

:::

## The sigmoid function {.smaller}

::: columns
::: {.column width="50%"}

$$
f(z)=\frac{1}{1+e^{-z}}
$$

-   Output real values $\in (0,1)$.

-   Natural interpretations as *probability*


```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/sigmoidFunction.png")
```

:::

::: {.column width="50%"}

```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/sigmoidFunctionDerivative.png")
```


:::
:::

## the hyperbolic tangent {.smaller}

::: columns
::: {.column width="50%"}
Also called `tanh`, function:

$$
f(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}
$$

-   outputs are zero-centered and bounded in −1,1

-   scaled and shifted Sigmoid

-   stronger gradient but still has vanishing gradient problem

-   Its derivative is $f'(z)=1-(f(z))^2$.
:::

::: {.column width="50%"}
```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/TanhFunction.png")
```
:::
:::

## The ReLU {.smaller}

::: columns
::: {.column width="55%"}
-   *rectified linear unit*: $f(z)=\max\{0,z\}$.

-   Close to a linear: piece-wise *linear* function with two linear pieces.

-   Outputs are in $(0,\infty)$ , thus not bounded

-   Half rectified: activation threshold at 0

-   No vanishing gradient problem
:::

::: {.column width="45%"}
```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/ReLUFunction.png")
```
:::
:::

## More activation functions

![](images/ActivationFunctions.png){width="100%"}.

## Softmax Activation Function {.smaller}

- **Softmax** is an activation function used in the **output layer** of classification models, especially for **multi-class problems**.

- It converts raw scores (logits) into **probabilities**, ensuring that $\sum_{i=1}^{N} P(y_i) = 1$
  where $P(y_i)$ is the predicted probability for class $i$.

- Given an input vector $z$, *Softmax* transforms it as:
  $$
  \sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{N} e^{z_j}}
  $$
  - The exponentiation **amplifies differences**, making the highest value more dominant.
  - The normalization **ensures that probabilities sum to 1**.

<!-- - It is thoroughfully used in classification problems because -->
<!--   - It allows interpreting outputs as **class probabilities**. -->
<!--   - It ensures a **clear decision boundary** between classes. -->
<!--   - It works well for **multi-class classification** problems. -->



## The Artificial Neuron in Short

```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/ArtificialNeuron.png")
```

## The Artificial Neuron in Short {.smaller}

- An AN takes a vector of input values $x_{1}, \ldots, x_{d}$ and combines it with some weights that are local to the neuron $\left(w_{0}, w_{1}, . ., w_{d}\right)$ to compute a net input $w_{0}+\sum_{i=1}^{d} w_{i} \cdot x_{i}$. 

- To compute its output, it then passes the net input through a possibly non-linear univariate activation function $g(\cdot)$, usually chosen from a set of options such as *Sigmoid*, *Tanh* or *ReLU* functions

- To deal with the  *bias*, we create an extra input variable $x_{0}$ with value always equal to 1 , and so the function computed by a single artificial neuron (parameterized by its weights $\mathbf{w}$ ) is:

$$
y(\mathbf{x})=g\left(w_{0}+\sum_{i=1}^{d} w_{i} x_{i}\right)=g\left(\sum_{i=0}^{d} w_{i} x_{i}\right)=g\left(\mathbf{w}^{\mathbf{T}} \mathbf{x}\right)
$$

## Perceptron Learning Rule {.smaller}

- The **Perceptron Rule** updates weights based on **misclassified samples**.
- It ensures convergence **only if data is linearly separable**.

- **Weight Update Formula**
$$
w_j \leftarrow w_j + \eta(y - \hat{y})x_j
$$
where: 

  - $x_j$ = input feature; $\quad \eta$ = learning rate; 

  - $y$ = true label;  $\quad \hat{y}$=predicted class ($\pm1$);   

- **Key Features**
  - Uses a **step function** (not differentiable).
  - Cannot be optimized using **gradient descent**.
  - Fails to converge if data is **not linearly separable**.

## The Perceptron Decision Rule

```{r, fig.align='center', out.width="70%"}
knitr::include_graphics("images/PerceptronTrain.png")
```


## The Delta Rule {.smaller}

- Uuses **gradient descent** to minimize the **Sum of Squared Errors (SSE)**.

- It applies to models with **differentiable activation functions**.

- **Weight Update Formula**
$$
w_j \leftarrow w_j + \eta(y - h(x))x_j
$$
where: 

    - $x_j$ = input feature, $\eta$ = learning rate; 

    - $y$ = true label; $\quad h(x) =$ predicted output.  

- **Key Features**
  - Works with **continuous activation functions** (e.g., linear in ADALINE).
  - **Not limited to linearly separable problems**.
  - Forms the foundation for **gradient-based learning in neural networks**.

## The Delta Rule

```{r, fig.align='center', out.width="70%"}
knitr::include_graphics("images/AdalineTrainAN.png")
```


# From neurons to neural networks

## The basic neural network

::: font90

- Following with the brain analogy one can combine (artificial) neurons to create better learners.

- A simple artificial neural network is usually created by combining two types of modifications to the basic perceptron (AN).

  - Stacking several neurons insteads of just one.
  
  - Adding an additional layer of neurons, which is call a *hidden* layer, 

- This yields a system where *the output of a neuron can be the input of another* in many different ways.

:::
## An Artificial Neural network

```{r, fig.align='center', out.width="90%"}
knitr::include_graphics("images/nn.jpg")
```

## The architecture of ANN {.smaller}

In this figure, we have used circles to also denote the inputs to the
network. 

- Circles labeled +1 are *bias units*, and correspond to the intercept term. 

- The leftmost layer of the network is called the *input layer*.

- The rightmost layer of the network is called the *output* layer.

- The middle layer of nodes is called the *hidden layer*, because its values are not observed in the training set.


Bias nodes are not counted when stating the neuron size. 

With all this in mind our example neural network has three layers with:

 - 3 input units (not counting the bias unit), 
 - 3 hidden units,  
 - 1 output unit.

## How an ANN works 

::: font90

An ANN is a predictive model (a *learner*) whose properties and behaviour  can be well characterized.

<!-- In practice this means: -->

- It operates through a process known as *forward propagation*, which encompasses the  information flow from the input layer to the output layer.

- Forward propagation is performed by composing a series of linear and non-linear (activation) functions.

- These are characterized (parametrized) by their *weights* and *biases*, that need to be *learnt*. 
  - This is done by *training the ANN*.

:::

## Training the ANN

::: font80

- The training process aims at finding *the best possible parameter values* for the learning task defined by the fnctions. This is done by

  - Selecting an appropriate (convex) loss function,
  - Finding weights that minimize the total *cost* function (avg. loss).

- This is usually done using some iterative optimization procedure such as *gradient descent*.
  - This requires evaluating derivatives in a huge number of points.
  - Such high number may be reduced by *Stochastic Gradient Descent*.
  - The evaluation of derivatives is simplified thanks to *Backpropagation*.

:::

## Forward propagation {.smaller}

The process that encompasses the computations required to go from the input values to the final output is known as *forward propagation*.

<!-- The  weights are combined with the input to produce the final output. -->

For the ANN with 3 input values and 3 neurons in the hidden layer we have:

1. Each node, $a_i^{(2)}$ of the hidden layer opperates on all nodes of the input values
  
::: font90  
\begin{eqnarray}
a_1^{(2)}&=&f(\theta_{10}^{(1)}+\theta_{11}^{(1)}x_1+\theta_{12}^{(1)}x_2+\theta_{13}^{(1)}x_3)\\
a_2^{(2)}&=&f(\theta_{20}^{(1)}+\theta_{21}^{(1)}x_1+\theta_{22}^{(1)}x_2+\theta_{23}^{(1)}x_3)\\
a_3^{(2)}&=&f(\theta_{30}^{(1)}+\theta_{31}^{(1)}x_1+\theta_{32}^{(1)}x_2+\theta_{33}^{(1)}x_3))
\end{eqnarray}
:::

  2. Output of hidden layer is transformed by the activation function:
  
$$
h_{\Theta}(x)=a_1^{(3)}=f(\theta_{10}^{(2)}+\theta_{11}^{(2)}a_1^{(2)}+\theta_{12}^{(2)}a_2^{(2)}+\theta_{13}^{(2)}a_3^{(2)}
$$

## A compact notation (1) {.smaller}

Let  $z_i^{(l)}$ denote the total weighted sum of inputs to unit $i$ in layer $l$:

$$
z_i^{(2)}=\theta_{i0}^{(1)}+\theta_{i1}^{(1)}x_1+\theta_{i2}^{(1)}x_2+\theta_{i3}^{(1)}x_3,
$$ 
the output becomes: $a_i^{(l)}=f(z_i^{(l)})$.


Extending the activation function $f(\cdot)$ to apply elementwise to vectors: 

$$
    f([z_1,z_2,z_3]) = [f(z_1), f(z_2),f(z_3)],
$$
we can write the previous equations more compactly as:

```{=tex}
\begin{eqnarray}
z^{(2)}&=&\Theta^{(1)}x\nonumber\\
a^{(2)}&=&f(z^{(2)})\nonumber\\
z^{(3)}&=&\Theta^{(2)}a^{(2)}\nonumber\\
h_{\Theta}(x)&=&a^{(3)}=f(z^{(3)})\nonumber
\end{eqnarray}
```

## A compact notation (2) 

More generally, recalling that we also use $a^{(1)}=x$ to also  denote the values from the input layer,

Given layer $l$'s activations $a^{(l)}$, we can compute layer   $l+1$'s activations $a^{(l+1)}$ as:


\begin{equation}
z^{(l+1)}=\Theta^{(l)}a^{(l)}
\label{eqforZs}
\end{equation}

\begin{equation}
a^{(l+1)}=f(z^{(l+1)})
\label{eqforAs}
\end{equation}

## A compact notation (3) {.smaller}


This can be used to provide a matrix representation for the weighted sum
of inputs of all neurons:

$$
z^{(l+1)}=
\begin{bmatrix}
z_1^{(l+1)}\\
z_2^{(l+1)}\\
\vdots\\
z_{s_{l+1}}^{(l)}
\end{bmatrix}=
\begin{bmatrix}
\theta_{10}^{(l)}& \theta_{11}^{(l)}&\theta_{12}^{(l)}&...&\theta_{1s_{l}}^{(l)}&\\
\theta_{20}^{(l)}& \theta_{21}^{(l)}&\theta_{22}^{(l)}&...&\theta_{2s_{l}}^{(l)}&\\
\vdots & \vdots& \vdots & \vdots & \vdots\\
\theta_{s_{l+1}0}^{(l)}& \theta_{s_{l+1}1}^{(l)}&\theta_{s_{l+1}2}^{(l)}&...&\theta_{s_{l+1}s_{l}}^{(l)}&\\
\end{bmatrix}
\cdot\begin{bmatrix}
1\\
a_1^{(l)}\\
a_2^{(l)}\\
\vdots\\
a_{s_l}^{(l)}
\end{bmatrix}
$$

## A compact notation (4) {.smaller}

So that, the activation is then:

$$
a^{(l+1)}=
\begin{bmatrix}
a_1^{(l+1)}\\
a_2^{(l+1)}\\
\vdots\\
a_{s_{l+1}}^{(l)}
\end{bmatrix}=f(z^{(l+1)})=\begin{bmatrix}
f(z_1^{(l+1)})\\
f(z_2^{(l+1)})\\
\vdots\\
f(z_{s_{l+1}}^{(l)})
\end{bmatrix}
$$

## Eficient Forward propagation

- The way input data is transformed, through a series of weightings and transformations, until the ouput layer is called   *forward propagation*.

- By organizing parameters in matrices, and using matrix-vector operations, fast linear algebra routines can be used to perform the required calculations in a fast efficent way.


## Multiple architectures for ANN

-   We have so far focused on a single hidden layer neural network of the example. 

- One can. however build neural networks with many distinct   architectures (meaning patterns of connectivity between neurons),
    including ones with multiple hidden layers.

-   See [here the Neural Network
    Zoo](https://www.asimovinstitute.org/neural-network-zoo/).


## Multiple architectures for ANN {.smaller}

:::: {.columns}

::: {.column width='50%'}

- We have so far focused on a single hidden layer neural network of the example

- One can build neural networks with many distinct architectures (meaning patterns of connectivity between neurons), including ones with multiple hidden layers.


:::

::: {.column width='50%'}

![](images/2.1-Introduction_to_ANN-Slides_insertimage_3.png)
[The Neural Network Zoo](https://www.asimovinstitute.org/neural-network-zoo/)

:::

::::


## Multiple layer dense Networks

-   Most common choice is a $n_l$-layered network:
    -   layer 1 is the input layer,
    -   layer $n_l$ is the output layer,
    -   and each layer $l$ is densely connected to layer $l+1$.
-   In this setting, to compute the output of the network, we can compute all the activations in layer $L_2$, then layer $L_3$, and so on, up to layer $L_{nl}$, using equations seen previously.

## Feed Forward NNs

-   The type of NN described is called feed-forward *neural network (FFNN)*, since
    -   All computations are done by Forward propagation
    -   The connectivity graph does not have any directed loops or cycles.

# Training Neural Networks


## Training an ANN 

::: font80

- An ANN is a predictive model whose properties and behaviour  can be mathematically characterized.

- In practice this means:
  - The ANN acts by composing a series of linear and non-linear (activation) functions.
  - These are characterized by their *weights* and *biases*, that need to be *learnt*.
  
- *Training* the network is done by 
  - Selecting an appropriate (convex) loss function,
  - Finding weights that minimize a the total *cost* function (avg loss).

:::

## Training an ANN

```{r, fig.align='center'}
knitr::include_graphics("images/ANN-Training.png")
```

::: font80
Source: [Overview of a Neural Network’s Learning Process](https://medium.com/data-science-365/overview-of-a-neural-networks-learning-process-61690a502fa)
:::

## The tools for training

- Training an ANN is usually done using some iterative optimization procedure such as *Gradient Descent*.

- This requires evaluating derivatives in a huge number of points.
    - Such high number may be reduced by *Stochastic Gradient Descent*.
    - The evaluation of derivatives is simplified thanks to *Backpropagation*.

## Loss functions for optimization {.smaller}

Depending on the activation function it may be advisable to use one or another form of loss function.

- A typical choice may be *quadratic  (or square) error loss*:
$$
  l(h_\theta(x),y)=\left (y-\frac{1}{1+e^{-\theta^\intercal x}}\right )^2
$$

- Given a sigmoid AF, the squared error loss [*is  not a convex problem*](https://community.deeplearning.ai/t/why-mse-is-not-a-good-loss-function-for-logistic-regression/255547) so that MSE is not appropriate.

- Quadratic loss may be used with ReLu activation.


## Cross-entropy loss function {.smaller}

- A common loss function to use with ANNs is Cross-entropy defined as:

::: font80
$$
    l(h_\theta(x),y)=\big{\{}\begin{array}{ll}
    -\log h_\theta(x) & \textrm{if }y=1\\
    -\log(1-h_\theta(x))& \textrm{if }y=0
    \end{array}
$$

:::

- This function can also be written as:

$$
l(h_\theta(x),y)=-y\log h_\theta(x) - (1-y)\log(1-h_\theta(x))
$$

- Using cross-entropy loss, the cost function is of the form:

::: font80

$$
  J(\theta)=-\frac{1}{n}\left[\sum_{i=1}^n (y^{(i)}\log h_\theta(x^{(i)})+ (1-y^{(i)})\log(1-h_\theta(x^{(i)}))\right]
$$

:::

- Now, this is a convex optimization problem.

<!-- ## Regularized cross entropy -->

<!-- In practice we often work with a *regularized version* of the cost function (we don't regularize the bias units) -->

<!-- ```{=tex} -->
<!-- \begin{eqnarray*} -->
<!-- J(\Theta)&=&-\frac{1}{n}\big[\sum_{i=1}^n \sum_{k=1}^K y_k^{(i)}\log( h_\theta(x^{(i)}))_k\\ -->
<!-- &+&(1-y_k^{(i)})\log(1-(h_\theta(x^{(i)}))_k)\big]\\ -->
<!-- &+&\lambda\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}} -->
<!-- (\theta_{ji}^{(l)})^2 -->
<!-- \end{eqnarray*} -->
<!-- ``` -->


## The parameters for tuning

::: font80

- Training a network corresponds to finding the parameters, that is, *the weights* and *the biases*, that minimize the cost function. 

- Althoug weights & biases are respectively matrices and vectors, it is convenient to represent them in a vectorized form stored as a single vector, that will be denoted here, by $\theta$. 

- We may suppose $\theta\in\mathbb{R}^p$, and write the cost function as $J(\theta)$ to emphasize its dependence on the parameters, that is:
$$
\begin{eqnarray*}
J: \mathbb{R}^p & \rightarrow \mathbb{R}\\
\theta &  \rightarrow J(\theta)
\end{eqnarray*}
$$

:::

## Gradient or *Steepest* Descent {.smaller}

:::: {.columns}

::: {.column width='50%'}

- A classical method in optimization to minimize a convex function $J(\theta)$.

- It proceeds iteratively, computing a sequence of vectors $\theta^1, \theta^2, ..., \theta^n$ in $\mathbb{R}^p$ with the aim of converging to a vector that minimizes the cost function. 

:::

::: {.column width='50%'}

<br>

```{r, fig.align='center', out.width="100%"}
knitr::include_graphics("images/GradientDescentSchema.png")
```

:::

::::

## Gradient Descent {.smaller}

::: font90

- Suppose that our current vector is $\theta$. 
*How should we choose a perturbation, $\Delta\theta$, so that the next vector, $\theta+\Delta\theta$, represents an improvement*, that is: $J(\theta +\Delta\theta) < J(\theta)$?

- *Linearize* the cost function using a Taylor approximation.

- If $\Delta\theta$ is small, then ignoring terms of order $||\Delta\theta||^2$ or higher:
$$
J(\theta+\Delta\theta)\approx J(\theta)+\sum_{i=1}^p\frac{\partial J(\theta)}{\partial\theta_i}\Delta\theta_i
$$ 
or, equivalently:
\begin{equation}\label{g2}
J(\theta+\Delta\theta)\approx J(\theta)+\nabla J(\theta)^\intercal\Delta\theta
\end{equation}
where $\nabla J(\theta)\in\mathbb{R}^p$
 denote the *gradient*, i.e. the vector of partial derivatives:
\begin{equation}\label{g1}
\nabla J(\theta)=\left(\frac{\partial J(\theta)}{\partial\theta_1},...,\frac{\partial J(\theta)}{\partial\theta_p}\right)^\intercal
\end{equation}

:::

## Gradient Descent

::: font80

- Goal: choose a perturbation, $\Delta\theta$, s.t.: $J(\theta +\Delta\theta) < J(\theta)$

- Taylor approximation above suggests that choosing $\Delta\theta$ to *make $\nabla J(\theta)^\intercal\Delta\theta$ negative* will make the value of $J(\theta+\Delta\theta)$ smaller. 

- Indeed it can be shown that the highest possible negative value will  come out when $-\nabla J(\theta)=\Delta\theta$, which leads to the gradient descent formula:
$$
\theta \rightarrow \theta-\eta\nabla J(\theta),
$$
where $\eta$, the *learning rate* is the size of the step taken at each iteration, which should be small because of Taylor App.

:::

## The Cauchy-Schwarz inequality and Gradient Descent {.smaller}

- The [Cauchy-Schwarz inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality), states that for any $f,g\in\mathbb{R}^p$, we have:
$$
|f^\intercal g|\leq ||f||\cdot ||g||.
$$ 
-  Moreover, the two sides are equal if and only if $f$ and $g$ are linearly dependent (meaning they are parallel).

- By Cauchy-Schwarz,biggest possible value for $\nabla J(\theta)^\intercal\Delta\theta$ is the upper bound, $||\nabla J(\theta)||\cdot ||\Delta\theta||$.
  - The equality is only reached when $||\nabla J(\theta)||= ||\Delta\theta||$

- This explains why we choose precisely  $-\nabla J(\theta)=\Delta\theta$


## Gradient Descent {.smaller}

In summary, givent a cost function $J(\theta)$ to be optimized the gradient descent optimization proceeds as follows:

1. **Initialize** $\theta_0$ randomly or with some predetermined values
2. **Repeat until convergence:**
    $$
    \theta_{t+1} = \theta_{t} - \eta \nabla J(\theta_{t})
    $$
3. **Stop when:** $|J(\theta_{t+1}) - J(\theta_{t})| < \epsilon$

  - $\theta_0$ is the initial parameter vector,
  - $\theta_t$ is the parameter vector at iteration $t$,
  - $\eta$ is the learning rate,
  - $\nabla J(\theta_{t})$ is the gradient of the loss function with respect to $\theta$ at iteration $t$,
  - $\epsilon$ is a small positive value indicating the desired level of convergence.

## Gradient descent Illustration

<!-- ```{r } -->
<!-- if(!require(animation)) install.packages('animation') -->
<!-- library(animation) -->
<!-- ani.options(interval = 0.5, nmax = 10) -->
<!-- xx = grad.desc() -->
<!-- ``` -->


- Gradient descent is an intuitiva approach that has been thoroughly illustrated in many different ways:


![https://assets.yihui.org/figures/animation/example/grad-desc](https://assets.yihui.org/figures/animation/example/grad-desc/demo-a.gif){fig-align="center" width="100%"} 

## Computing Gradients {.smaller}

- The gradient method optimizes weights and biases $\theta =\{W, b\}$ by minimizing the cost function $J(\theta)$.
- This requires computing partial derivatives:
  $$
  \frac{\partial}{\partial\theta_j}J(\theta)
  $$
- The algorithm used for this computation is called backpropagation.

## A short history

- Introduced in the 1970s in an MSc thesis.
- In 1986, [Rumelhart, Hinton, and Williams](https://www.nature.com/articles/323533a0) demonstrated that backpropagation significantly improves learning speed.
- This breakthrough enabled neural networks to solve previously intractable problems.

## Backpropagation intuition {.smaller}

- The term originates from error backpropagation.
- An artificial neural network computes outputs through forward propagation:
  - Input values pass through linear and non-linear transformations.
  - The network produces a prediction.
- The error (difference between predicted and true value) is:
  - Propagated backward to compute the error contribution of each neuron.
  - Used to adjust weights iteratively.

## Delta rule and error term $\delta$ {.smaller}

- The delta rule updates weights based on the error term $\delta_j$, which measures how much neuron $j$ contributes to the error.
- It is computed as:
  $$
  \delta_j = \sigma^{\prime}(z_j) (c_j - y_j)
  $$
  where:
  - $\sigma^{\prime}(z_j)$ is the derivative of the activation function.
  - $(c_j - y_j)$ is the error in neuron $j$.

## Weight update $\Delta W$ {.smaller}

- Once $\delta_j$ is computed, weights are updated using:
  $$
  \Delta w_{i}^{j} = \eta \delta_j x_{i}^{j}
  $$
  where:
  - $\eta$ is the learning rate.
  - $\delta_j$ determines the direction and magnitude of the update.
  - $x_{i}^{j}$ is the input to neuron $j$.

- In hidden layers, $\delta$ is propagated backward:
  $$
  \delta_k = \sigma^{\prime}(z_k) \sum_{j \in S_k} \delta_j w_k^j
  $$
  where $S_k$ is the set of output neurons connected to neuron $k$.

## Backpropagation algorithm {.smaller}

1. **Forward propagation**  
   - Compute the network output for a given input.
  
2. **Backward propagation**  
   - Compute $\delta_j$ for each neuron in the output layer.
   - Propagate $\delta_j$ backward to compute $\delta_k$ for hidden layers.
   - Update weights using gradient descent.

## Final algorithm {.smaller}

**Input:**  $W$ (weight vectors), $D$ (training dataset).

**Repeat until error is below threshold:**

1. Compute network output for each training instance.

2. For each neuron $j$ in the output layer:

   - Compute $\delta_j = \sigma^{\prime}(z_j) (c_j - y_j)$

   - Update weights: $\Delta w_i^j = \eta \delta_j x_i^j$

3. For each neuron $k$ in hidden layers:

   - Compute  $\delta_k = \sigma^{\prime}(z_k) \sum_{j \in S_k} \delta_j w_k^j$

   - Update weights : $\Delta w_i^k = \eta \delta_k x_i^k$

**Output:** Updated weight vectors $W$.

<!-- ## Key takeaways {.smaller} -->

<!-- - $\delta_j$ measures the contribution of each neuron to the total error.   -->
<!-- - $\Delta W$ updates weights using $\delta_j$ and the learning rate.   -->
<!-- - Backpropagation propagates $\delta$ backward to update all layers.   -->

## Chain rule in backpropagation {.smaller}

- In multi-layer networks, the error signal must be **propagated backward** through multiple layers.

- The weight update in hidden layers depends not only on the local error but also on how errors propagate from the output layer.

- This is achieved using the **chain rule of calculus**, which decomposes derivatives into simpler components.

- That is, the **gradient descent update rule** for the weight is:

$$
\Delta w_i^j = -\eta \frac{\partial J}{\partial w_{i}^{j}} = \eta \delta_j x_i^j
$$
where the derivative will be computed applying the chain rule.

## Applying the chain rule {.smaller}

- To compute the gradient of the loss function $J$ with respect to a weight $w_{i}^{j}$ connecting neuron $i$ to neuron $j$, we use the chain rule:

$$
\frac{\partial J}{\partial w_{i}^{j}} =
\frac{\partial J}{\partial y_j} \cdot
\frac{\partial y_j}{\partial z_j} \cdot
\frac{\partial z_j}{\partial w_{i}^{j}}
$$

  - $\frac{\partial J}{\partial y_j}$ measures how the loss changes with respect to the output of neuron $j$.
  - $\frac{\partial y_j}{\partial z_j} = \sigma^{\prime}(z_j)$ is the derivative of the activation function.
  - $\frac{\partial z_j}{\partial w_{i}^{j}} = x_i^{j}$ represents the input to the neuron.

## Automatic differentiation {.smaller}

- Modern deep learning frameworks do not compute gradients manually.
- Instead, they use **automatic differentiation** and **computational graphs** to simplify and speed up backpropagation.


- A **computational graph** represents the sequence of operations in a neural network as a directed graph.
  - Each node corresponds to an operation (e.g., addition, multiplication, activation function).
  - This structure allows efficient **backpropagation** by applying the **chain rule automatically**.

- **Automatic differentiation (AD)** relies the computational graph to apply the chain rule and compute gradients automatically in the Backwards pass.

- Frameworks like **TensorFlow, PyTorch, and JAX** use **reverse-mode differentiation**, which is particularly efficient for functions with many parameters (like neural networks).


## Tuning a Neural Network

```{r echo=FALSE}
Top = 100
Left = 50
Width =800
Height = 400
```


::: {.r-stack}

![](images/img01.gif){.fragment .absolute top=100 left=50 width="800" height="400" }

![](images/img02.gif){.fragment .absolute top=100 left=50 width="800" height="400" }

![](images/img03.gif){.fragment .absolute top=100 left=50 width="800" height="400" }

![](images/img04.gif){.fragment .absolute top=100 left=50 width="800" height="400" }

![](images/img05.gif){.fragment .absolute top=100 left=50 width="800" height="400" }

![](images/img06.gif){.fragment .absolute top=100 left=50 width="800" height="400"}

![](images/img07.gif){.fragment .absolute top=100 left=50 width="800" height="400" }

![](images/img08.gif){.fragment .absolute top=100 left=50 width="800" height="400" }
<!-- ::: -->



<!-- ## Backpropagation illustrated -->

<!-- ![](images/img08.gif){.fragment .absolute top=100 left=50 width="800" height="400"} -->

<!-- ::: {.r-stack} -->

![](images/img09.gif){.fragment .absolute top=100 left=50 width="800" height="400"}

![](images/img10.gif){.fragment .absolute top=100 left=50 width="800" height="400"}

![](images/img11.gif){.fragment .absolute top=100 left=50 width="800" height="400"}

![](images/img12.gif){.fragment .absolute top=100 left=50 width="800" height="400"}

![](images/img13.gif){.fragment .absolute top=100 left=50 width="800" height="400"}

![](images/img14.gif){.fragment .absolute top=100 left=50 width="800" height="400"}

![](images/img15.gif){.fragment .absolute top=100 left=50 width="800" height="400"}

![](images/img16.gif){.fragment .absolute top=100 left=50 width="800" height="400"}

![](images/img17.gif){.fragment .absolute top=100 left=50 width="900" height="500"}

![](images/img18.gif){.fragment .absolute top=100 left=50 width="900" height="500"}

![](images/img19.gif){.fragment .absolute top=100 left=50 width="900" height="500"}

:::



# Improving the learning process

## Learning optimization

- The learning porocess such as it has been derived may be improved in different ways.
  
  - Predictions can be be bad and require improvement.
  - Computations may be inefficent or slow.
  - The network may overfit and lack generalizability.

- This can be partially soved applying distinct approaches.

## Network architechture

- Network performance is affected by many hyperparameters

  - Network topology
  - Number of layers
  - Number of neurons per layer
  - Activation function(s)
  - Weights initialization procedure
  - etc.
  

## Hyperparameter tuning

- Hyperparameters selection and tuning may be hard, due simply to dimensionality.
- Standard approaches to search for best parameters combinations are used.

```{r fig.align='center'}
knitr::include_graphics("images/GridVsRandomSearch.png")
```

## How many (hidden) layers

- Traditionally considered that one layer may be enough

  - *Shallow Networks*
  
- Posterior research showed that adding more layers increases efficency
  - Number of neurons per layer decreases exponentially
- Although there is also risk of overfitting


## Epochs and iterations

- It has been shown that using the whole training set only once may not be enough for training an ANN.

- One iteration of the training set is known as an *epoch*.

- The number of epochs $N_E$, defines how many times we iterate along the whole training set.

- $N_E$ can be fixed, determined by cross-validation or left open and stop the training when it does not improve anymore.


## Iterations and batches

- A complementary strategy to increasing the number of epochs is decreasing the number of instances in each iteration.

- That is, the training set is broken in a number of *batches* that are trained separately.

  - Batch learning allows weights to be updated more frequently per epoch.
  
  - The advantage of batch learning is related to the gradient descent approach used.
  
## Training in batches

```{r out.width="150%", fig.align='center'}
knitr::include_graphics("images/2.1-Introduction_to_ANN-Slides_insertimage_5.png")
```


## Improving Gradient Descent {.smaller}

- Training deep neural networks involves **millions of parameters** and **large datasets**.
- **Computing the full gradient** in every iteration is computationally expensive because:
  - It requires **summing over all training points**.
  - The cost grows **linearly with dataset size**.
- For **large-scale machine learning**, standard gradient descent becomes impractical.
- To address this, we use **alternative optimization strategies**:
  - **Gradient Descent Variants**: Control how much data is used per update.
  - **Gradient Descent Optimizers**: Improve convergence and stability.
  
## Improving Gradient Descent

```{r}
knitr::include_graphics("images/ImprovingGradientDescent.png")
```

::: font80

[An overview of gradient descent optimization algorithms](https://arxiv.org/abs/1609.04747?ref=ruder.io)

:::

<!-- ## Stochastic Gradient {.smaller} -->

<!-- -   A much cheaper alternative is to replace the mean of the individual gradients over all training points *by the gradient at a single, randomly chosen, point*. -->

<!-- -   This leads to the simplest form of the *stochastic gradient method*: -->
<!--   -   Choose an integer $i$ uniformly at random from $\{1,...,n\}$ and update \begin{equation}\label{g4} -->
<!--     \theta_j=\theta_j-\eta\frac{\partial}{\partial\theta_j}J(\theta;x^{(i)}) -->
<!--     \end{equation} -->

<!-- -   $x^{(i)}$ incuded in the notation of $J(\theta;x^{(i)})$ to remark the dependence. -->

<!-- ## Rationale for SGD  {.smaller} -->

<!-- - At each step, the SGD method uses one randomly chosen training point to represent the full training set. -->
<!-- -   As the iteration proceeds, the method sees more training points. -->
<!-- -   So *there is some hope* that this dramatic reduction in cost-per-iteration will be worthwhile overall. -->
<!-- -   Note that, even for very small $\eta$, the update is not guaranteed to reduce the overall cost function because we traded the mean for a single sample. -->
<!--   -  Hence, although the phrase stochastic gradient descent is widely used, we prefer to use **stochastic gradient**. -->

<!-- ## Batch Gradient Descent -->

<!-- ::: font90 -->

<!-- - BGD, like GD, computes the error for each  example in the training dataset, *but the model is updated only after evaluating all examples*. -->

<!-- - As a **benefits** there is computational efficiency, which produces a stable error gradient and a stable convergence.  -->

<!-- - As **drawbacks**, the stable error gradient can sometimes result in a state of convergence that isn’t the best the model can achieve.  -->
<!--   - It also requires the entire training dataset to be in memory and available to the algorithm. -->

<!-- ::: -->

<!-- ## Mini-Batch GD -->

<!-- ::: font90 -->

<!-- - Mini-batch Gradient Descent (MBGD) divides the training dataset into small batches to compute error and update model coefficients. -->

<!-- - MBGD balances the robustness of stochastic gradient descent with the efficiency of batch gradient descent, making it a prevalent implementation in deep learning. -->

<!-- - A common batch size for MBGD is 32, although it can vary based on the specific problem and data. -->

<!-- ::: -->


## Gradient Descent Variants {.smaller}

- Define **how much data is used per update**.
- Control the **trade-off between computational cost and stability**.
- Three common approaches:
  - **Batch Gradient Descent**:
    - Computes the gradient using the **entire dataset**.
    - Stable but **slow for large datasets**.
  - **Stochastic Gradient Descent (SGD)**:
    - Computes the gradient using **a single training example**.
    - Faster updates but **high variance**, leading to noisy convergence.
  - **Mini-Batch Gradient Descent**:
    - Uses a **subset (mini-batch) of training data** per update.
    - Balances **speed and stability**.

## Gradient Descent Variants

```{r out.width="150%", fig.align='center'}
knitr::include_graphics("images/GradientDescentTypes.png")
```
[Source: Gradient Descent and its Types](https://www.analyticsvidhya.com/blog/2022/07/gradient-descent-and-its-types/)

<!-- ## Alternative optimizers -->

<!-- - Gradient descent can be improved adopting different strategies -->

<!-- - Other optimizers exist that may make the training faster with strategies such as accumulating velocity in relevant directions or adjusting learning rates based on parameter frequency. -->

<!-- -  Overall these optimizers enable quicker convergence and better handling of various data characteristics, making them favorable choices over traditional gradient descent. -->

<!-- ## Momentum -->

<!--   - Accelerates SGD by accumulating momentum in relevant directions, akin to a rolling ball gaining speed downhill. -->
<!--   - Increases acceleration for dimensions with consistent gradients, reducing updates for changing gradients, leading to faster convergence and reduced oscillation. -->

<!-- ## Adagrad -->

<!--   - Adapts learning rate based on parameter frequency, making smaller updates for frequent features and larger updates for rare ones, suitable for sparse data. -->
<!--   - Efficient for handling sparse data due to its adaptive learning rates. -->

<!-- ## Adadelta -->

<!--   - An extension of Adagrad aiming to mitigate its aggressive, monotonically decreasing learning rate. -->
<!--   - Restricts accumulated past gradients to a fixed-size window, avoiding over-accumulation. -->

<!-- ## Adam -->

<!--   - Combines momentum optimization with adaptive learning rates, tracking both past gradients' exponential decay and a moving average of gradients. -->
<!--   - Recent studies suggest caution with adaptive optimization methods like Adam due to potential generalization issues, prompting exploration of alternatives such as Momentum optimization. -->

## Gradient Descent Optimizers {.smaller}

- Improve **learning dynamics** by modifying **how gradients are applied**.
- Help avoid **local minima, vanishing gradients, and slow convergence**.
- Common optimizers:
  - **Momentum**: Uses past gradients to accelerate convergence.
  - **Adagrad**: Adapts the learning rate per parameter.
  - **Adadelta & RMSProp**: Improve Adagrad by reducing aggressive decay.
  - **Adam**: Combines momentum and adaptive learning rates (widely used).
  - **Nesterov Accelerated Gradient**: Improves Momentum with lookahead updates.



## Optimizing Training Speed {.smaller}

- Training speed can be improved by adjusting key factors that influence convergence.

  - **Weight Initialization**: Properly initializing weights helps prevent vanishing or exploding gradients, leading to faster convergence.
  
  - **Adjusting Learning Rate**: A well-tuned learning rate accelerates training while avoiding instability or slow convergence.
  
  - **Using Efficient Cost Functions**: Choosing an appropriate loss function (e.g., cross-entropy for classification) speeds up gradient updates.

  
## Optimizing to Avoid Overfitting {.smaller}

- Overfitting occurs when a model learns noise instead of general patterns. Common strategies to prevent it include:

  - **L2 Regularization**: Penalizes large weights to reduce model complexity and improve generalization.

  - **Early Stopping**: Stops training when validation loss starts increasing, preventing unnecessary overfitting.

  - **Dropout**: Randomly disables neurons during training to make the model more robust.

  - **Data Augmentation**: Expands the training set by applying transformations (e.g., rotations, scaling) to improve generalization.



## Techniques to Improve Training 

::: font50

| Techniques                        | Performance Improvement | Learning Speed | Overfitting | Description |
|:---------------------------------:|:----------------------:|:-------------:|:----------:|:-----------------------------------------|
| Network Architecture              |          X           |       X       |      X     | Adjust layers, neurons andconnections|
| Epochs, Iterations, and Batch Size |                      |       X       |            | Controls updates per epoch to improve efficiency. |
| Softmax                           |          X           |               |            | Turns outputs into probabilities|
| Training Algorithms               |          X           |       X       |            | GD Improvements            |                      |       X       |            | Reduces vanishing/exploding gradient issues. |
| Learning Rate                     |          X           |       X       |            | Step size in gradient updates. |
| Cross-Entropy Loss       |                      |       X       |            | Optimized for classification|
| L2 Regularization                 |                      |       X       |      X     | Penalizes large weights to prevent overfitting. |
| Early Stopping                    |                      |       X       |            | Stops training when validation loss worsens. |
| Dropout                            |          X           |               |      X     | Randomly disables neurons to enhance generalization. |
| Data Augmentation                  |                      |               |      X     | Expands training data by applying transformations. |

:::

# An example using R

## A predictive ANN

We use the `neuralnet` package to build a simple neural network to predict if a type of stock pays dividends or not.

```{r echo=TRUE}
if (!require(neuralnet)) 
  install.packages("neuralnet", dep=TRUE)
```

## Data for the example

And use the `dividendinfo.csv` dataset from <https://github.com/MGCodesandStats/datasets>

```{r echo=TRUE}
mydata <- read.csv("https://raw.githubusercontent.com/MGCodesandStats/datasets/master/dividendinfo.csv")
str(mydata)
```

## Data pre-processing

```{r echo=TRUE}
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
normData <- as.data.frame(lapply(mydata, normalize))
```

## Test and training sets

Finally we break our data in a test and a training set:

```{r echo=TRUE}
perc2Train <- 2/3
ssize <- nrow(normData)
set.seed(12345)
data_rows <- floor(perc2Train *ssize)
train_indices <- sample(c(1:ssize), data_rows)
trainset <- normData[train_indices,]
testset <- normData[-train_indices,]
```

## Training a neural network

We train a simple NN with two hidden layers, with 4 and 2 neurons respectively.

```{r echo=TRUE}
#Neural Network
library(neuralnet)
nn <- neuralnet(dividend ~ fcfps + earnings_growth + de + mcap + current_ratio, 
                data=trainset, 
                hidden=c(2,1), 
                linear.output=FALSE, 
                threshold=0.01)
```

## Network plot

The output of the procedure is a neural network with estimated weights

```{r echo=TRUE}
plot(nn, rep = "best")
```

## Predictions

```{r echo=TRUE}
temp_test <- subset(testset, select =
                      c("fcfps","earnings_growth", 
                        "de", "mcap", "current_ratio"))
nn.results <- compute(nn, temp_test)
results <- data.frame(actual = 
                  testset$dividend, 
                  prediction = nn.results$net.result)
head(results)
```

## Model evaluation

```{r echo=TRUE}
roundedresults<-sapply(results,round,digits=0)
roundedresultsdf=data.frame(roundedresults)
attach(roundedresultsdf)
table(actual,prediction)
```


# References and Resources


