---
title: "Lab 2: Ensembles of Trees"
authors: Alex SÃ¡nchez, Esteban Vegas and Ferran Reeverter
date: "`r Sys.Date()`"
# output:
#   html_document:
#     theme: united
#     toc: yes
#     toc_depth: 3
#     code-fold: false
#   pdf_document: default
# editor_options: 
#   chunk_output_type: console
format:
    html:
      toc: true
      toc-depth: 3
      code-fold: show
      fig-width: 8
      fig-height: 6
    pdf: default
knit:
  quarto:
    chunk_options:
      echo: true
      cache: true
      prompt: false
      tidy: true
      comment: NA
      message: false
      warning: false
    knit_options:
      width: 75
reference-location: margin
execute:
    echo: true
    message: false
    warning: false
    cache: true
bibliography: "../StatisticalLearning.bib"
editor_options:
  chunk_output_type: console
---

```{r packages, include=FALSE}
# If the package is not installed then it will be installed

if(!require("randomForest")) install.packages("randomForest")
if (!require(vip)) install.packages("vip", dep=TRUE)
if(!require(modeldata))
  install.packages("modeldata", dep=TRUE)
if(!require("tree")) install.packages("tree")
if(!require("rsample")) install.packages("rsample")
if(!require("caret")) install.packages("caret")
if(!require("skimr")) install.packages("skimr")
if(!require("ipred")) install.packages("ipred")
```

```{r message=FALSE}
# Helper packages
library(dplyr)       # for data wrangling
library(ggplot2)     # for awesome plotting
library(modeldata)  # for parallel backend to foreach
library(foreach)     # for parallel processing with for loops

# Modeling packages
#library(caret)       # for general model fitting
library(rpart)       # for fitting decision trees
library(ipred)       # for fitting bagged decision trees
```

# Introduction

This lab presents some examples on building ensemble predictors with a variety of methods.

In order to facilitate the comparison between methods and tools the same prediction problem will be solved with distinct methods and distinct parameter settings.

We will work with the `AmesHousing` dataset, a dataset with multiple variables about housing in Ames, IA. Our goal is to predict the sales prices stored in the `Sale_Price` variable.

## The dataset

Packge `AmesHousing` contains the data jointly with some instructions to create the required dataset.

We will use, however data from the `modeldata` package where some preprocessing of the data has already been performed (see: [https://www.tmwr.org/ames](https://www.tmwr.org/ames))

```{r}
data(ames, package = "modeldata")
dim(ames)
```

## Exploratory Data Analysis an preprocessing 

The dataset has 74 variables, and has already been prepared by the package `modeldata`maintainers.

In any case it is always recommended to do some Exploratory Analysis.

```{r}
library(skimr)
skim(ames)
```

The exploration shows that the data set is well formed with factor data types for categorical variables and no missings.

It can also be seen that tha response variabl, `Sales_Price` varies on a high range, as confirmed below.

```{r}
summary(ames$Sale_Price)
```

```{r}
boxplot(ames[,sapply(ames, is.numeric)], las=2, cex.axis=0.5)
```

Although not strictly necessary, given that the variable that has the widest range of variation is the variable to predict we can consider transforming it. In this case the simplest transform seems to express the price in thousands instead of dollars. The distribution of the variable is asymetrical so we may also consider taking logarithm, but given that it would complicate the interpretation of the results, and that something like normality is not required by the methods we use, only division by 1000 is performed.

```{r}
require(dplyr)
ames <- ames %>% mutate(Sale_Price = Sale_Price/1000)
boxplot(ames)
```

## Spliting the data into test/train

We split the data in separate test / training sets and do it in such a way that samplig is balanced for the response variable, `Sale_Price`.

```{r}
if(!require(rsample))
  install.packages("rsample", dep=TRUE)
# Stratified sampling with the rsample package
set.seed(123)
split <- rsample::initial_split(ames, prop = 0.7, 
                       strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```


# A simple regression tree

As a first attempt to predict Sales_Price we build a unique regression tree, which as has been discussed is a weak learner. The `tree` package will be used to fit and optimize the tree.

We build a tree using non-restrictive parameters. This will allow space for pruning it better.

We use the `tree.control` function to set the values for the `control` parameter in the `tree` function.

Let's start with a tree with the default values.

```{r}
library(tree)

ctlPars <-tree.control(nobs =nrow(ames_train), 
                       mincut = 5, 
                       minsize = 10, 
                       mindev = 0.01)

ames_rt1 <-  tree::tree(
                    formula = Sale_Price ~ .,
                    data    = ames_train,
                    split   = "deviance",
                    control = ctlPars)
summary(ames_rt1)
```

This gives a small tree with only 11 terminal nodes.

We can visualize the tree 

```{r plotAmes1}
plot(x = ames_rt1, type = "proportional")
text(x = ames_rt1, splits = TRUE, pretty = 0, cex = 0.6, col = "firebrick")
```

If instead of using the `tree`package we use `rpart` we can get a better plot:

```{r}
ames_rt1bis <- rpart(
  formula = Sale_Price ~ .,
  data    = ames_train,
  method  = "anova",
  control = ctlPars
)
library(rpart.plot)
rpart.plot(ames_rt1bis)

```


## Optimizing the tree

In order to optimize the tree we ccan ompute the best cost complexity value

```{r pruneAmes1}
set.seed(123)
cv_ames_rt1 <- tree::cv.tree(ames_rt1, K = 5)

optSize <- rev(cv_ames_rt1$size)[which.min(rev(cv_ames_rt1$dev))]
paste("Optimal size obtained is:", optSize)
```

The best value of alpha is obtained with the same tree, which suggests that *there will be no advantage in pruning*.

This is confirmed by plotting tree size vs deviance which shows that the tree with the samllest error is the biggest one that can be obtained.

```{r plotPruneAmes}
library(ggplot2)
library(ggpubr)

resultados_cv <- data.frame(
                   n_nodes  = cv_ames_rt1$size,
                   deviance = cv_ames_rt1$dev,
                   alpha    = cv_ames_rt1$k
                 )

p1 <- ggplot(data = resultados_cv, aes(x = n_nodes, y = deviance)) +
      geom_line() + 
      geom_point() +
      geom_vline(xintercept = optSize, color = "red") +
      labs(title = "Error vs tree size") +
      theme_bw() 
  
p2 <- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +
      geom_line() + 
      geom_point() +
      labs(title = "Error vs penalization (alpha)") +
      theme_bw() 

ggarrange(p1, p2)
```

We could have tried to obtain a bigger tree with the hope that pruning might find a better tree.  This can be done setting the tree parameters to minimal values.


```{r fitAmesRT2}
ctlPars2 <-tree.control(nobs=nrow(ames_train), mincut = 1, minsize = 2, mindev = 0)
ames_rt2 <-  tree::tree(
                    formula = Sale_Price ~ .,
                    data    = ames_train,
                    split   = "deviance",
                    control = ctlPars2)
summary(ames_rt2)
```

This bigger tree has indeed a smaller deviance but pruning provides no benefit:

```{r pruneAmes2}
set.seed(123)
cv_ames_rt2 <- tree::cv.tree(ames_rt2, K = 5)

optSize2 <- rev(cv_ames_rt2$size)[which.min(rev(cv_ames_rt2$dev))]
paste("Optimal size obtained is:", optSize2)
```

```{r}
prunedTree2 <- tree::prune.tree(
                  tree = ames_rt2,
                  best = optSize2
               )
summary(prunedTree2)
```


```{r}
res_cv2 <- data.frame(
                   n_nodes  = cv_ames_rt2$size,
                   deviance = cv_ames_rt2$dev,
                   alpha    = cv_ames_rt2$k
                 )

p1 <- ggplot(data = res_cv2, aes(x = n_nodes, y = deviance)) +
      geom_line() + 
      geom_point() +
      geom_vline(xintercept = optSize2, color = "red") +
      labs(title = "Error vs tree size") +
      theme_bw() 
  
p2 <- ggplot(data = res_cv2, aes(x = alpha, y = deviance)) +
      geom_line() + 
      geom_point() +
      labs(title = "Error vs penalization (alpha)") +
      theme_bw() 

ggarrange(p1, p2)
```

The performance of the trees is hardly different between small or big tree in pruned or non-pruned version.

```{r ames_rt_pred1}
ames_rt_pred1 <- predict(ames_rt1, newdata = ames_test)
test_rmse1    <- sqrt(mean((ames_rt_pred1 - ames_test$Sale_Price)^2))
paste("Error test (rmse) for initial tree:", round(test_rmse1,2))
```


```{r ames_rt_pred2}
ames_rt_pred2 <- predict(ames_rt2, newdata = ames_test)
test_rmse2    <- sqrt(mean((ames_rt_pred2 - ames_test$Sale_Price)^2))
paste("Error test (rmse) for big tree:", round(test_rmse2,2))
```

```{r ames_pruned_pred}
ames_pruned_pred <- predict(prunedTree2, newdata = ames_test)
test_rmse3    <- sqrt(mean((ames_pruned_pred - ames_test$Sale_Price)^2))
paste("Error test (rmse) for pruned tree:", round(test_rmse3,2))
improvement <- (test_rmse3-test_rmse2)/test_rmse2*100
```

The MSE for each model will be saved to facilitate comparison with other models

```{r}
errTable <- data.frame(Model=character(),  RMSE=double())
errTable[1, ] <-  c("Default Regression Tree", round(test_rmse1,2))
errTable[2, ] <-  c("Big Regression Tree", round(test_rmse2,2))
errTable[3, ] <-  c("Optimally pruned Regression Tree", round(test_rmse3,2))
```

```{r}
# kableExtra::kable(errTable) %>% kableExtra::kable_styling()
knitr::kable(errTable) 
```


In summary, what is illustrated by this example is that, *for some datasets, it is very hard to obtain an optimal tree because there seems to be a minimum complexity which is very hard to decrease*. 

Building a saturated tree only provides a slight improvement of less than 5% in RMSE at the cost of having to use 5 times more variables in a tree withh more than 1000 nodes.

This is a good point to consider using an ensemble instead of single trees.

## Feature interpretation

To measure feature importance, the reduction in the loss function (e.g., SSE) attributed to each variable at each split is tabulated. 

In some instances, a single variable could be used multiple times in a tree; consequently, the total reduction in the loss function across all splits by a variable are summed up and used as the total feature importance.


Not all packages store the informaticon required to compute variable importance. For instance, the `tree`packges does not, but `rpart`or `caret` do save it.

```{r}
vip(ames_rt1bis, num_features = 40, bar = FALSE)
```


# Bagging trees

The first attempt to build an ensemble may be to apply `bagging` that is building multiple trees from a set of resamples and averaging the predictions of each tree.

In this example, rather than use a single pruned decision tree, we can use, say, 100 bagged unpruned trees (by not pruning the trees we're keeping bias low and variance high which is when bagging will have the biggest effect).

Bagging is equivalent to RandomForest if we use all the trees so the library `randomForest` is used.

```{r fitbaggedTrees1}
# make bootstrapping reproducible
set.seed(123)

library(randomForest)
bag.Ames <- randomForest(Sale_Price ~ ., 
                         data = ames_train, 
                         mtry = ncol(ames_train-1), 
                         ntree = 100,
                         importance = TRUE)
show(bag.Ames)
```

Bagging, as most ensemble procedures, can be time consuming.
See [@Boehmke2020](https://bradleyboehmke.github.io/HOML/bagging.html#easily-parallelize) for an example on how to easily parallelize code, and save time.


## Distinct error rates

The following chunks of code show the fit between data and predictions for the train set, the test set and the out-of bag samples

### Error estimated from train samples

```{r predictBagTrain}
yhattrain.bag <- predict(bag.Ames, newdata = ames_train)
# train_mse_bag  <- sqrt(mean(yhattrain.bag - ames_train$Sale_Price)^2)
train_rmse_bag <- sqrt(mean((yhattrain.bag - ames_train$Sale_Price)^2))
showError<- paste("Error train (rmse) for bagged tree:", round(train_rmse_bag,6))
plot(yhattrain.bag, ames_train$Sale_Price, main=showError)
abline(0, 1)
```

THis error is much smaller even than those from trees because of overfitting

### Error estimated from test samples

```{r predictBagTest}
yhat.bag <- predict(bag.Ames, newdata = ames_test)
# test_mse_bag  <- sqrt(mean(yhat.bag - ames_test$Sale_Price)^2)
test_rmse_bag  <- sqrt(mean((yhat.bag - ames_test$Sale_Price)^2))
showError<- paste("Error test (rmse) for bagged tree:", round(test_rmse_bag,4))
plot(yhat.bag, ames_test$Sale_Price, main=showError)
abline(0, 1)

```


### Error estimated from out-of-bag samples

Bagging allows computing an out-of bag error estimate. 

Out of bag error rate is reported in the output and can be computed from the `predicted` (*"the predicted values of the input data based on out-of-bag samples"*) 


```{r predictBagOOB}
oob_err<- sqrt(mean((bag.Ames$predicted-ames_train$Sale_Price)^2))
showError <- paste("Out of bag error for bagged tree:", round(oob_err,4))
plot(bag.Ames$predicted, ames_train$Sale_Price, main=showError)
abline(0, 1)

```

Interestingly this may be not only bigger than the error estimated on the train set but also bigger than the error estimated on the test set.


We can collect error rates and compare to each other and also to those obtained from regression trees:

```{r}
errTable <- data.frame(Model=character(),  RMSE=double())
errTable[1, ] <-  c("Default Regression Tree", round(test_rmse1,2))
errTable[2, ] <-  c("Big Regression Tree", round(test_rmse2,2))
errTable[3, ] <-  c("Optimally pruned Regression Tree", round(test_rmse3,2))
errTable[4, ] <-  c("Bagged Tree with Train Data", round(train_rmse_bag,2))
errTable[5, ] <-  c("Bagged Tree with Test Data", round(test_rmse_bag,2))
errTable[6, ] <-  c("Bagged Tree with OOB error rate", round(oob_err,2))
```


## Bagging parameter tuning

Bagging tends to improve quickly as the number of resampled trees increases, and then it reaches a platform.

The figure below has been produced iterated the computation above over `nbagg` values of 1â200 and applied the `bagging()` function.

```{r, echo=FALSE, fig.align='center', out.width="100%", fig.cap="Error curve for bagging 1-200 deep, unpruned decision trees. The benefit of bagging is optimized at 187 trees although the majority of error reduction occurred within the first 100 trees"}
knitr::include_graphics("images/baggingRSME.png")
```

## Variable importance

Due to the bagging process, models that are normally perceived as interpretable are no longer so. 

However, we can still make inferences about how features are influencing our model using *feature importance* measures based on the sum of the reduction in the loss function (e.g., SSE) attributed to each variable at each split in a given tree.

```{r}
require(dplyr)
VIP <- importance(bag.Ames) 
VIP <- VIP[order(VIP[,1], decreasing = TRUE),]
head(VIP, n=30)
```

Importance values can be plotted directly:

```{r}
invVIP <-VIP[order(VIP[,1], decreasing = FALSE),1] 
tVIP<- tail(invVIP, n=15)
barplot(tVIP, horiz = TRUE, cex.names=0.5)
```

Alternatively one can use the `vip`function from the `vip` package


```{r}
library(vip)
vip(bag.Ames, num_features = 40, bar = FALSE)
```

# A random forest to improve bagging

Bagging can be improved if, instead of using all variables to build each tree we rely on subsets of variables, which are chosen at each split in order to decrease correlation between trees.

Random Forests are considered to produce good predictors with default values sop no parameter is set in a first iteration.

```{r fitRF}
# make bootstrapping reproducible
set.seed(123)

require(randomForest)
RF.Ames <- randomForest(Sale_Price ~ ., 
                         data = ames_train, 
                         importance = TRUE)
```


```{r}
show(RF.Ames)
```

```{r predictRF}
yhat.rf <- predict(RF.Ames, newdata = ames_test)
plot(yhat.rf, ames_test$Sale_Price)
abline(0, 1)

test_rmse_rf  <- sqrt(mean((yhat.rf - ames_test$Sale_Price)^2))
paste("Error test (rmse) for Random Forest:", round(test_rmse_rf,2))
errTable[7, ] <-  c("Random Forest (defaults)", round(test_rmse_rf,2))
```

There is some improvement on bagging but it is clearly small.

Notice however that the percentage of explained variance is bigger for RF than for Bag

## Parameter optimization for RF

Several parameters can be changed to optimize a random forest predictor, but, usually, the most important one is the *number of variables* to be randomly selected at each split $m_{try}$, followed by the number of tree, which tends to stabilize after a certain value.

A common strategy to find the optimum combination of parameters is  to perform a *grid search* through a combination of parameter values. Obviously it can be time consuming so a small grid is run in the example below to illustrate how to do it.


```{r}
num_trees_range <- seq(100, 400, 100)

num_vars_range <- floor(ncol(ames_train)/(seq(2,4,1)))

RFerrTable <- data.frame(Model=character(), 
                       NumTree=integer(), NumVar=integer(), 
                       RMSE=double())
errValue <- 1
system.time(
for (i in seq_along(num_trees_range)){
  for (j in seq_along(num_vars_range)) {  
    numTrees <- num_trees_range[i]
    numVars <- num_vars_range [j] # floor(ncol(ames_train)/3) # default
    RF.Ames.n <- randomForest(Sale_Price ~ ., 
                         data = ames_train, 
                         mtry = numVars,
                         ntree= numTrees,
                         importance = TRUE)
    yhat.rf <- predict(RF.Ames.n, newdata = ames_test)
    oob.rf <- RF.Ames.n$predicted
    
    test_rmse_rf  <- sqrt(mean((yhat.rf - ames_test$Sale_Price)^2))

  
    RFerrTable[errValue, ] <-  c("Random Forest", 
                            NumTree = numTrees, NumVar = numVars, 
                            RMSE = round(test_rmse_rf,2)) 
    errValue <- errValue+1
  }
}
)
```
  
```{r}
RFerrTable %>% knitr::kable()
```

The minimum RMSE is attained at.

```{r}
bestRF <- which(RFerrTable$RMSE==min(RFerrTable$RMSE))
RFerrTable[bestRF,]
minRFErr <- as.numeric(RFerrTable[bestRF,4])
errTable[8, ] <-  c("Random Forest (Optimized)", round(minRFErr,2))
```

## Error comparison for all approaches

```{r}
# kableExtra::kable(errTable) %>% kableExtra::kable_styling()
knitr::kable(errTable) 
```

In summary, it has been shown that a Random Forest with 400 trees and 37 variables provides the smallest error rate, though the improvement on the default values and even the bagging approach is very small. This may be seen as a confirmation from the fact that Random Forests are well known to be good "out-of-the-box predictors", that is that they perform well, even without tuning.

## Variable importance

As could be expected, a variable importance plot shows that there is hardly any difference between the  variables by bagging or random forests.

```{r}
library(vip)
vip(RF.Ames, num_features = 40, bar = FALSE)
```


# Random Forests with Python

The following link points to a good brief tutorial on how to train and evaluate Random Forests using Python

[DataCamp: Random Forest Classification with Scikit-Learn](https://www.datacamp.com/tutorial/random-forests-classifier-python)

# References
